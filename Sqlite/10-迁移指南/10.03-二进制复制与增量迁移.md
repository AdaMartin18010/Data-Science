# SQLite åˆ° PostgreSQL äºŒè¿›åˆ¶å¤åˆ¶ä¸å¢é‡è¿ç§»

> **åˆ›å»ºæ—¥æœŸ**ï¼š2025-11-13
> **æœ€åæ›´æ–°**ï¼š2025-01-15
> **é€‚ç”¨ç‰ˆæœ¬**ï¼šSQLite 3.31+ è‡³ 3.47.x â†’ PostgreSQL 12+

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æ·±å…¥åˆ†æSQLiteåˆ°PostgreSQLçš„äºŒè¿›åˆ¶æ•°æ®å¤åˆ¶æœºåˆ¶ã€å…¨é‡è¿ç§»ç­–ç•¥ã€å¢é‡è¿ç§»æ–¹æ¡ˆï¼Œä»¥åŠæ•°æ®ä¸€è‡´æ€§ä¿è¯æœºåˆ¶ã€‚

---

## ğŸ“‘ ç›®å½•

- [SQLite åˆ° PostgreSQL äºŒè¿›åˆ¶å¤åˆ¶ä¸å¢é‡è¿ç§»](#sqlite-åˆ°-postgresql-äºŒè¿›åˆ¶å¤åˆ¶ä¸å¢é‡è¿ç§»)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ğŸ“Š æ€ç»´å¯¼å›¾](#-æ€ç»´å¯¼å›¾)
  - [ä¸€ã€äºŒè¿›åˆ¶æ•°æ®å¤åˆ¶æœºåˆ¶](#ä¸€äºŒè¿›åˆ¶æ•°æ®å¤åˆ¶æœºåˆ¶)
    - [1.1 SQLiteäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼](#11-sqliteäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼)
      - [SQLiteæ•°æ®åº“æ–‡ä»¶ç»“æ„](#sqliteæ•°æ®åº“æ–‡ä»¶ç»“æ„)
      - [é¡µç»“æ„åˆ†æ](#é¡µç»“æ„åˆ†æ)
      - [äºŒè¿›åˆ¶æ•°æ®å­˜å‚¨](#äºŒè¿›åˆ¶æ•°æ®å­˜å‚¨)
    - [1.2 PostgreSQLäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼](#12-postgresqläºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼)
      - [PostgreSQLå­˜å‚¨æ ¼å¼](#postgresqlå­˜å‚¨æ ¼å¼)
      - [BYTEAå­˜å‚¨æ ¼å¼](#byteaå­˜å‚¨æ ¼å¼)
    - [1.3 äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ç­–ç•¥](#13-äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ç­–ç•¥)
      - [è½¬æ¢ç­–ç•¥çŸ©é˜µ](#è½¬æ¢ç­–ç•¥çŸ©é˜µ)
      - [äºŒè¿›åˆ¶è½¬æ¢å®ç°](#äºŒè¿›åˆ¶è½¬æ¢å®ç°)
    - [1.4 BLOBæ•°æ®è¿ç§»](#14-blobæ•°æ®è¿ç§»)
      - [å¤§BLOBè¿ç§»ç­–ç•¥](#å¤§blobè¿ç§»ç­–ç•¥)
  - [äºŒã€å…¨é‡è¿ç§»ç­–ç•¥](#äºŒå…¨é‡è¿ç§»ç­–ç•¥)
    - [2.1 å…¨é‡è¿ç§»æ¶æ„](#21-å…¨é‡è¿ç§»æ¶æ„)
      - [è¿ç§»æ¶æ„è®¾è®¡](#è¿ç§»æ¶æ„è®¾è®¡)
      - [è¿ç§»æµç¨‹å›¾](#è¿ç§»æµç¨‹å›¾)
    - [2.2 è¡¨çº§å…¨é‡è¿ç§»](#22-è¡¨çº§å…¨é‡è¿ç§»)
      - [å•è¡¨è¿ç§»ç­–ç•¥](#å•è¡¨è¿ç§»ç­–ç•¥)
      - [å¤šè¡¨è¿ç§»ç­–ç•¥](#å¤šè¡¨è¿ç§»ç­–ç•¥)
    - [2.3 æ•°æ®å®Œæ•´æ€§éªŒè¯](#23-æ•°æ®å®Œæ•´æ€§éªŒè¯)
      - [è¡Œæ•°éªŒè¯](#è¡Œæ•°éªŒè¯)
      - [æ•°æ®æ ¡éªŒå’ŒéªŒè¯](#æ•°æ®æ ¡éªŒå’ŒéªŒè¯)
      - [é€è¡ŒéªŒè¯](#é€è¡ŒéªŒè¯)
    - [2.4 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#24-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
      - [æ‰¹é‡æ“ä½œä¼˜åŒ–](#æ‰¹é‡æ“ä½œä¼˜åŒ–)
      - [å¹¶è¡Œè¿ç§»](#å¹¶è¡Œè¿ç§»)
  - [ä¸‰ã€å¢é‡è¿ç§»ç­–ç•¥](#ä¸‰å¢é‡è¿ç§»ç­–ç•¥)
    - [3.1 å¢é‡è¿ç§»æ¶æ„](#31-å¢é‡è¿ç§»æ¶æ„)
      - [å¢é‡è¿ç§»æ¶æ„è®¾è®¡](#å¢é‡è¿ç§»æ¶æ„è®¾è®¡)
    - [3.2 å˜æ›´æ•è·æœºåˆ¶](#32-å˜æ›´æ•è·æœºåˆ¶)
      - [ç­–ç•¥1ï¼šè§¦å‘å™¨æ–¹å¼](#ç­–ç•¥1è§¦å‘å™¨æ–¹å¼)
      - [ç­–ç•¥2ï¼šWALæ—¥å¿—è§£æ](#ç­–ç•¥2walæ—¥å¿—è§£æ)
      - [ç­–ç•¥3ï¼šåº”ç”¨å±‚å˜æ›´æ•è·](#ç­–ç•¥3åº”ç”¨å±‚å˜æ›´æ•è·)
    - [3.3 å¢é‡åŒæ­¥ç­–ç•¥](#33-å¢é‡åŒæ­¥ç­–ç•¥)
      - [å®æ—¶åŒæ­¥](#å®æ—¶åŒæ­¥)
      - [æ‰¹é‡åŒæ­¥](#æ‰¹é‡åŒæ­¥)
    - [3.4 å†²çªè§£å†³æœºåˆ¶](#34-å†²çªè§£å†³æœºåˆ¶)
      - [å†²çªæ£€æµ‹](#å†²çªæ£€æµ‹)
      - [å†²çªè§£å†³ç­–ç•¥](#å†²çªè§£å†³ç­–ç•¥)
  - [å››ã€æ•°æ®ä¸€è‡´æ€§ä¿è¯](#å››æ•°æ®ä¸€è‡´æ€§ä¿è¯)
    - [4.1 äº‹åŠ¡ä¸€è‡´æ€§](#41-äº‹åŠ¡ä¸€è‡´æ€§)
      - [äº‹åŠ¡è¾¹ç•Œå¤„ç†](#äº‹åŠ¡è¾¹ç•Œå¤„ç†)
    - [4.2 æ•°æ®æ ¡éªŒæœºåˆ¶](#42-æ•°æ®æ ¡éªŒæœºåˆ¶)
      - [æ ¡éªŒå’ŒéªŒè¯](#æ ¡éªŒå’ŒéªŒè¯)
    - [4.3 å›æ»šç­–ç•¥](#43-å›æ»šç­–ç•¥)
  - [äº”ã€è¿ç§»å·¥å…·å®ç°](#äº”è¿ç§»å·¥å…·å®ç°)
    - [5.1 äºŒè¿›åˆ¶å¤åˆ¶å·¥å…·](#51-äºŒè¿›åˆ¶å¤åˆ¶å·¥å…·)
    - [5.2 å¢é‡åŒæ­¥å·¥å…·](#52-å¢é‡åŒæ­¥å·¥å…·)
    - [5.3 ç›‘æ§ä¸æŠ¥å‘Š](#53-ç›‘æ§ä¸æŠ¥å‘Š)
  - [å…­ã€å®æˆ˜æ¡ˆä¾‹](#å…­å®æˆ˜æ¡ˆä¾‹)
    - [6.1 å¤§å‹æ•°æ®åº“å…¨é‡è¿ç§»](#61-å¤§å‹æ•°æ®åº“å…¨é‡è¿ç§»)
    - [6.2 åœ¨çº¿å¢é‡è¿ç§»](#62-åœ¨çº¿å¢é‡è¿ç§»)
    - [6.3 é›¶åœæœºè¿ç§»](#63-é›¶åœæœºè¿ç§»)
  - [ğŸ”— ç›¸å…³èµ„æº](#-ç›¸å…³èµ„æº)
  - [ğŸ”— äº¤å‰å¼•ç”¨](#-äº¤å‰å¼•ç”¨)
    - [ç†è®ºæ¨¡å‹ ğŸ†•](#ç†è®ºæ¨¡å‹-)
    - [è®¾è®¡æ¨¡å‹ ğŸ†•](#è®¾è®¡æ¨¡å‹-)
  - [ğŸ“š å‚è€ƒèµ„æ–™](#-å‚è€ƒèµ„æ–™)

---

## ğŸ“Š æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((äºŒè¿›åˆ¶å¤åˆ¶ä¸å¢é‡è¿ç§»))
    äºŒè¿›åˆ¶æ•°æ®å¤åˆ¶æœºåˆ¶
      SQLiteäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼
        æ•°æ®åº“æ–‡ä»¶ç»“æ„
        é¡µç»“æ„åˆ†æ
        äºŒè¿›åˆ¶æ•°æ®å­˜å‚¨
      PostgreSQLäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼
        PostgreSQLå­˜å‚¨æ ¼å¼
        BYTEAå­˜å‚¨æ ¼å¼
      äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ç­–ç•¥
        è½¬æ¢ç­–ç•¥çŸ©é˜µ
        äºŒè¿›åˆ¶è½¬æ¢å®ç°
      BLOBæ•°æ®è¿ç§»
        å¤§BLOBè¿ç§»ç­–ç•¥
        æ€§èƒ½ä¼˜åŒ–
    å…¨é‡è¿ç§»ç­–ç•¥
      å…¨é‡è¿ç§»æ¶æ„
        è¿ç§»æ¶æ„è®¾è®¡
        è¿ç§»æµç¨‹å›¾
      è¡¨çº§å…¨é‡è¿ç§»
        å•è¡¨è¿ç§»ç­–ç•¥
        å¤šè¡¨è¿ç§»ç­–ç•¥
      æ•°æ®å®Œæ•´æ€§éªŒè¯
        è¡Œæ•°éªŒè¯
        æ•°æ®æ ¡éªŒå’ŒéªŒè¯
        é€è¡ŒéªŒè¯
      æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
        æ‰¹é‡æ“ä½œä¼˜åŒ–
        å¹¶è¡Œè¿ç§»
    å¢é‡è¿ç§»ç­–ç•¥
      å¢é‡è¿ç§»æ¶æ„
        å¢é‡è¿ç§»æ¶æ„è®¾è®¡
      å˜æ›´æ•è·æœºåˆ¶
        è§¦å‘å™¨æ–¹å¼
        WALæ—¥å¿—è§£æ
        åº”ç”¨å±‚å˜æ›´æ•è·
      å¢é‡åŒæ­¥ç­–ç•¥
        å®æ—¶åŒæ­¥
        æ‰¹é‡åŒæ­¥
      å†²çªè§£å†³æœºåˆ¶
        å†²çªæ£€æµ‹
        å†²çªè§£å†³ç­–ç•¥
    æ•°æ®ä¸€è‡´æ€§ä¿è¯
      äº‹åŠ¡ä¸€è‡´æ€§
        äº‹åŠ¡è¾¹ç•Œå¤„ç†
      æ•°æ®æ ¡éªŒæœºåˆ¶
        æ ¡éªŒå’ŒéªŒè¯
      å›æ»šç­–ç•¥
        å›æ»šæœºåˆ¶
    è¿ç§»å·¥å…·å®ç°
      äºŒè¿›åˆ¶å¤åˆ¶å·¥å…·
        å·¥å…·å®ç°
      å¢é‡åŒæ­¥å·¥å…·
        å·¥å…·å®ç°
      ç›‘æ§ä¸æŠ¥å‘Š
        ç›‘æ§æœºåˆ¶
        æŠ¥å‘Šç”Ÿæˆ
    å®æˆ˜æ¡ˆä¾‹
      å¤§å‹æ•°æ®åº“å…¨é‡è¿ç§»
        æ¡ˆä¾‹è¯´æ˜
      åœ¨çº¿å¢é‡è¿ç§»
        æ¡ˆä¾‹è¯´æ˜
      é›¶åœæœºè¿ç§»
        æ¡ˆä¾‹è¯´æ˜
```

---

## ä¸€ã€äºŒè¿›åˆ¶æ•°æ®å¤åˆ¶æœºåˆ¶

### 1.1 SQLiteäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼

#### SQLiteæ•°æ®åº“æ–‡ä»¶ç»“æ„

**æ–‡ä»¶æ ¼å¼æ¦‚è§ˆ**ï¼š

```text
SQLiteæ•°æ®åº“æ–‡ä»¶ = {
    æ–‡ä»¶å¤´ï¼ˆ100å­—èŠ‚ï¼‰: ç‰ˆæœ¬ä¿¡æ¯ã€é¡µå¤§å°ç­‰
    é¡µï¼ˆPageï¼‰: æ•°æ®å­˜å‚¨å•å…ƒ
        - é¡µå¤§å°: 512-65536å­—èŠ‚ï¼ˆé»˜è®¤4096ï¼‰
        - é¡µç±»å‹: B-Treeé¡µã€æº¢å‡ºé¡µã€ç©ºé—²é¡µ
    æ—¥å¿—æ–‡ä»¶ï¼ˆWALï¼‰: å†™å‰æ—¥å¿—
}
```

#### é¡µç»“æ„åˆ†æ

**B-Treeé¡µç»“æ„**ï¼š

```text
B-Treeé¡µ = {
    é¡µå¤´ï¼ˆ8-12å­—èŠ‚ï¼‰: é¡µç±»å‹ã€ç©ºé—²ç©ºé—´ç­‰
    å•å…ƒæ ¼æŒ‡é’ˆæ•°ç»„: æŒ‡å‘å•å…ƒæ ¼çš„åç§»é‡
    å•å…ƒæ ¼: å®é™…æ•°æ®
        å•å…ƒæ ¼å¤´: å˜é•¿æ•´æ•°ç¼–ç 
        é”®å€¼: ä¸»é”®æˆ–ç´¢å¼•é”®
        æ•°æ®: è¡Œæ•°æ®ï¼ˆå˜é•¿ç¼–ç ï¼‰
    ç©ºé—²ç©ºé—´: æœªä½¿ç”¨ç©ºé—´
}
```

#### äºŒè¿›åˆ¶æ•°æ®å­˜å‚¨

**BLOBå­˜å‚¨æ–¹å¼**ï¼š

```python
# SQLite BLOBå­˜å‚¨ç¤ºä¾‹
import sqlite3

conn = sqlite3.connect('example.db')
cursor = conn.cursor()

# æ’å…¥BLOBæ•°æ®
with open('image.jpg', 'rb') as f:
    image_data = f.read()
    cursor.execute("INSERT INTO images(name, data) VALUES (?, ?)",
                   ('photo.jpg', image_data))

# BLOBåœ¨é¡µä¸­çš„å­˜å‚¨ï¼š
# 1. å°BLOBï¼ˆ<é¡µå¤§å°-å¼€é”€ï¼‰: ç›´æ¥å­˜å‚¨åœ¨å•å…ƒæ ¼ä¸­
# 2. å¤§BLOBï¼ˆ>é¡µå¤§å°-å¼€é”€ï¼‰: å­˜å‚¨åœ¨æº¢å‡ºé¡µä¸­
```

**å­˜å‚¨æ ¼å¼ç‰¹ç‚¹**ï¼š

- **å˜é•¿ç¼–ç **ï¼šæ•´æ•°å’Œå­—ç¬¦ä¸²ä½¿ç”¨å˜é•¿ç¼–ç 
- **å­—èŠ‚åº**ï¼šå¤§ç«¯åºï¼ˆBig-Endianï¼‰
- **å¯¹é½**ï¼šæ•°æ®æŒ‰é¡µè¾¹ç•Œå¯¹é½

### 1.2 PostgreSQLäºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼

#### PostgreSQLå­˜å‚¨æ ¼å¼

**TOASTå­˜å‚¨æœºåˆ¶**ï¼š

```text
PostgreSQLå­˜å‚¨ = {
    å †æ–‡ä»¶ï¼ˆHeap Fileï¼‰: è¡¨æ•°æ®
        - è¡Œï¼ˆTupleï¼‰: å›ºå®šæ ¼å¼
        - TOAST: å¤§å¯¹è±¡å­˜å‚¨
    TOASTè¡¨: å¤§å¯¹è±¡æº¢å‡ºå­˜å‚¨
        - è§¦å‘æ¡ä»¶: è¡Œå¤§å° > 2KB
        - å­˜å‚¨æ–¹å¼: å‹ç¼© + åˆ†å—
}
```

#### BYTEAå­˜å‚¨æ ¼å¼

**BYTEAç¼–ç **ï¼š

```python
# PostgreSQL BYTEAå­˜å‚¨ç¤ºä¾‹
import psycopg2

conn = psycopg2.connect("dbname=test user=postgres")
cursor = conn.cursor()

# æ’å…¥BYTEAæ•°æ®
with open('image.jpg', 'rb') as f:
    image_data = f.read()
    cursor.execute("INSERT INTO images(name, data) VALUES (%s, %s)",
                   ('photo.jpg', psycopg2.Binary(image_data)))

# BYTEAå­˜å‚¨æ ¼å¼ï¼š
# 1. å°æ•°æ®ï¼ˆ<2KBï¼‰: ç›´æ¥å­˜å‚¨åœ¨è¡Œä¸­
# 2. å¤§æ•°æ®ï¼ˆ>2KBï¼‰: å­˜å‚¨åœ¨TOASTè¡¨ä¸­
```

**å­˜å‚¨æ ¼å¼ç‰¹ç‚¹**ï¼š

- **å›ºå®šæ ¼å¼**ï¼šè¡Œæ ¼å¼å›ºå®š
- **å­—èŠ‚åº**ï¼šå¹³å°ç›¸å…³ï¼ˆé€šå¸¸å°ç«¯åºï¼‰
- **å‹ç¼©**ï¼šTOASTè‡ªåŠ¨å‹ç¼©

### 1.3 äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ç­–ç•¥

#### è½¬æ¢ç­–ç•¥çŸ©é˜µ

| æ•°æ®ç±»å‹ | SQLiteæ ¼å¼ | PostgreSQLæ ¼å¼ | è½¬æ¢æ–¹æ³• | æ•°æ®å®Œæ•´æ€§ |
|---------|-----------|---------------|---------|-----------|
| **BLOB** | åŸå§‹å­—èŠ‚ | BYTEA | ç›´æ¥å¤åˆ¶ | âœ… 100% |
| **INTEGER** | å˜é•¿ç¼–ç  | å›ºå®š4/8å­—èŠ‚ | è§£ç åç¼–ç  | âœ… 100% |
| **REAL** | IEEE 754 | IEEE 754 | ç›´æ¥å¤åˆ¶ | âœ… 100% |
| **TEXT** | UTF-8/16 | UTF-8 | ç¼–ç è½¬æ¢ | âš ï¸ 95% |

#### äºŒè¿›åˆ¶è½¬æ¢å®ç°

**ç­–ç•¥1ï¼šç›´æ¥äºŒè¿›åˆ¶å¤åˆ¶ï¼ˆBLOBï¼‰**:

```python
def copy_blob_direct(sqlite_conn, pg_conn, table, blob_column):
    """ç›´æ¥å¤åˆ¶BLOBæ•°æ®"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # è¯»å–SQLite BLOB
    sqlite_cursor.execute(f"SELECT id, {blob_column} FROM {table}")

    for row_id, blob_data in sqlite_cursor:
        # ç›´æ¥å¤åˆ¶äºŒè¿›åˆ¶æ•°æ®
        pg_cursor.execute(
            f"UPDATE {table} SET {blob_column} = %s WHERE id = %s",
            (psycopg2.Binary(blob_data), row_id)
        )

    pg_conn.commit()
```

**ç­–ç•¥2ï¼šç¼–ç è½¬æ¢ï¼ˆTEXTï¼‰**:

```python
def copy_text_with_encoding(sqlite_conn, pg_conn, table, text_column):
    """å¤åˆ¶TEXTæ•°æ®ï¼ˆå¤„ç†ç¼–ç ï¼‰"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # æ£€æµ‹SQLiteç¼–ç 
    sqlite_cursor.execute("PRAGMA encoding")
    sqlite_encoding = sqlite_cursor.fetchone()[0]

    sqlite_cursor.execute(f"SELECT id, {text_column} FROM {table}")

    for row_id, text_data in sqlite_cursor:
        if isinstance(text_data, bytes):
            # è§£ç åé‡æ–°ç¼–ç ä¸ºUTF-8
            text_utf8 = text_data.decode(sqlite_encoding).encode('utf-8')
        else:
            text_utf8 = text_data.encode('utf-8')

        pg_cursor.execute(
            f"UPDATE {table} SET {text_column} = %s WHERE id = %s",
            (text_utf8.decode('utf-8'), row_id)
        )

    pg_conn.commit()
```

### 1.4 BLOBæ•°æ®è¿ç§»

#### å¤§BLOBè¿ç§»ç­–ç•¥

**ç­–ç•¥1ï¼šæµå¼ä¼ è¾“**:

```python
def migrate_large_blobs_streaming(sqlite_conn, pg_conn, table, blob_column, chunk_size=8192):
    """æµå¼ä¼ è¾“å¤§BLOB"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    sqlite_cursor.execute(f"SELECT id, {blob_column} FROM {table}")

    for row_id, blob_data in sqlite_cursor:
        if blob_data is None:
            continue

        # åˆ†å—ä¼ è¾“
        total_size = len(blob_data)
        offset = 0

        # ä½¿ç”¨PostgreSQLå¤§å¯¹è±¡æ¥å£
        pg_cursor.execute("SELECT lo_create(0)")
        oid = pg_cursor.fetchone()[0]

        # å†™å…¥å¤§å¯¹è±¡
        with pg_conn.cursor() as write_cursor:
            for i in range(0, total_size, chunk_size):
                chunk = blob_data[i:i+chunk_size]
                write_cursor.execute(
                    "SELECT lowrite(lo_open(%s, 131072), %s)",
                    (oid, psycopg2.Binary(chunk))
                )

        # å…³è”åˆ°è¡¨
        pg_cursor.execute(
            f"UPDATE {table} SET {blob_column} = lo_get({oid}) WHERE id = %s",
            (row_id,)
        )

        # åˆ é™¤ä¸´æ—¶å¤§å¯¹è±¡
        pg_cursor.execute(f"SELECT lo_unlink({oid})")

    pg_conn.commit()
```

**ç­–ç•¥2ï¼šæ‰¹é‡ä¼ è¾“**:

```python
def migrate_blobs_batch(sqlite_conn, pg_conn, table, blob_column, batch_size=100):
    """æ‰¹é‡ä¼ è¾“BLOB"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    sqlite_cursor.execute(f"SELECT id, {blob_column} FROM {table}")

    batch = []
    for row_id, blob_data in sqlite_cursor:
        batch.append((row_id, psycopg2.Binary(blob_data) if blob_data else None))

        if len(batch) >= batch_size:
            # æ‰¹é‡æ›´æ–°
            pg_cursor.executemany(
                f"UPDATE {table} SET {blob_column} = %s WHERE id = %s",
                [(blob, row_id) for row_id, blob in batch]
            )
            batch = []

    # å¤„ç†å‰©ä½™æ•°æ®
    if batch:
        pg_cursor.executemany(
            f"UPDATE {table} SET {blob_column} = %s WHERE id = %s",
            [(blob, row_id) for row_id, blob in batch]
        )

    pg_conn.commit()
```

---

## äºŒã€å…¨é‡è¿ç§»ç­–ç•¥

### 2.1 å…¨é‡è¿ç§»æ¶æ„

#### è¿ç§»æ¶æ„è®¾è®¡

```text
å…¨é‡è¿ç§»æ¶æ„ = {
    é˜¶æ®µ1: å‡†å¤‡é˜¶æ®µ
        - åˆ†æSQLiteæ•°æ®åº“ç»“æ„
        - åˆ›å»ºPostgreSQLè¡¨ç»“æ„
        - å‡†å¤‡è¿ç§»è„šæœ¬

    é˜¶æ®µ2: æ•°æ®è¿ç§»é˜¶æ®µ
        - è¡¨ç»“æ„è¿ç§»
        - æ•°æ®è¿ç§»ï¼ˆåˆ†æ‰¹ï¼‰
        - ç´¢å¼•åˆ›å»º
        - çº¦æŸåˆ›å»º

    é˜¶æ®µ3: éªŒè¯é˜¶æ®µ
        - æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        - æ•°æ®ä¸€è‡´æ€§éªŒè¯
        - æ€§èƒ½æµ‹è¯•
}
```

#### è¿ç§»æµç¨‹å›¾

```text
å¼€å§‹
  â†“
åˆ†æSQLiteæ•°æ®åº“
  â†“
åˆ›å»ºPostgreSQLè¡¨ç»“æ„
  â†“
è¿ç§»æ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
  â”œâ”€â†’ è¡¨1æ•°æ®è¿ç§»
  â”œâ”€â†’ è¡¨2æ•°æ®è¿ç§»
  â””â”€â†’ ...
  â†“
åˆ›å»ºç´¢å¼•å’Œçº¦æŸ
  â†“
æ•°æ®å®Œæ•´æ€§éªŒè¯
  â†“
å®Œæˆ
```

### 2.2 è¡¨çº§å…¨é‡è¿ç§»

#### å•è¡¨è¿ç§»ç­–ç•¥

**ç­–ç•¥1ï¼šå…¨è¡¨æ‰«æè¿ç§»**:

```python
def migrate_table_full(sqlite_conn, pg_conn, table_name, batch_size=1000):
    """å…¨è¡¨è¿ç§»"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # è·å–è¡¨ç»“æ„
    sqlite_cursor.execute(f"PRAGMA table_info({table_name})")
    columns = [col[1] for col in sqlite_cursor.fetchall()]

    # è·å–æ€»è¡Œæ•°
    sqlite_cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    total_rows = sqlite_cursor.fetchone()[0]

    # åˆ†æ‰¹è¿ç§»
    offset = 0
    while offset < total_rows:
        # è¯»å–ä¸€æ‰¹æ•°æ®
        sqlite_cursor.execute(
            f"SELECT * FROM {table_name} LIMIT ? OFFSET ?",
            (batch_size, offset)
        )

        rows = sqlite_cursor.fetchall()
        if not rows:
            break

        # æ‰¹é‡æ’å…¥PostgreSQL
        placeholders = ','.join(['%s'] * len(columns))
        insert_sql = f"INSERT INTO {table_name} ({','.join(columns)}) VALUES ({placeholders})"

        pg_cursor.executemany(insert_sql, rows)
        pg_conn.commit()

        offset += batch_size
        print(f"Migrated {offset}/{total_rows} rows")
```

**ç­–ç•¥2ï¼šåŸºäºä¸»é”®èŒƒå›´è¿ç§»**:

```python
def migrate_table_by_range(sqlite_conn, pg_conn, table_name, pk_column, batch_size=1000):
    """åŸºäºä¸»é”®èŒƒå›´è¿ç§»"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # è·å–ä¸»é”®èŒƒå›´
    sqlite_cursor.execute(f"SELECT MIN({pk_column}), MAX({pk_column}) FROM {table_name}")
    min_id, max_id = sqlite_cursor.fetchone()

    # æŒ‰èŒƒå›´åˆ†æ‰¹è¿ç§»
    current_id = min_id
    while current_id <= max_id:
        # è¯»å–ä¸€ä¸ªèŒƒå›´çš„æ•°æ®
        sqlite_cursor.execute(
            f"SELECT * FROM {table_name} WHERE {pk_column} >= ? AND {pk_column} < ?",
            (current_id, current_id + batch_size)
        )

        rows = sqlite_cursor.fetchall()
        if not rows:
            break

        # æ‰¹é‡æ’å…¥
        columns = [desc[0] for desc in sqlite_cursor.description]
        placeholders = ','.join(['%s'] * len(columns))
        insert_sql = f"INSERT INTO {table_name} ({','.join(columns)}) VALUES ({placeholders})"

        pg_cursor.executemany(insert_sql, rows)
        pg_conn.commit()

        current_id += batch_size
        print(f"Migrated up to ID {current_id}")
```

#### å¤šè¡¨è¿ç§»ç­–ç•¥

**ä¾èµ–å…³ç³»å¤„ç†**ï¼š

```python
def migrate_tables_with_dependencies(sqlite_conn, pg_conn, tables_order):
    """æŒ‰ä¾èµ–å…³ç³»é¡ºåºè¿ç§»è¡¨"""
    for table_name in tables_order:
        print(f"Migrating table: {table_name}")

        # ç¦ç”¨å¤–é”®æ£€æŸ¥ï¼ˆè¿ç§»æœŸé—´ï¼‰
        pg_cursor = pg_conn.cursor()
        pg_cursor.execute("SET session_replication_role = 'replica'")

        # è¿ç§»è¡¨æ•°æ®
        migrate_table_full(sqlite_conn, pg_conn, table_name)

        # æ¢å¤å¤–é”®æ£€æŸ¥
        pg_cursor.execute("SET session_replication_role = 'origin'")
        pg_conn.commit()

        print(f"Completed table: {table_name}")
```

### 2.3 æ•°æ®å®Œæ•´æ€§éªŒè¯

#### è¡Œæ•°éªŒè¯

```python
def verify_row_count(sqlite_conn, pg_conn, table_name):
    """éªŒè¯è¡Œæ•°"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    sqlite_cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    sqlite_count = sqlite_cursor.fetchone()[0]

    pg_cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    pg_count = pg_cursor.fetchone()[0]

    if sqlite_count == pg_count:
        print(f"âœ“ {table_name}: Row count matches ({sqlite_count})")
        return True
    else:
        print(f"âœ— {table_name}: Row count mismatch (SQLite: {sqlite_count}, PG: {pg_count})")
        return False
```

#### æ•°æ®æ ¡éªŒå’ŒéªŒè¯

```python
import hashlib

def verify_data_checksum(sqlite_conn, pg_conn, table_name, pk_column):
    """éªŒè¯æ•°æ®æ ¡éªŒå’Œ"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # è®¡ç®—SQLiteæ•°æ®æ ¡éªŒå’Œ
    sqlite_cursor.execute(f"SELECT {pk_column}, * FROM {table_name} ORDER BY {pk_column}")
    sqlite_data = sqlite_cursor.fetchall()
    sqlite_hash = hashlib.md5(str(sqlite_data).encode()).hexdigest()

    # è®¡ç®—PostgreSQLæ•°æ®æ ¡éªŒå’Œ
    pg_cursor.execute(f"SELECT {pk_column}, * FROM {table_name} ORDER BY {pk_column}")
    pg_data = pg_cursor.fetchall()
    pg_hash = hashlib.md5(str(pg_data).encode()).hexdigest()

    if sqlite_hash == pg_hash:
        print(f"âœ“ {table_name}: Data checksum matches")
        return True
    else:
        print(f"âœ— {table_name}: Data checksum mismatch")
        return False
```

#### é€è¡ŒéªŒè¯

```python
def verify_row_by_row(sqlite_conn, pg_conn, table_name, pk_column, sample_size=1000):
    """é€è¡ŒéªŒè¯ï¼ˆé‡‡æ ·ï¼‰"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # éšæœºé‡‡æ ·
    sqlite_cursor.execute(
        f"SELECT {pk_column} FROM {table_name} ORDER BY RANDOM() LIMIT ?",
        (sample_size,)
    )
    sample_ids = [row[0] for row in sqlite_cursor.fetchall()]

    mismatches = []
    for row_id in sample_ids:
        # è¯»å–SQLiteè¡Œ
        sqlite_cursor.execute(f"SELECT * FROM {table_name} WHERE {pk_column} = ?", (row_id,))
        sqlite_row = sqlite_cursor.fetchone()

        # è¯»å–PostgreSQLè¡Œ
        pg_cursor.execute(f"SELECT * FROM {table_name} WHERE {pk_column} = %s", (row_id,))
        pg_row = pg_cursor.fetchone()

        # æ¯”è¾ƒ
        if sqlite_row != pg_row:
            mismatches.append(row_id)

    if not mismatches:
        print(f"âœ“ {table_name}: All sampled rows match")
        return True
    else:
        print(f"âœ— {table_name}: {len(mismatches)} mismatches found")
        return False
```

### 2.4 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
def migrate_optimized(sqlite_conn, pg_conn, table_name, batch_size=10000):
    """ä¼˜åŒ–çš„æ‰¹é‡è¿ç§»"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # ä½¿ç”¨COPYå‘½ä»¤ï¼ˆæœ€å¿«ï¼‰
    sqlite_cursor.execute(f"SELECT * FROM {table_name}")

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
        temp_file = f.name

        # å†™å…¥CSVæ ¼å¼
        import csv
        writer = csv.writer(f)
        writer.writerow([desc[0] for desc in sqlite_cursor.description])

        rows = sqlite_cursor.fetchall()
        writer.writerows(rows)

    # ä½¿ç”¨PostgreSQL COPYå‘½ä»¤
    with open(temp_file, 'r') as f:
        pg_cursor.copy_expert(
            f"COPY {table_name} FROM STDIN WITH CSV HEADER",
            f
        )

    pg_conn.commit()

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import os
    os.unlink(temp_file)
```

#### å¹¶è¡Œè¿ç§»

```python
from concurrent.futures import ThreadPoolExecutor

def migrate_parallel(sqlite_conn, pg_conn, tables, max_workers=4):
    """å¹¶è¡Œè¿ç§»å¤šä¸ªè¡¨"""
    def migrate_table(table_name):
        migrate_table_full(sqlite_conn, pg_conn, table_name)
        return table_name

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(migrate_table, table) for table in tables]

        for future in futures:
            table_name = future.result()
            print(f"Completed: {table_name}")
```

---

## ä¸‰ã€å¢é‡è¿ç§»ç­–ç•¥

### 3.1 å¢é‡è¿ç§»æ¶æ„

#### å¢é‡è¿ç§»æ¶æ„è®¾è®¡

```text
å¢é‡è¿ç§»æ¶æ„ = {
    å˜æ›´æ•è·å±‚: æ•è·SQLiteå˜æ›´
        - è§¦å‘å™¨æ–¹å¼
        - æ—¥å¿—è§£ææ–¹å¼
        - åº”ç”¨å±‚æ–¹å¼

    å˜æ›´é˜Ÿåˆ—: å­˜å‚¨å¾…åŒæ­¥å˜æ›´
        - å†…å­˜é˜Ÿåˆ—
        - æŒä¹…åŒ–é˜Ÿåˆ—

    åŒæ­¥å±‚: åº”ç”¨å˜æ›´åˆ°PostgreSQL
        - å®æ—¶åŒæ­¥
        - æ‰¹é‡åŒæ­¥
        - å»¶è¿ŸåŒæ­¥
}
```

### 3.2 å˜æ›´æ•è·æœºåˆ¶

#### ç­–ç•¥1ï¼šè§¦å‘å™¨æ–¹å¼

**SQLiteå˜æ›´è§¦å‘å™¨**ï¼š

```sql
-- ä¸ºæ¯ä¸ªè¡¨åˆ›å»ºå˜æ›´æ—¥å¿—è¡¨
CREATE TABLE change_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    table_name TEXT NOT NULL,
    operation TEXT NOT NULL,  -- INSERT, UPDATE, DELETE
    row_id INTEGER NOT NULL,
    old_data TEXT,  -- JSONæ ¼å¼
    new_data TEXT,  -- JSONæ ¼å¼
    timestamp INTEGER DEFAULT (strftime('%s', 'now'))
);

-- åˆ›å»ºå˜æ›´è§¦å‘å™¨
CREATE TRIGGER users_insert_trigger
AFTER INSERT ON users
BEGIN
    INSERT INTO change_log(table_name, operation, row_id, new_data)
    VALUES ('users', 'INSERT', NEW.id, json_object(
        'id', NEW.id,
        'name', NEW.name,
        'email', NEW.email
    ));
END;

CREATE TRIGGER users_update_trigger
AFTER UPDATE ON users
BEGIN
    INSERT INTO change_log(table_name, operation, row_id, old_data, new_data)
    VALUES ('users', 'UPDATE', NEW.id,
        json_object('id', OLD.id, 'name', OLD.name, 'email', OLD.email),
        json_object('id', NEW.id, 'name', NEW.name, 'email', NEW.email)
    );
END;

CREATE TRIGGER users_delete_trigger
AFTER DELETE ON users
BEGIN
    INSERT INTO change_log(table_name, operation, row_id, old_data)
    VALUES ('users', 'DELETE', OLD.id,
        json_object('id', OLD.id, 'name', OLD.name, 'email', OLD.email)
    );
END;
```

**å˜æ›´æ•è·å®ç°**ï¼š

```python
def capture_changes(sqlite_conn, change_log_table='change_log'):
    """æ•è·å˜æ›´"""
    cursor = sqlite_conn.cursor()

    # è¯»å–æœªå¤„ç†çš„å˜æ›´
    cursor.execute(f"""
        SELECT id, table_name, operation, row_id, old_data, new_data, timestamp
        FROM {change_log_table}
        WHERE processed = 0
        ORDER BY timestamp, id
    """)

    changes = []
    for row in cursor.fetchall():
        changes.append({
            'id': row[0],
            'table_name': row[1],
            'operation': row[2],
            'row_id': row[3],
            'old_data': json.loads(row[4]) if row[4] else None,
            'new_data': json.loads(row[5]) if row[5] else None,
            'timestamp': row[6]
        })

    return changes
```

#### ç­–ç•¥2ï¼šWALæ—¥å¿—è§£æ

**WALæ–‡ä»¶è§£æ**ï¼š

```python
import struct

def parse_wal_file(wal_path):
    """è§£æWALæ–‡ä»¶"""
    with open(wal_path, 'rb') as f:
        # è¯»å–WALå¤´ï¼ˆ32å­—èŠ‚ï¼‰
        wal_header = f.read(32)
        magic, version, page_size, checkpoint_seq, salt1, salt2, checksum1, checksum2 = \
            struct.unpack('>IIIIIIII', wal_header)

        changes = []

        # è¯»å–WALå¸§
        while True:
            # è¯»å–å¸§å¤´ï¼ˆ24å­—èŠ‚ï¼‰
            frame_header = f.read(24)
            if len(frame_header) < 24:
                break

            page_number, db_size, salt1, salt2, checksum1, checksum2 = \
                struct.unpack('>IIIIII', frame_header)

            # è¯»å–é¡µæ•°æ®
            page_data = f.read(page_size)

            # è§£æé¡µä¸­çš„å˜æ›´
            changes.extend(parse_page_changes(page_data, page_number))

        return changes
```

#### ç­–ç•¥3ï¼šåº”ç”¨å±‚å˜æ›´æ•è·

**åº”ç”¨å±‚å®ç°**ï¼š

```python
class ChangeCapture:
    def __init__(self, sqlite_conn, change_queue):
        self.sqlite_conn = sqlite_conn
        self.change_queue = change_queue

    def execute_with_capture(self, table_name, operation, sql, params):
        """æ‰§è¡ŒSQLå¹¶æ•è·å˜æ›´"""
        cursor = self.sqlite_conn.cursor()

        if operation == 'INSERT':
            cursor.execute(sql, params)
            new_id = cursor.lastrowid
            # æ•è·å˜æ›´
            self.change_queue.put({
                'table_name': table_name,
                'operation': 'INSERT',
                'row_id': new_id,
                'data': self._get_row_data(table_name, new_id)
            })

        elif operation == 'UPDATE':
            # è·å–æ—§æ•°æ®
            old_data = self._get_row_data(table_name, params[0])
            cursor.execute(sql, params)
            # è·å–æ–°æ•°æ®
            new_data = self._get_row_data(table_name, params[0])
            # æ•è·å˜æ›´
            self.change_queue.put({
                'table_name': table_name,
                'operation': 'UPDATE',
                'row_id': params[0],
                'old_data': old_data,
                'new_data': new_data
            })

        elif operation == 'DELETE':
            # è·å–æ—§æ•°æ®
            old_data = self._get_row_data(table_name, params[0])
            cursor.execute(sql, params)
            # æ•è·å˜æ›´
            self.change_queue.put({
                'table_name': table_name,
                'operation': 'DELETE',
                'row_id': params[0],
                'old_data': old_data
            })

        self.sqlite_conn.commit()
```

### 3.3 å¢é‡åŒæ­¥ç­–ç•¥

#### å®æ—¶åŒæ­¥

```python
def sync_realtime(changes, pg_conn):
    """å®æ—¶åŒæ­¥å˜æ›´"""
    pg_cursor = pg_conn.cursor()

    for change in changes:
        table_name = change['table_name']
        operation = change['operation']
        row_id = change['row_id']

        if operation == 'INSERT':
            data = change['new_data']
            columns = ','.join(data.keys())
            values = ','.join(['%s'] * len(data))
            sql = f"INSERT INTO {table_name} ({columns}) VALUES ({values})"
            pg_cursor.execute(sql, list(data.values()))

        elif operation == 'UPDATE':
            data = change['new_data']
            set_clause = ','.join([f"{k} = %s" for k in data.keys()])
            sql = f"UPDATE {table_name} SET {set_clause} WHERE id = %s"
            pg_cursor.execute(sql, list(data.values()) + [row_id])

        elif operation == 'DELETE':
            sql = f"DELETE FROM {table_name} WHERE id = %s"
            pg_cursor.execute(sql, (row_id,))

        pg_conn.commit()
```

#### æ‰¹é‡åŒæ­¥

```python
def sync_batch(changes, pg_conn, batch_size=100):
    """æ‰¹é‡åŒæ­¥å˜æ›´"""
    pg_cursor = pg_conn.cursor()

    # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
    inserts = [c for c in changes if c['operation'] == 'INSERT']
    updates = [c for c in changes if c['operation'] == 'UPDATE']
    deletes = [c for c in changes if c['operation'] == 'DELETE']

    # æ‰¹é‡æ’å…¥
    if inserts:
        # æŒ‰è¡¨åˆ†ç»„
        by_table = {}
        for change in inserts:
            table = change['table_name']
            if table not in by_table:
                by_table[table] = []
            by_table[table].append(change)

        for table, table_changes in by_table.items():
            for i in range(0, len(table_changes), batch_size):
                batch = table_changes[i:i+batch_size]
                data_list = [c['new_data'] for c in batch]

                if data_list:
                    columns = ','.join(data_list[0].keys())
                    values = ','.join(['%s'] * len(data_list[0]))
                    sql = f"INSERT INTO {table} ({columns}) VALUES ({values})"
                    pg_cursor.executemany(sql, [list(d.values()) for d in data_list])

    # æ‰¹é‡æ›´æ–°
    if updates:
        for change in updates:
            table_name = change['table_name']
            data = change['new_data']
            row_id = change['row_id']
            set_clause = ','.join([f"{k} = %s" for k in data.keys()])
            sql = f"UPDATE {table_name} SET {set_clause} WHERE id = %s"
            pg_cursor.execute(sql, list(data.values()) + [row_id])

    # æ‰¹é‡åˆ é™¤
    if deletes:
        for change in deletes:
            table_name = change['table_name']
            row_id = change['row_id']
            sql = f"DELETE FROM {table_name} WHERE id = %s"
            pg_cursor.execute(sql, (row_id,))

    pg_conn.commit()
```

### 3.4 å†²çªè§£å†³æœºåˆ¶

#### å†²çªæ£€æµ‹

```python
def detect_conflicts(change, pg_conn):
    """æ£€æµ‹å†²çª"""
    pg_cursor = pg_conn.cursor()
    table_name = change['table_name']
    row_id = change['row_id']
    operation = change['operation']

    if operation == 'UPDATE':
        # æ£€æŸ¥PostgreSQLä¸­çš„å½“å‰å€¼
        pg_cursor.execute(f"SELECT * FROM {table_name} WHERE id = %s", (row_id,))
        pg_row = pg_cursor.fetchone()

        if pg_row:
            # æ¯”è¾ƒæ—§æ•°æ®
            old_data = change['old_data']
            pg_data = dict(zip([desc[0] for desc in pg_cursor.description], pg_row))

            # æ£€æŸ¥æ˜¯å¦æœ‰å†²çª
            for key, old_value in old_data.items():
                if key in pg_data and pg_data[key] != old_value:
                    return True, f"Conflict in column {key}"

    return False, None
```

#### å†²çªè§£å†³ç­–ç•¥

```python
def resolve_conflict(change, pg_conn, strategy='last_write_wins'):
    """è§£å†³å†²çª"""
    if strategy == 'last_write_wins':
        # æœ€åå†™å…¥è·èƒœ
        sync_realtime([change], pg_conn)

    elif strategy == 'first_write_wins':
        # é¦–æ¬¡å†™å…¥è·èƒœï¼ˆå¿½ç•¥å†²çªï¼‰
        pass

    elif strategy == 'merge':
        # åˆå¹¶ç­–ç•¥
        pg_cursor = pg_conn.cursor()
        table_name = change['table_name']
        row_id = change['row_id']

        # è·å–PostgreSQLå½“å‰å€¼
        pg_cursor.execute(f"SELECT * FROM {table_name} WHERE id = %s", (row_id,))
        pg_row = pg_cursor.fetchone()
        pg_data = dict(zip([desc[0] for desc in pg_cursor.description], pg_row))

        # åˆå¹¶æ–°æ•°æ®
        new_data = change['new_data']
        merged_data = {**pg_data, **new_data}

        # æ›´æ–°
        set_clause = ','.join([f"{k} = %s" for k in merged_data.keys()])
        sql = f"UPDATE {table_name} SET {set_clause} WHERE id = %s"
        pg_cursor.execute(sql, list(merged_data.values()) + [row_id])
        pg_conn.commit()
```

---

## å››ã€æ•°æ®ä¸€è‡´æ€§ä¿è¯

### 4.1 äº‹åŠ¡ä¸€è‡´æ€§

#### äº‹åŠ¡è¾¹ç•Œå¤„ç†

```python
def migrate_with_transaction(sqlite_conn, pg_conn, migration_func):
    """åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œè¿ç§»"""
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    try:
        # å¼€å§‹äº‹åŠ¡
        sqlite_cursor.execute("BEGIN TRANSACTION")

        # æ‰§è¡Œè¿ç§»
        result = migration_func(sqlite_cursor, pg_cursor)

        # æäº¤
        sqlite_conn.commit()
        pg_conn.commit()

        return result

    except Exception as e:
        # å›æ»š
        sqlite_conn.rollback()
        pg_conn.rollback()
        raise e
```

### 4.2 æ•°æ®æ ¡éªŒæœºåˆ¶

#### æ ¡éªŒå’ŒéªŒè¯

```python
def verify_migration_checksum(sqlite_conn, pg_conn, table_name):
    """éªŒè¯è¿ç§»æ ¡éªŒå’Œ"""
    # è®¡ç®—SQLiteæ ¡éªŒå’Œ
    sqlite_cursor = sqlite_conn.cursor()
    sqlite_cursor.execute(f"SELECT * FROM {table_name} ORDER BY id")
    sqlite_data = sqlite_cursor.fetchall()
    sqlite_checksum = hashlib.md5(str(sqlite_data).encode()).hexdigest()

    # è®¡ç®—PostgreSQLæ ¡éªŒå’Œ
    pg_cursor = pg_conn.cursor()
    pg_cursor.execute(f"SELECT * FROM {table_name} ORDER BY id")
    pg_data = pg_cursor.fetchall()
    pg_checksum = hashlib.md5(str(pg_data).encode()).hexdigest()

    return sqlite_checksum == pg_checksum
```

### 4.3 å›æ»šç­–ç•¥

```python
def rollback_migration(pg_conn, table_name):
    """å›æ»šè¿ç§»"""
    pg_cursor = pg_conn.cursor()

    # åˆ é™¤è¡¨
    pg_cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
    pg_conn.commit()

    print(f"Rolled back migration for {table_name}")
```

---

## äº”ã€è¿ç§»å·¥å…·å®ç°

### 5.1 äºŒè¿›åˆ¶å¤åˆ¶å·¥å…·

```python
class BinaryCopyTool:
    def __init__(self, sqlite_path, pg_conn_string):
        self.sqlite_conn = sqlite3.connect(sqlite_path)
        self.pg_conn = psycopg2.connect(pg_conn_string)

    def copy_table_binary(self, table_name, blob_columns):
        """äºŒè¿›åˆ¶å¤åˆ¶è¡¨"""
        # å®ç°äºŒè¿›åˆ¶å¤åˆ¶é€»è¾‘
        pass

    def copy_blob_column(self, table_name, column_name):
        """å¤åˆ¶BLOBåˆ—"""
        # å®ç°BLOBå¤åˆ¶é€»è¾‘
        pass
```

### 5.2 å¢é‡åŒæ­¥å·¥å…·

```python
class IncrementalSyncTool:
    def __init__(self, sqlite_conn, pg_conn):
        self.sqlite_conn = sqlite_conn
        self.pg_conn = pg_conn
        self.change_queue = queue.Queue()

    def start_sync(self):
        """å¯åŠ¨å¢é‡åŒæ­¥"""
        # å¯åŠ¨å˜æ›´æ•è·çº¿ç¨‹
        # å¯åŠ¨åŒæ­¥çº¿ç¨‹
        pass

    def stop_sync(self):
        """åœæ­¢å¢é‡åŒæ­¥"""
        # åœæ­¢æ‰€æœ‰çº¿ç¨‹
        pass
```

### 5.3 ç›‘æ§ä¸æŠ¥å‘Š

```python
class MigrationMonitor:
    def __init__(self):
        self.stats = {
            'tables_migrated': 0,
            'rows_migrated': 0,
            'errors': [],
            'start_time': None,
            'end_time': None
        }

    def report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        print(f"Tables migrated: {self.stats['tables_migrated']}")
        print(f"Rows migrated: {self.stats['rows_migrated']}")
        print(f"Errors: {len(self.stats['errors'])}")
        if self.stats['start_time'] and self.stats['end_time']:
            duration = self.stats['end_time'] - self.stats['start_time']
            print(f"Duration: {duration}")
```

---

## å…­ã€å®æˆ˜æ¡ˆä¾‹

### 6.1 å¤§å‹æ•°æ®åº“å…¨é‡è¿ç§»

**åœºæ™¯**ï¼šè¿ç§»100GB SQLiteæ•°æ®åº“

**ç­–ç•¥**ï¼š

1. åˆ†è¡¨è¿ç§»
2. å¹¶è¡Œå¤„ç†
3. æµå¼ä¼ è¾“å¤§BLOB

### 6.2 åœ¨çº¿å¢é‡è¿ç§»

**åœºæ™¯**ï¼šé›¶åœæœºè¿ç§»

**ç­–ç•¥**ï¼š

1. å…¨é‡è¿ç§» + å¢é‡åŒæ­¥
2. åŒå†™æ¨¡å¼
3. åˆ‡æ¢éªŒè¯

### 6.3 é›¶åœæœºè¿ç§»

**åœºæ™¯**ï¼šç”Ÿäº§ç¯å¢ƒè¿ç§»

**ç­–ç•¥**ï¼š

1. å…¨é‡è¿ç§»
2. å¢é‡åŒæ­¥
3. æµé‡åˆ‡æ¢
4. å›æ»šå‡†å¤‡

---

## ğŸ”— ç›¸å…³èµ„æº

- [10.01 SQLiteåˆ°PostgreSQLè¿ç§»æŒ‡å—](./10.01-SQLiteåˆ°PostgreSQLè¿ç§»æŒ‡å—.md)
- [10.02 SQLå…¼å®¹æ€§æ·±åº¦åˆ†æ](./10.02-SQLå…¼å®¹æ€§æ·±åº¦åˆ†æ.md)
- [10.04 æ•°æ®è¿ç§»ä¸è¯­ä¹‰æ¨¡å‹å…¼å®¹æ€§](./10.04-æ•°æ®è¿ç§»ä¸è¯­ä¹‰æ¨¡å‹å…¼å®¹æ€§.md)

---

## ğŸ”— äº¤å‰å¼•ç”¨

### ç†è®ºæ¨¡å‹ ğŸ†•

- â­â­ [å­˜å‚¨ç†è®º](../11-ç†è®ºæ¨¡å‹/11.05-å­˜å‚¨ç†è®º.md) - äºŒè¿›åˆ¶æ•°æ®å­˜å‚¨ç†è®ºã€å¢é‡è¿ç§»ç†è®º
- â­ [ç®—æ³•å¤æ‚åº¦ç†è®º](../11-ç†è®ºæ¨¡å‹/11.03-ç®—æ³•å¤æ‚åº¦ç†è®º.md) - è¿ç§»ç®—æ³•å¤æ‚åº¦

### è®¾è®¡æ¨¡å‹ ğŸ†•

- â­â­ [è®¾è®¡å†³ç­–](../12-è®¾è®¡æ¨¡å‹/12.04-è®¾è®¡å†³ç­–.md) - è¿ç§»ç­–ç•¥å†³ç­–ã€å¢é‡è¿ç§»å†³ç­–
- â­ [è®¾è®¡æ¨¡å¼](../12-è®¾è®¡æ¨¡å‹/12.03-è®¾è®¡æ¨¡å¼.md) - è¿ç§»æ¨¡å¼

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [SQLiteæ–‡ä»¶æ ¼å¼](https://www.sqlite.org/fileformat.html)
- [PostgreSQL TOAST](https://www.postgresql.org/docs/current/storage-toast.html)
- [PostgreSQL COPYå‘½ä»¤](https://www.postgresql.org/docs/current/sql-copy.html)

---

**ç»´æŠ¤è€…**ï¼šData-Science Team
**æœ€åæ›´æ–°**ï¼š2025-01-15
