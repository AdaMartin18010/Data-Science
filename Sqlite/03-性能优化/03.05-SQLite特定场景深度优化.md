# SQLiteç‰¹å®šåœºæ™¯æ·±åº¦ä¼˜åŒ–

> **åˆ›å»ºæ—¥æœŸ**: 2025-12-05
> **ç‰ˆæœ¬**: SQLite 3.47.x
> **éš¾åº¦**: â­â­â­â­â­
> **é€‚ç”¨åœºæ™¯**: ç‰¹å®šä¸šåŠ¡åœºæ™¯çš„æ€§èƒ½æè‡´ä¼˜åŒ–

---

## ğŸ“‘ ç›®å½•

- [SQLiteç‰¹å®šåœºæ™¯æ·±åº¦ä¼˜åŒ–](#sqliteç‰¹å®šåœºæ™¯æ·±åº¦ä¼˜åŒ–)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¼˜åŒ–åœºæ™¯æ€»è§ˆ](#ä¼˜åŒ–åœºæ™¯æ€»è§ˆ)
  - [1. æ—¶é—´åºåˆ—æ•°æ®ä¼˜åŒ–](#1-æ—¶é—´åºåˆ—æ•°æ®ä¼˜åŒ–)
    - [1.1 åˆ†åŒºè¡¨è®¾è®¡](#11-åˆ†åŒºè¡¨è®¾è®¡)
    - [1.2 æ—¶é—´ç´¢å¼•ä¼˜åŒ–](#12-æ—¶é—´ç´¢å¼•ä¼˜åŒ–)
    - [1.3 æ•°æ®å½’æ¡£ç­–ç•¥](#13-æ•°æ®å½’æ¡£ç­–ç•¥)
  - [2. å…¨æ–‡æœç´¢ä¼˜åŒ–](#2-å…¨æ–‡æœç´¢ä¼˜åŒ–)
    - [2.1 FTS5é…ç½®ä¼˜åŒ–](#21-fts5é…ç½®ä¼˜åŒ–)
    - [2.2 æœç´¢æ€§èƒ½ä¼˜åŒ–](#22-æœç´¢æ€§èƒ½ä¼˜åŒ–)
  - [3. æ—¥å¿—æ•°æ®ä¼˜åŒ–](#3-æ—¥å¿—æ•°æ®ä¼˜åŒ–)
    - [3.1 è¿½åŠ å†™å…¥ä¼˜åŒ–](#31-è¿½åŠ å†™å…¥ä¼˜åŒ–)
    - [3.2 æ—¥å¿—è½®è½¬ç­–ç•¥](#32-æ—¥å¿—è½®è½¬ç­–ç•¥)
  - [4. ç¼“å­˜æ•°æ®ä¼˜åŒ–](#4-ç¼“å­˜æ•°æ®ä¼˜åŒ–)
    - [4.1 å†…å­˜æ•°æ®åº“](#41-å†…å­˜æ•°æ®åº“)
    - [4.2 ç¼“å­˜æ·˜æ±°ç­–ç•¥](#42-ç¼“å­˜æ·˜æ±°ç­–ç•¥)
  - [5. åˆ†ææŸ¥è¯¢ä¼˜åŒ–](#5-åˆ†ææŸ¥è¯¢ä¼˜åŒ–)
    - [5.1 ç‰©åŒ–è§†å›¾](#51-ç‰©åŒ–è§†å›¾)
    - [5.2 é¢„èšåˆè¡¨](#52-é¢„èšåˆè¡¨)
  - [6. æ‰¹é‡å¯¼å…¥ä¼˜åŒ–](#6-æ‰¹é‡å¯¼å…¥ä¼˜åŒ–)
    - [6.1 æ‰¹é‡æ’å…¥ç­–ç•¥](#61-æ‰¹é‡æ’å…¥ç­–ç•¥)
    - [6.2 å¯¼å…¥æ€§èƒ½ä¼˜åŒ–](#62-å¯¼å…¥æ€§èƒ½ä¼˜åŒ–)
  - [7. ğŸ”— ç›¸å…³èµ„æº](#7--ç›¸å…³èµ„æº)
    - [å†…éƒ¨èµ„æº](#å†…éƒ¨èµ„æº)
    - [å¤–éƒ¨èµ„æº](#å¤–éƒ¨èµ„æº)

---

## ä¼˜åŒ–åœºæ™¯æ€»è§ˆ

```text
ç‰¹å®šåœºæ™¯ä¼˜åŒ–çŸ©é˜µ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åœºæ™¯                 â”‚ ä¸»è¦æŒ‘æˆ˜     â”‚ ä¼˜åŒ–ç­–ç•¥      â”‚ æ€§èƒ½æå‡      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ—¶é—´åºåˆ—æ•°æ®         â”‚ æ•°æ®é‡å¤§      â”‚ åˆ†åŒº+å½’æ¡£     â”‚ 10-100å€     â”‚
â”‚                     â”‚ æŸ¥è¯¢æ…¢        â”‚ æ—¶é—´ç´¢å¼•     â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å…¨æ–‡æœç´¢             â”‚ æœç´¢æ…¢       â”‚ FTS5ä¼˜åŒ–      â”‚ 5-20å€       â”‚
â”‚                     â”‚ ç´¢å¼•å¤§        â”‚ åˆ†è¯ä¼˜åŒ–     â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ—¥å¿—æ•°æ®             â”‚ å†™å…¥é¢‘ç¹     â”‚ è¿½åŠ å†™å…¥      â”‚ 50-200å€     â”‚
â”‚                     â”‚ æ–‡ä»¶å¢é•¿å¿«    â”‚ WALæ¨¡å¼      â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¼“å­˜æ•°æ®             â”‚ å†…å­˜é™åˆ¶     â”‚ å†…å­˜æ•°æ®åº“    â”‚ 100-1000å€   â”‚
â”‚                     â”‚ æ·˜æ±°ç­–ç•¥      â”‚ LRUç­–ç•¥      â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åˆ†ææŸ¥è¯¢             â”‚ èšåˆæ…¢       â”‚ ç‰©åŒ–è§†å›¾      â”‚ 10-50å€      â”‚
â”‚                     â”‚ å®æ—¶æ€§è¦æ±‚    â”‚ é¢„èšåˆè¡¨      â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ‰¹é‡å¯¼å…¥             â”‚ å¯¼å…¥æ…¢       â”‚ æ‰¹é‡äº‹åŠ¡      â”‚ 100-1000å€   â”‚
â”‚                     â”‚ é”ç«äº‰        â”‚ ä¼˜åŒ–é…ç½®     â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. æ—¶é—´åºåˆ—æ•°æ®ä¼˜åŒ–

### 1.1 åˆ†åŒºè¡¨è®¾è®¡

```python
import sqlite3
from datetime import datetime, timedelta

class TimeSeriesPartition:
    def __init__(self, db_path: str, table_prefix: str = 'metrics'):
        self.db_path = db_path
        self.table_prefix = table_prefix
        self.conn = sqlite3.connect(db_path)
        self.setup_partitions()

    def setup_partitions(self):
        """è®¾ç½®åˆ†åŒºè¡¨"""
        cursor = self.conn.cursor()

        # åˆ›å»ºä¸»è¡¨ï¼ˆè§†å›¾ï¼‰
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp INTEGER NOT NULL,
                metric_name TEXT NOT NULL,
                value REAL NOT NULL,
                tags TEXT
            )
        """)

        # åˆ›å»ºæ—¶é—´ç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_metrics_time
            ON metrics(timestamp, metric_name)
        """)

        self.conn.commit()

    def get_partition_table(self, timestamp: int) -> str:
        """æ ¹æ®æ—¶é—´æˆ³è·å–åˆ†åŒºè¡¨å"""
        dt = datetime.fromtimestamp(timestamp)
        # æŒ‰æœˆåˆ†åŒº
        partition_name = f"{self.table_prefix}_{dt.strftime('%Y%m')}"
        return partition_name

    def ensure_partition(self, timestamp: int):
        """ç¡®ä¿åˆ†åŒºè¡¨å­˜åœ¨"""
        partition_name = self.get_partition_table(timestamp)
        cursor = self.conn.cursor()

        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {partition_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp INTEGER NOT NULL,
                metric_name TEXT NOT NULL,
                value REAL NOT NULL,
                tags TEXT
            )
        """)

        cursor.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_{partition_name}_time
            ON {partition_name}(timestamp, metric_name)
        """)

        self.conn.commit()

    def insert_metric(self, timestamp: int, metric_name: str, value: float, tags: str = None):
        """æ’å…¥æŒ‡æ ‡æ•°æ®"""
        partition_name = self.get_partition_table(timestamp)
        self.ensure_partition(partition_name)

        cursor = self.conn.cursor()
        cursor.execute(f"""
            INSERT INTO {partition_name} (timestamp, metric_name, value, tags)
            VALUES (?, ?, ?, ?)
        """, (timestamp, metric_name, value, tags))

        self.conn.commit()

    def query_range(self, start_time: int, end_time: int, metric_name: str = None):
        """æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ•°æ®"""
        results = []
        cursor = self.conn.cursor()

        # è·å–æ‰€æœ‰æ¶‰åŠçš„åˆ†åŒº
        start_dt = datetime.fromtimestamp(start_time)
        end_dt = datetime.fromtimestamp(end_time)

        current = start_dt.replace(day=1)
        while current <= end_dt:
            partition_name = f"{self.table_prefix}_{current.strftime('%Y%m')}"

            query = f"""
                SELECT * FROM {partition_name}
                WHERE timestamp >= ? AND timestamp <= ?
            """
            params = [start_time, end_time]

            if metric_name:
                query += " AND metric_name = ?"
                params.append(metric_name)

            cursor.execute(query, params)
            results.extend(cursor.fetchall())

            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœˆ
            if current.month == 12:
                current = current.replace(year=current.year + 1, month=1)
            else:
                current = current.replace(month=current.month + 1)

        return results
```

### 1.2 æ—¶é—´ç´¢å¼•ä¼˜åŒ–

```python
class TimeIndexOptimizer:
    def __init__(self, conn):
        self.conn = conn

    def create_optimized_time_index(self, table_name: str, time_column: str):
        """åˆ›å»ºä¼˜åŒ–çš„æ—¶é—´ç´¢å¼•"""
        cursor = self.conn.cursor()

        # ä½¿ç”¨è¦†ç›–ç´¢å¼•
        cursor.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_{table_name}_time_covering
            ON {table_name}({time_column}, metric_name)
            INCLUDE (value, tags)
        """)

        # åˆ›å»ºéƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æœ€è¿‘çš„æ•°æ®ï¼‰
        cursor.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_{table_name}_recent
            ON {table_name}({time_column})
            WHERE {time_column} > strftime('%s', 'now', '-30 days')
        """)

        self.conn.commit()
```

### 1.3 æ•°æ®å½’æ¡£ç­–ç•¥

```python
class TimeSeriesArchiver:
    def __init__(self, db_path: str, archive_path: str):
        self.db_path = db_path
        self.archive_path = archive_path
        self.conn = sqlite3.connect(db_path)

    def archive_old_data(self, days_to_keep: int = 90):
        """å½’æ¡£æ—§æ•°æ®"""
        cutoff_time = int((datetime.now() - timedelta(days=days_to_keep)).timestamp())

        cursor = self.conn.cursor()

        # è·å–æ‰€æœ‰åˆ†åŒº
        cursor.execute("""
            SELECT name FROM sqlite_master
            WHERE type='table' AND name LIKE 'metrics_%'
        """)

        partitions = cursor.fetchall()

        for (partition_name,) in partitions:
            # æ£€æŸ¥åˆ†åŒºä¸­æœ€æ—§çš„æ•°æ®
            cursor.execute(f"""
                SELECT MIN(timestamp) FROM {partition_name}
            """)
            min_time = cursor.fetchone()[0]

            if min_time and min_time < cutoff_time:
                # å½’æ¡£æ•´ä¸ªåˆ†åŒº
                self._archive_partition(partition_name, cutoff_time)

    def _archive_partition(self, partition_name: str, cutoff_time: int):
        """å½’æ¡£åˆ†åŒº"""
        # å¯¼å‡ºåˆ°å½’æ¡£æ•°æ®åº“
        archive_conn = sqlite3.connect(self.archive_path)
        cursor = self.conn.cursor()
        archive_cursor = archive_conn.cursor()

        # åˆ›å»ºå½’æ¡£è¡¨
        archive_cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {partition_name} AS
            SELECT * FROM {partition_name}
            WHERE timestamp < ?
        """, (cutoff_time,))

        # åˆ é™¤åŸè¡¨ä¸­çš„æ—§æ•°æ®
        cursor.execute(f"""
            DELETE FROM {partition_name}
            WHERE timestamp < ?
        """, (cutoff_time,))

        archive_conn.commit()
        self.conn.commit()
        archive_conn.close()
```

---

## 2. å…¨æ–‡æœç´¢ä¼˜åŒ–

### 2.1 FTS5é…ç½®ä¼˜åŒ–

```python
class FTS5Optimizer:
    def __init__(self, conn):
        self.conn = conn

    def create_optimized_fts5(self, table_name: str):
        """åˆ›å»ºä¼˜åŒ–çš„FTS5è¡¨"""
        cursor = self.conn.cursor()

        # åˆ›å»ºFTS5è™šæ‹Ÿè¡¨
        cursor.execute(f"""
            CREATE VIRTUAL TABLE IF NOT EXISTS {table_name}_fts USING fts5(
                title,
                content,
                tokenize='porter unicode61',
                content='{table_name}',
                content_rowid='id'
            )
        """)

        # åˆ›å»ºè§¦å‘å™¨ä¿æŒåŒæ­¥
        cursor.execute(f"""
            CREATE TRIGGER IF NOT EXISTS {table_name}_fts_insert AFTER INSERT ON {table_name}
            BEGIN
                INSERT INTO {table_name}_fts(rowid, title, content)
                VALUES (new.id, new.title, new.content);
            END
        """)

        cursor.execute(f"""
            CREATE TRIGGER IF NOT EXISTS {table_name}_fts_update AFTER UPDATE ON {table_name}
            BEGIN
                UPDATE {table_name}_fts SET title = new.title, content = new.content
                WHERE rowid = new.id;
            END
        """)

        cursor.execute(f"""
            CREATE TRIGGER IF NOT EXISTS {table_name}_fts_delete AFTER DELETE ON {table_name}
            BEGIN
                DELETE FROM {table_name}_fts WHERE rowid = old.id;
            END
        """)

        self.conn.commit()

    def optimize_fts5(self, table_name: str):
        """ä¼˜åŒ–FTS5ç´¢å¼•"""
        cursor = self.conn.cursor()

        # åˆå¹¶FTS5æ®µ
        cursor.execute(f"INSERT INTO {table_name}_fts({table_name}_fts) VALUES('merge', 4)")

        # ä¼˜åŒ–FTS5ç´¢å¼•
        cursor.execute(f"INSERT INTO {table_name}_fts({table_name}_fts) VALUES('optimize')")

        self.conn.commit()
```

### 2.2 æœç´¢æ€§èƒ½ä¼˜åŒ–

```python
class SearchOptimizer:
    def __init__(self, conn):
        self.conn = conn

    def optimized_search(self, query: str, limit: int = 20):
        """ä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢"""
        cursor = self.conn.cursor()

        # ä½¿ç”¨BM25æ’åº
        cursor.execute("""
            SELECT
                rowid,
                title,
                content,
                bm25(documents_fts) AS rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """, (query, limit))

        return cursor.fetchall()

    def phrase_search(self, phrase: str):
        """çŸ­è¯­æœç´¢"""
        cursor = self.conn.cursor()

        # ä½¿ç”¨å¼•å·è¿›è¡ŒçŸ­è¯­æœç´¢
        cursor.execute("""
            SELECT * FROM documents_fts
            WHERE documents_fts MATCH ?
        """, (f'"{phrase}"',))

        return cursor.fetchall()
```

---

## 3. æ—¥å¿—æ•°æ®ä¼˜åŒ–

### 3.1 è¿½åŠ å†™å…¥ä¼˜åŒ–

```python
class LogWriter:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.setup_optimizations()

    def setup_optimizations(self):
        """è®¾ç½®å†™å…¥ä¼˜åŒ–"""
        cursor = self.conn.cursor()

        # WALæ¨¡å¼ï¼ˆæœ€ä½³å†™å…¥æ€§èƒ½ï¼‰
        cursor.execute("PRAGMA journal_mode = WAL")

        # é™ä½åŒæ­¥çº§åˆ«ï¼ˆæé«˜å†™å…¥é€Ÿåº¦ï¼‰
        cursor.execute("PRAGMA synchronous = NORMAL")

        # å¢å¤§é¡µé¢å¤§å°
        cursor.execute("PRAGMA page_size = 4096")

        # å¢å¤§ç¼“å­˜
        cursor.execute("PRAGMA cache_size = -100000")  # 100MB

        self.conn.commit()

    def batch_write_logs(self, logs: List[Dict]):
        """æ‰¹é‡å†™å…¥æ—¥å¿—"""
        cursor = self.conn.cursor()

        # ä½¿ç”¨executemanyæ‰¹é‡æ’å…¥
        cursor.executemany("""
            INSERT INTO logs (timestamp, level, message, metadata)
            VALUES (?, ?, ?, ?)
        """, [
            (log['timestamp'], log['level'], log['message'], log.get('metadata'))
            for log in logs
        ])

        self.conn.commit()
```

### 3.2 æ—¥å¿—è½®è½¬ç­–ç•¥

```python
class LogRotator:
    def __init__(self, db_path: str, max_size_mb: int = 100):
        self.db_path = db_path
        self.max_size_mb = max_size_mb

    def should_rotate(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è½®è½¬"""
        file_size = Path(self.db_path).stat().st_size
        max_size_bytes = self.max_size_mb * 1024 * 1024
        return file_size > max_size_bytes

    def rotate_logs(self):
        """è½®è½¬æ—¥å¿—"""
        if not self.should_rotate():
            return

        # å½’æ¡£æ—§æ—¥å¿—
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_path = f"{self.db_path}.{timestamp}"

        shutil.move(self.db_path, archive_path)

        # åˆ›å»ºæ–°æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp INTEGER NOT NULL,
                level TEXT NOT NULL,
                message TEXT NOT NULL,
                metadata TEXT
            )
        """)
        conn.close()
```

---

## 4. ç¼“å­˜æ•°æ®ä¼˜åŒ–

### 4.1 å†…å­˜æ•°æ®åº“

```python
class InMemoryCache:
    def __init__(self):
        self.conn = sqlite3.connect(':memory:')
        self.setup_cache()

    def setup_cache(self):
        """è®¾ç½®ç¼“å­˜è¡¨"""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE cache (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL,
                expires_at INTEGER,
                access_count INTEGER DEFAULT 0,
                last_accessed INTEGER
            )
        """)

        cursor.execute("""
            CREATE INDEX idx_cache_expires ON cache(expires_at)
        """)

        self.conn.commit()

    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT value, expires_at FROM cache
            WHERE key = ? AND (expires_at IS NULL OR expires_at > ?)
        """, (key, int(time.time())))

        result = cursor.fetchone()
        if result:
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            cursor.execute("""
                UPDATE cache
                SET access_count = access_count + 1,
                    last_accessed = ?
                WHERE key = ?
            """, (int(time.time()), key))
            self.conn.commit()
            return result[0]
        return None

    def set(self, key: str, value: str, ttl: int = None):
        """è®¾ç½®ç¼“å­˜"""
        cursor = self.conn.cursor()
        expires_at = int(time.time()) + ttl if ttl else None

        cursor.execute("""
            INSERT OR REPLACE INTO cache (key, value, expires_at, last_accessed)
            VALUES (?, ?, ?, ?)
        """, (key, value, expires_at, int(time.time())))

        self.conn.commit()

    def evict_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cursor = self.conn.cursor()
        cursor.execute("""
            DELETE FROM cache
            WHERE expires_at IS NOT NULL AND expires_at < ?
        """, (int(time.time()),))
        self.conn.commit()
```

### 4.2 ç¼“å­˜æ·˜æ±°ç­–ç•¥

```python
class LRUCache:
    def __init__(self, max_size: int = 1000):
        self.conn = sqlite3.connect(':memory:')
        self.max_size = max_size
        self.setup_cache()

    def setup_cache(self):
        """è®¾ç½®LRUç¼“å­˜"""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE lru_cache (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL,
                last_accessed INTEGER NOT NULL
            )
        """)

        cursor.execute("""
            CREATE INDEX idx_lru_access ON lru_cache(last_accessed)
        """)

        self.conn.commit()

    def get(self, key: str):
        """è·å–ç¼“å­˜ï¼ˆæ›´æ–°è®¿é—®æ—¶é—´ï¼‰"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT value FROM lru_cache WHERE key = ?
        """, (key,))

        result = cursor.fetchone()
        if result:
            # æ›´æ–°è®¿é—®æ—¶é—´
            cursor.execute("""
                UPDATE lru_cache SET last_accessed = ? WHERE key = ?
            """, (int(time.time()), key))
            self.conn.commit()
            return result[0]
        return None

    def set(self, key: str, value: str):
        """è®¾ç½®ç¼“å­˜ï¼ˆLRUæ·˜æ±°ï¼‰"""
        cursor = self.conn.cursor()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·˜æ±°
        cursor.execute("SELECT COUNT(*) FROM lru_cache")
        count = cursor.fetchone()[0]

        if count >= self.max_size:
            # åˆ é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹
            cursor.execute("""
                DELETE FROM lru_cache
                WHERE key = (
                    SELECT key FROM lru_cache
                    ORDER BY last_accessed ASC
                    LIMIT 1
                )
            """)

        # æ’å…¥æˆ–æ›´æ–°
        cursor.execute("""
            INSERT OR REPLACE INTO lru_cache (key, value, last_accessed)
            VALUES (?, ?, ?)
        """, (key, value, int(time.time())))

        self.conn.commit()
```

---

## 5. åˆ†ææŸ¥è¯¢ä¼˜åŒ–

### 5.1 ç‰©åŒ–è§†å›¾

```python
class MaterializedView:
    def __init__(self, conn):
        self.conn = conn

    def create_materialized_view(self, view_name: str, query: str):
        """åˆ›å»ºç‰©åŒ–è§†å›¾"""
        cursor = self.conn.cursor()

        # åˆ›å»ºç‰©åŒ–è§†å›¾è¡¨
        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {view_name} AS {query}
        """)

        # åˆ›å»ºç´¢å¼•
        cursor.execute(f"""
            CREATE INDEX IF NOT EXISTS idx_{view_name}_id
            ON {view_name}(id)
        """)

        self.conn.commit()

    def refresh_materialized_view(self, view_name: str, query: str):
        """åˆ·æ–°ç‰©åŒ–è§†å›¾"""
        cursor = self.conn.cursor()

        # åˆ é™¤æ—§æ•°æ®
        cursor.execute(f"DELETE FROM {view_name}")

        # é‡æ–°è®¡ç®—
        cursor.execute(f"INSERT INTO {view_name} {query}")

        self.conn.commit()
```

### 5.2 é¢„èšåˆè¡¨

```python
class PreAggregation:
    def __init__(self, conn):
        self.conn = conn

    def create_aggregation_table(self):
        """åˆ›å»ºé¢„èšåˆè¡¨"""
        cursor = self.conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS daily_stats (
                date TEXT PRIMARY KEY,
                total_sales REAL,
                total_orders INTEGER,
                avg_order_value REAL,
                last_updated INTEGER
            )
        """)

        self.conn.commit()

    def update_daily_stats(self, date: str):
        """æ›´æ–°æ¯æ—¥ç»Ÿè®¡"""
        cursor = self.conn.cursor()

        cursor.execute("""
            INSERT OR REPLACE INTO daily_stats
            SELECT
                date(created_at, 'unixepoch') as date,
                SUM(amount) as total_sales,
                COUNT(*) as total_orders,
                AVG(amount) as avg_order_value,
                strftime('%s', 'now') as last_updated
            FROM orders
            WHERE date(created_at, 'unixepoch') = ?
            GROUP BY date
        """, (date,))

        self.conn.commit()
```

---

## 6. æ‰¹é‡å¯¼å…¥ä¼˜åŒ–

### 6.1 æ‰¹é‡æ’å…¥ç­–ç•¥

```python
class BulkImporter:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.setup_optimizations()

    def setup_optimizations(self):
        """è®¾ç½®æ‰¹é‡å¯¼å…¥ä¼˜åŒ–"""
        cursor = self.conn.cursor()

        # WALæ¨¡å¼
        cursor.execute("PRAGMA journal_mode = WAL")

        # å…³é—­åŒæ­¥ï¼ˆæ‰¹é‡å¯¼å…¥æ—¶ï¼‰
        cursor.execute("PRAGMA synchronous = OFF")

        # å¢å¤§é¡µé¢å¤§å°
        cursor.execute("PRAGMA page_size = 65536")  # 64KB

        # å¢å¤§ç¼“å­˜
        cursor.execute("PRAGMA cache_size = -200000")  # 200MB

        # å…³é—­å¤–é”®æ£€æŸ¥
        cursor.execute("PRAGMA foreign_keys = OFF")

        self.conn.commit()

    def bulk_insert(self, table_name: str, data: List[tuple], batch_size: int = 10000):
        """æ‰¹é‡æ’å…¥"""
        cursor = self.conn.cursor()

        # å¼€å§‹äº‹åŠ¡
        cursor.execute("BEGIN TRANSACTION")

        try:
            for i in range(0, len(data), batch_size):
                batch = data[i:i+batch_size]
                placeholders = ','.join(['?'] * len(batch[0]))
                cursor.executemany(
                    f"INSERT INTO {table_name} VALUES ({placeholders})",
                    batch
                )

            cursor.execute("COMMIT")
        except Exception as e:
            cursor.execute("ROLLBACK")
            raise e

    def finish_import(self):
        """å®Œæˆå¯¼å…¥ï¼ˆæ¢å¤è®¾ç½®ï¼‰"""
        cursor = self.conn.cursor()

        # æ¢å¤åŒæ­¥
        cursor.execute("PRAGMA synchronous = NORMAL")

        # å¯ç”¨å¤–é”®æ£€æŸ¥
        cursor.execute("PRAGMA foreign_keys = ON")

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        cursor.execute("ANALYZE")

        self.conn.commit()
```

### 6.2 å¯¼å…¥æ€§èƒ½ä¼˜åŒ–

```python
class OptimizedImporter(BulkImporter):
    def parallel_import(self, table_name: str, data: List[tuple], num_threads: int = 4):
        """å¹¶è¡Œå¯¼å…¥"""
        import threading

        chunk_size = len(data) // num_threads
        threads = []

        def import_chunk(chunk, thread_id):
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("PRAGMA journal_mode = WAL")
            cursor.execute("PRAGMA synchronous = OFF")

            placeholders = ','.join(['?'] * len(chunk[0]))
            cursor.executemany(
                f"INSERT INTO {table_name} VALUES ({placeholders})",
                chunk
            )
            conn.commit()
            conn.close()

        for i in range(num_threads):
            start = i * chunk_size
            end = start + chunk_size if i < num_threads - 1 else len(data)
            chunk = data[start:end]

            thread = threading.Thread(target=import_chunk, args=(chunk, i))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()
```

---

## 7. ğŸ”— ç›¸å…³èµ„æº

### å†…éƒ¨èµ„æº

- [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](./03.02-ä¼˜åŒ–ç­–ç•¥.md) - é€šç”¨ä¼˜åŒ–ç­–ç•¥
- [æ€§èƒ½è°ƒä¼˜å†³ç­–æ ‘](./03.03-æ€§èƒ½è°ƒä¼˜å†³ç­–æ ‘.md) - ä¼˜åŒ–å†³ç­–æ ‘
- [æŸ¥è¯¢ä¼˜åŒ–](../08-ç¼–ç¨‹å®è·µ/08.03-æŸ¥è¯¢ä¼˜åŒ–.md) - æŸ¥è¯¢ä¼˜åŒ–å®è·µ

### å¤–éƒ¨èµ„æº

- [SQLiteæ€§èƒ½ä¼˜åŒ–](https://www.sqlite.org/performance.html) - å®˜æ–¹æ€§èƒ½æ–‡æ¡£
- [FTS5æ–‡æ¡£](https://www.sqlite.org/fts5.html) - FTS5å…¨æ–‡æœç´¢

---

**æœ€åæ›´æ–°**: 2025-12-05
**ç»´æŠ¤è€…**: Data-Science Team
