# 大型数据库优化案例

> **案例类型**：性能优化实战案例
> **应用场景**：日志分析系统、数据仓库、历史数据存储
> **技术特点**：大型数据库优化、查询性能提升、存储空间优化

---

## 📑 目录

- [大型数据库优化案例](#大型数据库优化案例)
  - [📑 目录](#-目录)
  - [一、场景描述](#一场景描述)
    - [1.1 业务背景](#11-业务背景)
    - [1.2 系统规模](#12-系统规模)
  - [二、技术挑战](#二技术挑战)
    - [2.1 查询性能挑战](#21-查询性能挑战)
    - [2.2 存储空间挑战](#22-存储空间挑战)
    - [2.3 写入性能挑战](#23-写入性能挑战)
  - [三、优化方案](#三优化方案)
    - [3.1 表结构优化](#31-表结构优化)
    - [3.2 索引优化策略](#32-索引优化策略)
    - [3.3 查询优化](#33-查询优化)
    - [3.4 批量操作优化](#34-批量操作优化)
    - [3.5 数据归档策略](#35-数据归档策略)
    - [3.6 PRAGMA配置优化](#36-pragma配置优化)
  - [四、优化效果](#四优化效果)
    - [4.1 查询性能提升](#41-查询性能提升)
    - [4.2 写入性能提升](#42-写入性能提升)
    - [4.3 存储空间优化](#43-存储空间优化)
    - [4.4 系统资源使用](#44-系统资源使用)
  - [五、最佳实践](#五最佳实践)
    - [5.1 索引设计最佳实践](#51-索引设计最佳实践)
    - [5.2 查询优化最佳实践](#52-查询优化最佳实践)
    - [5.3 数据管理最佳实践](#53-数据管理最佳实践)
    - [5.4 配置优化最佳实践](#54-配置优化最佳实践)
  - [🔗 相关资源](#-相关资源)
  - [🔗 交叉引用](#-交叉引用)
    - [理论模型 🆕](#理论模型-)
    - [设计模型 🆕](#设计模型-)

---

## 一、场景描述

### 1.1 业务背景

某互联网公司需要构建一个日志分析系统，用于存储和分析应用日志数据。系统要求：

- **数据量**：日均新增日志 5000 万条
- **存储周期**：保存 90 天历史数据
- **总数据量**：约 45 亿条记录
- **数据库大小**：约 200GB
- **查询需求**：支持按时间范围、用户ID、操作类型等维度查询
- **性能要求**：单次查询响应时间 < 1秒

### 1.2 系统规模

- **表数量**：10+ 个业务表
- **最大表记录数**：45 亿条
- **数据库文件大小**：200GB+
- **日均查询量**：10,000+ 次
- **并发查询**：50+ 并发

---

## 二、技术挑战

### 2.1 查询性能挑战

**挑战**：

- 数据量巨大，全表扫描性能极差
- 时间范围查询需要扫描大量数据
- 多条件组合查询性能差
- 聚合查询（COUNT、SUM）耗时过长

**影响**：

- 查询响应时间 > 10秒
- 用户体验差
- 系统负载高

### 2.2 存储空间挑战

**挑战**：

- 数据库文件过大（200GB+）
- 删除操作后空间无法回收
- 索引占用大量空间
- 碎片化严重

**影响**：

- 存储成本高
- 备份和恢复时间长
- 磁盘I/O性能下降

### 2.3 写入性能挑战

**挑战**：

- 批量写入性能不足
- 索引更新开销大
- 事务提交时间长
- 并发写入冲突

**影响**：

- 数据写入延迟
- 系统吞吐量不足

---

## 三、优化方案

### 3.1 表结构优化

**原始设计**：

```sql
-- 原始日志表设计
CREATE TABLE logs (
    id INTEGER PRIMARY KEY,
    user_id INTEGER,
    action TEXT,
    resource_id TEXT,
    timestamp TEXT,
    details TEXT
);
```

**优化后设计**：

```sql
-- 优化后的日志表设计
CREATE TABLE logs (
    id INTEGER PRIMARY KEY,
    user_id INTEGER NOT NULL,
    action TEXT NOT NULL,
    resource_id TEXT,
    timestamp INTEGER NOT NULL,  -- 使用INTEGER存储时间戳
    details TEXT,
    -- 创建覆盖索引
    INDEX idx_user_time (user_id, timestamp DESC),
    INDEX idx_action_time (action, timestamp DESC),
    INDEX idx_timestamp (timestamp DESC)
);

-- 使用分区表（按日期分区）
CREATE TABLE logs_2025_01 AS SELECT * FROM logs WHERE 1=0;
CREATE TABLE logs_2025_02 AS SELECT * FROM logs WHERE 1=0;
-- ... 更多分区表
```

### 3.2 索引优化策略

**索引设计原则**：

1. **覆盖索引**：包含查询所需的所有列
2. **复合索引**：按查询条件顺序创建
3. **部分索引**：只索引常用数据

```sql
-- 覆盖索引：包含查询所需的所有列
CREATE INDEX idx_user_action_time ON logs(user_id, action, timestamp DESC);

-- 部分索引：只索引最近30天的数据
CREATE INDEX idx_recent_logs ON logs(timestamp DESC)
WHERE timestamp > strftime('%s', 'now', '-30 days');

-- 表达式索引：优化时间范围查询
CREATE INDEX idx_date ON logs(date(timestamp, 'unixepoch'));
```

### 3.3 查询优化

**优化前查询**：

```sql
-- 性能差：全表扫描
SELECT * FROM logs
WHERE user_id = 123
  AND timestamp > '2025-01-01'
  AND timestamp < '2025-01-31'
ORDER BY timestamp DESC
LIMIT 100;
```

**优化后查询**：

```sql
-- 性能好：使用索引
SELECT user_id, action, timestamp, details
FROM logs
WHERE user_id = 123
  AND timestamp >= strftime('%s', '2025-01-01')
  AND timestamp < strftime('%s', '2025-01-31')
ORDER BY timestamp DESC
LIMIT 100;

-- 使用覆盖索引，避免回表
SELECT user_id, action, timestamp
FROM logs
WHERE user_id = 123
  AND timestamp >= strftime('%s', '2025-01-01')
ORDER BY timestamp DESC
LIMIT 100;
```

### 3.4 批量操作优化

**批量插入优化**：

```python
import sqlite3
import time

def optimized_batch_insert(conn, logs):
    """优化的批量插入"""
    cursor = conn.cursor()

    # 1. 使用批量事务
    cursor.execute('BEGIN TRANSACTION')

    try:
        # 2. 使用executemany批量插入
        cursor.executemany("""
            INSERT INTO logs (user_id, action, resource_id, timestamp, details)
            VALUES (?, ?, ?, ?, ?)
        """, logs)

        # 3. 提交事务
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e

# 性能对比
# 方式1：逐条插入（慢）
# 10000条记录：~5000ms

# 方式2：批量插入（快）
# 10000条记录：~50ms
# 性能提升：100倍
```

### 3.5 数据归档策略

**归档策略**：

```python
def archive_old_logs(conn, days=90):
    """归档旧日志数据"""
    cutoff_timestamp = int(time.time()) - (days * 24 * 3600)

    cursor = conn.cursor()

    # 1. 导出旧数据到归档表
    cursor.execute("""
        CREATE TABLE logs_archive AS
        SELECT * FROM logs
        WHERE timestamp < ?
    """, (cutoff_timestamp,))

    # 2. 删除旧数据
    cursor.execute("""
        DELETE FROM logs
        WHERE timestamp < ?
    """, (cutoff_timestamp,))

    # 3. 执行VACUUM回收空间
    conn.execute('VACUUM')

    conn.commit()
```

### 3.6 PRAGMA配置优化

**性能优化配置**：

```python
def optimize_database(conn):
    """优化数据库配置"""
    # WAL模式：提升并发性能
    conn.execute('PRAGMA journal_mode=WAL')

    # 缓存大小：64MB（根据可用内存调整）
    conn.execute('PRAGMA cache_size=-16000')

    # 同步模式：NORMAL（平衡性能和安全性）
    conn.execute('PRAGMA synchronous=NORMAL')

    # 临时存储：内存
    conn.execute('PRAGMA temp_store=MEMORY')

    # 页面大小：4KB（默认）
    conn.execute('PRAGMA page_size=4096')

    # 自动检查点：1000页
    conn.execute('PRAGMA wal_autocheckpoint=1000')
```

**大型数据库优化工具类**：

```python
import sqlite3
import time
from typing import List, Dict

class LargeDatabaseOptimizer:
    """大型数据库优化工具"""

    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self._apply_optimizations()

    def _apply_optimizations(self):
        """应用优化配置"""
        self.conn.execute('PRAGMA journal_mode=WAL')
        self.conn.execute('PRAGMA cache_size=-64000')  # 64MB
        self.conn.execute('PRAGMA synchronous=NORMAL')
        self.conn.execute('PRAGMA temp_store=MEMORY')
        self.conn.execute('PRAGMA mmap_size=268435456')  # 256MB内存映射
        self.conn.commit()

    def analyze_query_performance(self, sql, params=None, iterations=10):
        """分析查询性能"""
        # 预热
        for _ in range(5):
            self.conn.execute(sql, params or ()).fetchall()

        # 测量
        start = time.time()
        for _ in range(iterations):
            self.conn.execute(sql, params or ()).fetchall()
        elapsed = time.time() - start

        avg_time = (elapsed / iterations) * 1000
        print(f"平均执行时间: {avg_time:.3f}ms")
        return avg_time

    def optimize_indexes(self, table_name):
        """优化表索引"""
        # 分析表
        self.conn.execute(f'ANALYZE {table_name}')

        # 重建索引
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT name FROM sqlite_master
            WHERE type='index' AND tbl_name='{table_name}'
        """)

        for (index_name,) in cursor.fetchall():
            print(f"重建索引: {index_name}")
            self.conn.execute(f'REINDEX {index_name}')

        self.conn.commit()
        print(f"✅ 表 {table_name} 索引优化完成")

    def archive_old_data(self, table_name, date_column, days=90):
        """归档旧数据"""
        cutoff_date = int(time.time()) - (days * 86400)

        # 创建归档表
        self.conn.execute(f"""
            CREATE TABLE IF NOT EXISTS {table_name}_archive AS
            SELECT * FROM {table_name}
            WHERE {date_column} < ?
        """, (cutoff_date,))

        # 删除旧数据
        deleted = self.conn.execute(f"""
            DELETE FROM {table_name}
            WHERE {date_column} < ?
        """, (cutoff_date,)).rowcount

        self.conn.commit()
        print(f"✅ 归档 {deleted:,} 条记录")

        # 执行VACUUM
        print("执行VACUUM...")
        self.conn.execute('VACUUM')
        print("✅ VACUUM完成")

    def get_database_stats(self):
        """获取数据库统计信息"""
        stats = {}

        # 页数和大小
        cursor = self.conn.execute('PRAGMA page_count')
        stats['page_count'] = cursor.fetchone()[0]

        cursor = self.conn.execute('PRAGMA page_size')
        stats['page_size'] = cursor.fetchone()[0]

        stats['total_size'] = stats['page_count'] * stats['page_size']

        # 空闲页数
        cursor = self.conn.execute('PRAGMA freelist_count')
        stats['freelist_count'] = cursor.fetchone()[0]

        # 碎片率
        if stats['page_count'] > 0:
            stats['fragmentation'] = (stats['freelist_count'] / stats['page_count']) * 100
        else:
            stats['fragmentation'] = 0

        return stats

# 使用示例
optimizer = LargeDatabaseOptimizer('large_database.db')

# 分析查询性能
optimizer.analyze_query_performance(
    'SELECT * FROM logs WHERE user_id = ? AND timestamp > ?',
    (123, int(time.time()) - 86400)
)

# 优化索引
optimizer.optimize_indexes('logs')

# 归档旧数据
optimizer.archive_old_data('logs', 'timestamp', days=90)

# 获取统计信息
stats = optimizer.get_database_stats()
print(f"数据库大小: {stats['total_size'] / 1024 / 1024 / 1024:.2f} GB")
print(f"碎片率: {stats['fragmentation']:.2f}%")

# 分析统计信息
conn.execute('ANALYZE')

```

---

## 四、优化效果

### 4.1 查询性能提升

**优化前后对比**：

| 查询类型 | 优化前 | 优化后 | 提升倍数 |
|---------|--------|--------|---------|
| 单用户时间范围查询 | 8.5秒 | 0.3秒 | 28倍 |
| 操作类型统计查询 | 15.2秒 | 0.8秒 | 19倍 |
| 聚合查询（COUNT） | 25.6秒 | 1.2秒 | 21倍 |
| 覆盖索引查询 | 5.3秒 | 0.1秒 | 53倍 |

### 4.2 写入性能提升

**优化前后对比**：

| 操作类型 | 优化前 | 优化后 | 提升倍数 |
|---------|--------|--------|---------|
| 单条插入 | 2.5ms | 0.8ms | 3倍 |
| 批量插入（1000条） | 2500ms | 25ms | 100倍 |
| 批量插入（10000条） | 25000ms | 180ms | 139倍 |

### 4.3 存储空间优化

**优化前后对比**：

| 指标 | 优化前 | 优化后 | 改善 |
|------|--------|--------|------|
| 数据库文件大小 | 200GB | 180GB | -10% |
| 索引占用空间 | 40GB | 25GB | -37.5% |
| 碎片化率 | 15% | 2% | -87% |

### 4.4 系统资源使用

**优化前后对比**：

| 资源类型 | 优化前 | 优化后 | 改善 |
|---------|--------|--------|------|
| CPU使用率 | 85% | 45% | -47% |
| 内存使用 | 8GB | 4GB | -50% |
| 磁盘I/O | 高 | 中 | 显著降低 |

---

## 五、最佳实践

### 5.1 索引设计最佳实践

1. **覆盖索引优先**
   - 包含查询所需的所有列
   - 避免回表查询

2. **复合索引顺序**
   - 按查询条件频率排序
   - 等值条件在前，范围条件在后

3. **部分索引使用**
   - 只索引常用数据
   - 减少索引大小和维护开销

### 5.2 查询优化最佳实践

1. **使用索引列**
   - WHERE条件使用索引列
   - 避免在索引列上使用函数

2. **限制结果集**
   - 使用LIMIT限制返回行数
   - 避免SELECT *

3. **批量操作**
   - 使用批量事务
   - 使用executemany批量插入

### 5.3 数据管理最佳实践

1. **定期归档**
   - 定期归档旧数据
   - 执行VACUUM回收空间

2. **统计信息更新**
   - 定期执行ANALYZE
   - 更新查询优化器统计信息

3. **监控和调优**
   - 监控查询性能
   - 定期检查慢查询
   - 根据实际情况调整索引

### 5.4 配置优化最佳实践

```python
# 大型数据库推荐配置
LARGE_DB_CONFIG = {
    'journal_mode': 'WAL',           # WAL模式
    'cache_size': -16000,            # 64MB缓存
    'synchronous': 'NORMAL',         # 平衡性能和安全性
    'temp_store': 'MEMORY',          # 临时表在内存
    'page_size': 4096,               # 4KB页大小
    'wal_autocheckpoint': 1000,      # 自动检查点
    'mmap_size': 268435456,          # 256MB内存映射
}
```

---

## 🔗 相关资源

- [03.01 性能特征分析](../03-性能优化/03.01-性能特征分析.md) - 性能特征分析
- [03.02 优化策略](../03-性能优化/03.02-优化策略.md) - 索引优化、查询优化
- [03.03 性能调优决策树](../03-性能优化/03.03-性能调优决策树.md) - 调优决策流程
- [08.03 查询优化](../08-编程实践/08.03-查询优化.md) - 查询优化实践
- [08.04 PRAGMA配置](../08-编程实践/08.04-PRAGMA配置.md) - PRAGMA配置详解

---

## 🔗 交叉引用

### 理论模型 🆕

- ⭐⭐⭐ [算法复杂度理论](../11-理论模型/11.03-算法复杂度理论.md) - 查询操作复杂度、B-Tree操作复杂度
- ⭐⭐ [存储理论](../11-理论模型/11.05-存储理论.md) - 索引理论、缓存理论、存储优化理论

### 设计模型 🆕

- ⭐⭐ [设计决策](../12-设计模型/12.04-设计决策.md) - 存储设计决策、索引设计决策
- ⭐ [设计模式](../12-设计模型/12.03-设计模式.md) - 批量处理模式

---

**维护者**：Data-Science Team
**最后更新**：2025-01-15
