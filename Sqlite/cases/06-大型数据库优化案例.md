# 大型数据库优化案例

> **案例类型**：性能优化实战案例
> **应用场景**：日志分析系统、数据仓库、历史数据存储
> **技术特点**：大型数据库优化、查询性能提升、存储空间优化

---

## 1. 📑 目录

- [大型数据库优化案例](#大型数据库优化案例)
  - [1. 📑 目录](#1--目录)
  - [2. 场景描述](#2-场景描述)
    - [2.1. 业务背景](#21-业务背景)
    - [2.2. 系统规模](#22-系统规模)
  - [3. 技术挑战](#3-技术挑战)
    - [3.1. 查询性能挑战](#31-查询性能挑战)
    - [3.2. 存储空间挑战](#32-存储空间挑战)
    - [3.3. 写入性能挑战](#33-写入性能挑战)
  - [4. 优化方案](#4-优化方案)
    - [4.1. 表结构优化](#41-表结构优化)
    - [4.2. 索引优化策略](#42-索引优化策略)
    - [4.3. 查询优化](#43-查询优化)
    - [4.4. 批量操作优化](#44-批量操作优化)
    - [4.5. 列存储模拟优化 🆕](#45-列存储模拟优化-)
  - [5. 数据归档策略](#5-数据归档策略)
  - [6. PRAGMA配置优化](#6-pragma配置优化)
  - [7. 优化效果](#7-优化效果)
    - [7.1. 查询性能提升](#71-查询性能提升)
    - [7.2. 写入性能提升](#72-写入性能提升)
    - [7.3. 存储空间优化](#73-存储空间优化)
    - [7.4. 系统资源使用](#74-系统资源使用)
  - [8. 最佳实践](#8-最佳实践)
    - [8.1. 索引设计最佳实践](#81-索引设计最佳实践)
    - [8.2. 查询优化最佳实践](#82-查询优化最佳实践)
    - [8.3. 数据管理最佳实践](#83-数据管理最佳实践)
    - [8.4. 配置优化最佳实践](#84-配置优化最佳实践)
  - [9. 🔗 相关资源](#9--相关资源)
  - [10. 🔗 交叉引用](#10--交叉引用)
    - [10.1. 理论模型 🆕](#101-理论模型-)
    - [10.2. 设计模型 🆕](#102-设计模型-)

---

## 2. 场景描述

### 2.1. 业务背景

某互联网公司需要构建一个日志分析系统，用于存储和分析应用日志数据。系统要求：

- **数据量**：日均新增日志 5000 万条
- **存储周期**：保存 90 天历史数据
- **总数据量**：约 45 亿条记录
- **数据库大小**：约 200GB
- **查询需求**：支持按时间范围、用户ID、操作类型等维度查询
- **性能要求**：单次查询响应时间 < 1秒

### 2.2. 系统规模

- **表数量**：10+ 个业务表
- **最大表记录数**：45 亿条
- **数据库文件大小**：200GB+
- **日均查询量**：10,000+ 次
- **并发查询**：50+ 并发

---

## 3. 技术挑战

### 3.1. 查询性能挑战

**挑战**：

- 数据量巨大，全表扫描性能极差
- 时间范围查询需要扫描大量数据
- 多条件组合查询性能差
- 聚合查询（COUNT、SUM）耗时过长

**影响**：

- 查询响应时间 > 10秒
- 用户体验差
- 系统负载高

### 3.2. 存储空间挑战

**挑战**：

- 数据库文件过大（200GB+）
- 删除操作后空间无法回收
- 索引占用大量空间
- 碎片化严重

**影响**：

- 存储成本高
- 备份和恢复时间长
- 磁盘I/O性能下降

### 3.3. 写入性能挑战

**挑战**：

- 批量写入性能不足
- 索引更新开销大
- 事务提交时间长
- 并发写入冲突

**影响**：

- 数据写入延迟
- 系统吞吐量不足

---

## 4. 优化方案

### 4.1. 表结构优化

**原始设计**：

```sql
-- 原始日志表设计
CREATE TABLE logs (
    id INTEGER PRIMARY KEY,
    user_id INTEGER,
    action TEXT,
    resource_id TEXT,
    timestamp TEXT,
    details TEXT
);
```

**优化后设计**：

```sql
-- 优化后的日志表设计
CREATE TABLE logs (
    id INTEGER PRIMARY KEY,
    user_id INTEGER NOT NULL,
    action TEXT NOT NULL,
    resource_id TEXT,
    timestamp INTEGER NOT NULL,  -- 使用INTEGER存储时间戳
    details TEXT,
    -- 创建覆盖索引
    INDEX idx_user_time (user_id, timestamp DESC),
    INDEX idx_action_time (action, timestamp DESC),
    INDEX idx_timestamp (timestamp DESC)
);

-- 使用分区表（按日期分区）
CREATE TABLE logs_2025_01 AS SELECT * FROM logs WHERE 1=0;
CREATE TABLE logs_2025_02 AS SELECT * FROM logs WHERE 1=0;
-- ... 更多分区表
```

### 4.2. 索引优化策略

**索引设计原则**：

1. **覆盖索引**：包含查询所需的所有列
2. **复合索引**：按查询条件顺序创建
3. **部分索引**：只索引常用数据

```sql
-- 覆盖索引：包含查询所需的所有列
CREATE INDEX idx_user_action_time ON logs(user_id, action, timestamp DESC);

-- 部分索引：只索引最近30天的数据
CREATE INDEX idx_recent_logs ON logs(timestamp DESC)
WHERE timestamp > strftime('%s', 'now', '-30 days');

-- 表达式索引：优化时间范围查询
CREATE INDEX idx_date ON logs(date(timestamp, 'unixepoch'));
```

### 4.3. 查询优化

**优化前查询**：

```sql
-- 性能差：全表扫描
SELECT * FROM logs
WHERE user_id = 123
  AND timestamp > '2025-01-01'
  AND timestamp < '2025-01-31'
ORDER BY timestamp DESC
LIMIT 100;
```

**优化后查询**：

```sql
-- 性能好：使用索引
SELECT user_id, action, timestamp, details
FROM logs
WHERE user_id = 123
  AND timestamp >= strftime('%s', '2025-01-01')
  AND timestamp < strftime('%s', '2025-01-31')
ORDER BY timestamp DESC
LIMIT 100;

-- 使用覆盖索引，避免回表
SELECT user_id, action, timestamp
FROM logs
WHERE user_id = 123
  AND timestamp >= strftime('%s', '2025-01-01')
ORDER BY timestamp DESC
LIMIT 100;
```

### 4.4. 批量操作优化

**批量插入优化**：

```python
import sqlite3
import time

def optimized_batch_insert(conn, logs):
    """优化的批量插入"""
    cursor = conn.cursor()

# 1. 使用批量事务
    cursor.execute('BEGIN TRANSACTION')

    try:
# 2. 使用executemany批量插入
        cursor.executemany("""
            INSERT INTO logs (user_id, action, resource_id, timestamp, details)
            VALUES (?, ?, ?, ?, ?)
        """, logs)

# 3. 提交事务
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise e

# 性能对比
# 方式1：逐条插入（慢）
# 10000条记录：~5000ms

# 方式2：批量插入（快）
# 10000条记录：~50ms
# 性能提升：100倍
```

### 4.5. 列存储模拟优化 🆕

**对于分析查询（聚合、统计），可以使用列存储模拟方案大幅提升性能**：

**方案1：列式物化视图（分析查询优化）**：

```python
import sqlite3
from collections import defaultdict

class ColumnarMaterializedView:
    """列式物化视图：为分析查询创建列存储结构"""

    def __init__(self, conn, source_table, columns):
        self.conn = conn
        self.source_table = source_table
        self.columns = columns
        self.column_tables = {}

    def create_column_tables(self):
        """为每列创建单独的表（模拟列存储）"""
        for col in self.columns:
            col_table = f"{self.source_table}_{col}_column"
            self.column_tables[col] = col_table

            self.conn.execute(f"""
                CREATE TABLE IF NOT EXISTS {col_table} (
                    row_id INTEGER PRIMARY KEY,
                    value TEXT,
                    INDEX idx_value (value)
                )
            """)

    def populate_columns(self):
        """从原始表填充列表（定期刷新）"""
        cursor = self.conn.cursor()

        # 清空现有数据
        for col_table in self.column_tables.values():
            cursor.execute(f"DELETE FROM {col_table}")

        # 获取所有行
        columns_str = ', '.join(self.columns)
        cursor.execute(f"SELECT rowid, {columns_str} FROM {self.source_table}")
        rows = cursor.fetchall()

        # 按列存储
        for col_idx, col in enumerate(self.columns):
            col_table = self.column_tables[col]
            col_values = [(row[0], str(row[col_idx + 1])) for row in rows]

            cursor.executemany(
                f"INSERT INTO {col_table} (row_id, value) VALUES (?, ?)",
                col_values
            )

        self.conn.commit()
        print(f"✅ 列式物化视图已更新，共 {len(rows)} 行")

    def query_aggregate(self, column, aggregate_func='COUNT', condition=None):
        """使用列存储进行聚合查询"""
        col_table = self.column_tables[column]

        if aggregate_func == 'COUNT':
            if condition:
                query = f"SELECT COUNT(*) FROM {col_table} WHERE {condition}"
            else:
                query = f"SELECT COUNT(*) FROM {col_table}"
        elif aggregate_func == 'COUNT_DISTINCT':
            query = f"SELECT COUNT(DISTINCT value) FROM {col_table}"
            if condition:
                query = f"SELECT COUNT(DISTINCT value) FROM {col_table} WHERE {condition}"
        else:
            raise ValueError(f"Unsupported aggregate function: {aggregate_func}")

        result = self.conn.execute(query).fetchone()
        return result[0] if result else 0

    def query_group_by(self, column, group_column=None):
        """使用列存储进行分组查询"""
        col_table = self.column_tables[column]

        if group_column:
            group_table = self.column_tables[group_column]
            query = f"""
                SELECT g.value as group_value, COUNT(*) as count
                FROM {col_table} c
                JOIN {group_table} g ON c.row_id = g.row_id
                GROUP BY g.value
            """
        else:
            query = f"""
                SELECT value, COUNT(*) as count
                FROM {col_table}
                GROUP BY value
            """

        return self.conn.execute(query).fetchall()

# 使用示例
conn = sqlite3.connect('logs.db')

# 创建列式物化视图
view = ColumnarMaterializedView(conn, 'logs', ['user_id', 'action', 'timestamp', 'duration'])
view.create_column_tables()

# 定期刷新（如每小时或每天）
view.populate_columns()

# 列存储查询优势：只扫描需要的列
# 原始查询：SELECT COUNT(*) FROM logs WHERE action = 'login'
# 需要扫描整表（200GB）
action_count = view.query_aggregate('action', 'COUNT', "value = 'login'")
# 只扫描action列（约20GB），性能提升10倍

# 分组查询：SELECT action, COUNT(*) FROM logs GROUP BY action
action_stats = view.query_group_by('action')
# 只扫描action列，性能大幅提升
```

**方案2：列压缩存储（低基数列优化）**：

```python
import sqlite3
import zlib
import pickle
from collections import Counter

class ColumnCompressor:
    """列压缩器：对低基数列应用压缩算法"""

    def compress_action_column(self, conn, table_name='logs'):
        """压缩action列（低基数列）"""
        cursor = conn.cursor()

        # 读取action列数据
        cursor.execute(f"SELECT action FROM {table_name}")
        actions = [row[0] for row in cursor.fetchall()]

        # 字典编码
        unique_actions = list(set(actions))
        dictionary = {action: idx for idx, action in enumerate(unique_actions)}
        encoded = [dictionary[action] for action in actions]

        # 存储压缩后的数据
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS logs_action_compressed (
                row_id INTEGER PRIMARY KEY,
                encoded_value INTEGER,
                INDEX idx_encoded (encoded_value)
            )
        """)

        # 存储字典
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS logs_action_dict (
                encoded_value INTEGER PRIMARY KEY,
                original_value TEXT
            )
        """)

        cursor.executemany(
            "INSERT OR REPLACE INTO logs_action_compressed (row_id, encoded_value) VALUES (?, ?)",
            [(i+1, val) for i, val in enumerate(encoded)]
        )

        cursor.executemany(
            "INSERT OR REPLACE INTO logs_action_dict (encoded_value, original_value) VALUES (?, ?)",
            [(idx, action) for action, idx in dictionary.items()]
        )

        conn.commit()

        # 计算压缩率
        original_size = len(actions) * sum(len(a.encode('utf-8')) for a in actions) / len(actions)
        compressed_size = len(encoded) * 4 + len(unique_actions) * 20
        compression_ratio = compressed_size / original_size

        print(f"✅ Action列压缩完成，压缩率: {compression_ratio:.2%}")
        return compression_ratio

# 使用示例
compressor = ColumnCompressor()
compression_ratio = compressor.compress_action_column(conn)
# 通常可达到40-60%的压缩率
```

**方案3：分析查询专用表（预聚合）**：

```sql
-- 创建预聚合表，定期更新
CREATE TABLE logs_daily_stats (
    date INTEGER PRIMARY KEY,
    total_count INTEGER,
    login_count INTEGER,
    logout_count INTEGER,
    error_count INTEGER,
    avg_duration REAL,
    unique_users INTEGER
);

-- 定期更新预聚合表（如每小时）
INSERT OR REPLACE INTO logs_daily_stats
SELECT
    date(timestamp, 'unixepoch') as date,
    COUNT(*) as total_count,
    SUM(CASE WHEN action = 'login' THEN 1 ELSE 0 END) as login_count,
    SUM(CASE WHEN action = 'logout' THEN 1 ELSE 0 END) as logout_count,
    SUM(CASE WHEN action = 'error' THEN 1 ELSE 0 END) as error_count,
    AVG(duration) as avg_duration,
    COUNT(DISTINCT user_id) as unique_users
FROM logs
WHERE timestamp >= strftime('%s', date('now', '-1 day'))
GROUP BY date(timestamp, 'unixepoch');

-- 分析查询直接使用预聚合表
SELECT * FROM logs_daily_stats
WHERE date >= date('now', '-7 days');
-- 查询速度提升100倍+
```

**列存储模拟优化效果对比**：

| 查询类型 | 原始查询时间 | 列存储模拟后 | 性能提升 |
|---------|------------|------------|---------|
| **COUNT聚合** | 25.6秒 | 1.2秒 | 21倍 |
| **GROUP BY聚合** | 45.3秒 | 2.1秒 | 22倍 |
| **单列扫描** | 18.9秒 | 0.8秒 | 24倍 |
| **多列JOIN聚合** | 62.4秒 | 3.5秒 | 18倍 |

**适用场景**：

- ✅ 分析查询（聚合、统计）
- ✅ 读多写少场景
- ✅ 低基数列（重复值多）
- ✅ 定期刷新的物化视图
- ❌ 实时写入场景
- ❌ 点查询（单行查询）

## 5. 数据归档策略

**归档策略**：

```python
def archive_old_logs(conn, days=90):
    """归档旧日志数据"""
    cutoff_timestamp = int(time.time()) - (days * 24 * 3600)

    cursor = conn.cursor()

# 1. 导出旧数据到归档表
    cursor.execute("""
        CREATE TABLE logs_archive AS
        SELECT * FROM logs
        WHERE timestamp < ?
    """, (cutoff_timestamp,))

# 2. 删除旧数据
    cursor.execute("""
        DELETE FROM logs
        WHERE timestamp < ?
    """, (cutoff_timestamp,))

# 3. 执行VACUUM回收空间
    conn.execute('VACUUM')

    conn.commit()
```

## 6. PRAGMA配置优化

**性能优化配置**：

```python
def optimize_database(conn):
    """优化数据库配置"""
# WAL模式：提升并发性能
    conn.execute('PRAGMA journal_mode=WAL')

# 缓存大小：64MB（根据可用内存调整）
    conn.execute('PRAGMA cache_size=-16000')

# 同步模式：NORMAL（平衡性能和安全性）
    conn.execute('PRAGMA synchronous=NORMAL')

# 临时存储：内存
    conn.execute('PRAGMA temp_store=MEMORY')

# 页面大小：4KB（默认）
    conn.execute('PRAGMA page_size=4096')

# 自动检查点：1000页
    conn.execute('PRAGMA wal_autocheckpoint=1000')
```

**大型数据库优化工具类**：

```python
import sqlite3
import time
from typing import List, Dict

class LargeDatabaseOptimizer:
    """大型数据库优化工具"""

    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self._apply_optimizations()

    def _apply_optimizations(self):
        """应用优化配置"""
        self.conn.execute('PRAGMA journal_mode=WAL')
        self.conn.execute('PRAGMA cache_size=-64000')  # 64MB
        self.conn.execute('PRAGMA synchronous=NORMAL')
        self.conn.execute('PRAGMA temp_store=MEMORY')
        self.conn.execute('PRAGMA mmap_size=268435456')  # 256MB内存映射
        self.conn.commit()

    def analyze_query_performance(self, sql, params=None, iterations=10):
        """分析查询性能"""
# 预热
        for _ in range(5):
            self.conn.execute(sql, params or ()).fetchall()

# 测量
        start = time.time()
        for _ in range(iterations):
            self.conn.execute(sql, params or ()).fetchall()
        elapsed = time.time() - start

        avg_time = (elapsed / iterations) * 1000
        print(f"平均执行时间: {avg_time:.3f}ms")
        return avg_time

    def optimize_indexes(self, table_name):
        """优化表索引"""
# 分析表
        self.conn.execute(f'ANALYZE {table_name}')

# 重建索引
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT name FROM sqlite_master
            WHERE type='index' AND tbl_name='{table_name}'
        """)

        for (index_name,) in cursor.fetchall():
            print(f"重建索引: {index_name}")
            self.conn.execute(f'REINDEX {index_name}')

        self.conn.commit()
        print(f"✅ 表 {table_name} 索引优化完成")

    def archive_old_data(self, table_name, date_column, days=90):
        """归档旧数据"""
        cutoff_date = int(time.time()) - (days * 86400)

# 创建归档表
        self.conn.execute(f"""
            CREATE TABLE IF NOT EXISTS {table_name}_archive AS
            SELECT * FROM {table_name}
            WHERE {date_column} < ?
        """, (cutoff_date,))

# 删除旧数据
        deleted = self.conn.execute(f"""
            DELETE FROM {table_name}
            WHERE {date_column} < ?
        """, (cutoff_date,)).rowcount

        self.conn.commit()
        print(f"✅ 归档 {deleted:,} 条记录")

# 执行VACUUM
        print("执行VACUUM...")
        self.conn.execute('VACUUM')
        print("✅ VACUUM完成")

    def get_database_stats(self):
        """获取数据库统计信息"""
        stats = {}

# 页数和大小
        cursor = self.conn.execute('PRAGMA page_count')
        stats['page_count'] = cursor.fetchone()[0]

        cursor = self.conn.execute('PRAGMA page_size')
        stats['page_size'] = cursor.fetchone()[0]

        stats['total_size'] = stats['page_count'] * stats['page_size']

# 空闲页数
        cursor = self.conn.execute('PRAGMA freelist_count')
        stats['freelist_count'] = cursor.fetchone()[0]

# 碎片率
        if stats['page_count'] > 0:
            stats['fragmentation'] = (stats['freelist_count'] / stats['page_count']) * 100
        else:
            stats['fragmentation'] = 0

        return stats

# 使用示例
optimizer = LargeDatabaseOptimizer('large_database.db')

# 分析查询性能
optimizer.analyze_query_performance(
    'SELECT * FROM logs WHERE user_id = ? AND timestamp > ?',
    (123, int(time.time()) - 86400)
)

# 优化索引
optimizer.optimize_indexes('logs')

# 归档旧数据
optimizer.archive_old_data('logs', 'timestamp', days=90)

# 获取统计信息
stats = optimizer.get_database_stats()
print(f"数据库大小: {stats['total_size'] / 1024 / 1024 / 1024:.2f} GB")
print(f"碎片率: {stats['fragmentation']:.2f}%")

# 分析统计信息
conn.execute('ANALYZE')

```

---

## 7. 优化效果

### 7.1. 查询性能提升

**优化前后对比**：

| 查询类型 | 优化前 | 优化后 | 提升倍数 |
|---------|--------|--------|---------|
| 单用户时间范围查询 | 8.5秒 | 0.3秒 | 28倍 |
| 操作类型统计查询 | 15.2秒 | 0.8秒 | 19倍 |
| 聚合查询（COUNT） | 25.6秒 | 1.2秒 | 21倍 |
| 覆盖索引查询 | 5.3秒 | 0.1秒 | 53倍 |

### 7.2. 写入性能提升

**优化前后对比**：

| 操作类型 | 优化前 | 优化后 | 提升倍数 |
|---------|--------|--------|---------|
| 单条插入 | 2.5ms | 0.8ms | 3倍 |
| 批量插入（1000条） | 2500ms | 25ms | 100倍 |
| 批量插入（10000条） | 25000ms | 180ms | 139倍 |

### 7.3. 存储空间优化

**优化前后对比**：

| 指标 | 优化前 | 优化后 | 改善 |
|------|--------|--------|------|
| 数据库文件大小 | 200GB | 180GB | -10% |
| 索引占用空间 | 40GB | 25GB | -37.5% |
| 碎片化率 | 15% | 2% | -87% |

### 7.4. 系统资源使用

**优化前后对比**：

| 资源类型 | 优化前 | 优化后 | 改善 |
|---------|--------|--------|------|
| CPU使用率 | 85% | 45% | -47% |
| 内存使用 | 8GB | 4GB | -50% |
| 磁盘I/O | 高 | 中 | 显著降低 |

---

## 8. 最佳实践

### 8.1. 索引设计最佳实践

1. **覆盖索引优先**
   - 包含查询所需的所有列
   - 避免回表查询

2. **复合索引顺序**
   - 按查询条件频率排序
   - 等值条件在前，范围条件在后

3. **部分索引使用**
   - 只索引常用数据
   - 减少索引大小和维护开销

### 8.2. 查询优化最佳实践

1. **使用索引列**
   - WHERE条件使用索引列
   - 避免在索引列上使用函数

2. **限制结果集**
   - 使用LIMIT限制返回行数
   - 避免SELECT *

3. **批量操作**
   - 使用批量事务
   - 使用executemany批量插入

### 8.3. 数据管理最佳实践

1. **定期归档**
   - 定期归档旧数据
   - 执行VACUUM回收空间

2. **统计信息更新**
   - 定期执行ANALYZE
   - 更新查询优化器统计信息

3. **监控和调优**
   - 监控查询性能
   - 定期检查慢查询
   - 根据实际情况调整索引

### 8.4. 配置优化最佳实践

```python
# 大型数据库推荐配置
LARGE_DB_CONFIG = {
    'journal_mode': 'WAL',           # WAL模式
    'cache_size': -16000,            # 64MB缓存
    'synchronous': 'NORMAL',         # 平衡性能和安全性
    'temp_store': 'MEMORY',          # 临时表在内存
    'page_size': 4096,               # 4KB页大小
    'wal_autocheckpoint': 1000,      # 自动检查点
    'mmap_size': 268435456,          # 256MB内存映射
}
```

---

## 9. 🔗 相关资源

- [03.01 性能特征分析](../03-性能优化/03.01-性能特征分析.md) - 性能特征分析
- [03.02 优化策略](../03-性能优化/03.02-优化策略.md) - 索引优化、查询优化
- [03.03 性能调优决策树](../03-性能优化/03.03-性能调优决策树.md) - 调优决策流程
- [08.03 查询优化](../08-编程实践/08.03-查询优化.md) - 查询优化实践
- [08.04 PRAGMA配置](../08-编程实践/08.04-PRAGMA配置.md) - PRAGMA配置详解

---

## 10. 🔗 交叉引用

### 10.1. 理论模型 🆕

- ⭐⭐⭐ [算法复杂度理论](../11-理论模型/11.03-算法复杂度理论.md) - 查询操作复杂度、B-Tree操作复杂度
- ⭐⭐ [存储理论](../11-理论模型/11.05-存储理论.md) - 索引理论、缓存理论、存储优化理论

### 10.2. 设计模型 🆕

- ⭐⭐ [设计决策](../12-设计模型/12.04-设计决策.md) - 存储设计决策、索引设计决策
- ⭐ [设计模式](../12-设计模型/12.03-设计模式.md) - 批量处理模式

---

**维护者**：Data-Science Team
**最后更新**：2025-01-15
