# å­˜å‚¨ç©ºé—´ä¼˜åŒ–æ¡ˆä¾‹

> **æ¡ˆä¾‹ç±»å‹**ï¼šæ€§èƒ½ä¼˜åŒ–å®æˆ˜æ¡ˆä¾‹
> **åº”ç”¨åœºæ™¯**ï¼šç§»åŠ¨åº”ç”¨ã€åµŒå…¥å¼ç³»ç»Ÿã€èµ„æºå—é™ç¯å¢ƒ
> **æŠ€æœ¯ç‰¹ç‚¹**ï¼šå­˜å‚¨ç©ºé—´ä¼˜åŒ–ã€æ•°æ®å‹ç¼©ã€ç¢ç‰‡æ•´ç†

---

## 1. ğŸ“‘ ç›®å½•

- [å­˜å‚¨ç©ºé—´ä¼˜åŒ–æ¡ˆä¾‹](#å­˜å‚¨ç©ºé—´ä¼˜åŒ–æ¡ˆä¾‹)
  - [1. ğŸ“‘ ç›®å½•](#1--ç›®å½•)
  - [2. åœºæ™¯æè¿°](#2-åœºæ™¯æè¿°)
    - [2.1. ä¸šåŠ¡èƒŒæ™¯](#21-ä¸šåŠ¡èƒŒæ™¯)
    - [2.2. ç³»ç»Ÿè§„æ¨¡](#22-ç³»ç»Ÿè§„æ¨¡)
  - [3. æŠ€æœ¯æŒ‘æˆ˜](#3-æŠ€æœ¯æŒ‘æˆ˜)
    - [3.1. å­˜å‚¨ç©ºé—´æŒ‘æˆ˜](#31-å­˜å‚¨ç©ºé—´æŒ‘æˆ˜)
    - [3.2. æ•°æ®å†—ä½™æŒ‘æˆ˜](#32-æ•°æ®å†—ä½™æŒ‘æˆ˜)
    - [3.3. ç¢ç‰‡åŒ–æŒ‘æˆ˜](#33-ç¢ç‰‡åŒ–æŒ‘æˆ˜)
  - [4. ä¼˜åŒ–æ–¹æ¡ˆ](#4-ä¼˜åŒ–æ–¹æ¡ˆ)
    - [4.1. æ•°æ®ç±»å‹ä¼˜åŒ–](#41-æ•°æ®ç±»å‹ä¼˜åŒ–)
    - [4.2. è¡¨ç»“æ„ä¼˜åŒ–](#42-è¡¨ç»“æ„ä¼˜åŒ–)
    - [4.3. ç´¢å¼•ä¼˜åŒ–](#43-ç´¢å¼•ä¼˜åŒ–)
    - [4.4. æ•°æ®å‹ç¼©](#44-æ•°æ®å‹ç¼©)
    - [4.5. åˆ—å­˜å‚¨æ¨¡æ‹Ÿæ–¹æ¡ˆ](#45-åˆ—å­˜å‚¨æ¨¡æ‹Ÿæ–¹æ¡ˆ)
  - [5. ç¢ç‰‡æ•´ç†](#5-ç¢ç‰‡æ•´ç†)
  - [6. æ•°æ®å½’æ¡£](#6-æ•°æ®å½’æ¡£)
  - [7. ä¼˜åŒ–æ•ˆæœ](#7-ä¼˜åŒ–æ•ˆæœ)
    - [7.1. å­˜å‚¨ç©ºé—´èŠ‚çœ](#71-å­˜å‚¨ç©ºé—´èŠ‚çœ)
    - [7.2. æ€§èƒ½å½±å“](#72-æ€§èƒ½å½±å“)
    - [7.3. æˆæœ¬é™ä½](#73-æˆæœ¬é™ä½)
  - [8. æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)
    - [8.1. æ•°æ®ç±»å‹é€‰æ‹©æœ€ä½³å®è·µ](#81-æ•°æ®ç±»å‹é€‰æ‹©æœ€ä½³å®è·µ)
    - [8.2. è¡¨è®¾è®¡æœ€ä½³å®è·µ](#82-è¡¨è®¾è®¡æœ€ä½³å®è·µ)
    - [8.3. ç´¢å¼•è®¾è®¡æœ€ä½³å®è·µ](#83-ç´¢å¼•è®¾è®¡æœ€ä½³å®è·µ)
    - [8.4. æ•°æ®ç®¡ç†æœ€ä½³å®è·µ](#84-æ•°æ®ç®¡ç†æœ€ä½³å®è·µ)
  - [9. ğŸ”— ç›¸å…³èµ„æº](#9--ç›¸å…³èµ„æº)
  - [10. ğŸ”— äº¤å‰å¼•ç”¨](#10--äº¤å‰å¼•ç”¨)
    - [10.1. ç†è®ºæ¨¡å‹ ğŸ†•](#101-ç†è®ºæ¨¡å‹-)
    - [10.2. è®¾è®¡æ¨¡å‹ ğŸ†•](#102-è®¾è®¡æ¨¡å‹-)

---

## 2. åœºæ™¯æè¿°

### 2.1. ä¸šåŠ¡èƒŒæ™¯

æŸç§»åŠ¨åº”ç”¨éœ€è¦åœ¨æœ‰é™çš„å­˜å‚¨ç©ºé—´ä¸­å­˜å‚¨å¤§é‡ç”¨æˆ·æ•°æ®ã€‚ç³»ç»Ÿè¦æ±‚ï¼š

- **å­˜å‚¨ç©ºé—´é™åˆ¶**ï¼šåº”ç”¨å¯ç”¨å­˜å‚¨ç©ºé—´ < 100MB
- **æ•°æ®é‡**ï¼šéœ€è¦å­˜å‚¨ 100ä¸‡+ æ¡è®°å½•
- **æ•°æ®å¢é•¿**ï¼šæ—¥å‡æ–°å¢ 10,000 æ¡è®°å½•
- **æ€§èƒ½è¦æ±‚**ï¼šæŸ¥è¯¢å“åº”æ—¶é—´ < 100ms
- **å­˜å‚¨å‘¨æœŸ**ï¼šæ•°æ®ä¿å­˜ 30 å¤©

### 2.2. ç³»ç»Ÿè§„æ¨¡

- **æ•°æ®åº“æ–‡ä»¶å¤§å°**ï¼šåˆå§‹ 50MBï¼Œä¼˜åŒ–åç›®æ ‡ < 30MB
- **è¡¨æ•°é‡**ï¼š15+ ä¸ªä¸šåŠ¡è¡¨
- **æœ€å¤§è¡¨è®°å½•æ•°**ï¼š100ä¸‡æ¡
- **ç´¢å¼•æ•°é‡**ï¼š20+ ä¸ªç´¢å¼•
- **å­˜å‚¨ç©ºé—´èŠ‚çœç›®æ ‡**ï¼š40%+

---

## 3. æŠ€æœ¯æŒ‘æˆ˜

### 3.1. å­˜å‚¨ç©ºé—´æŒ‘æˆ˜

**æŒ‘æˆ˜**ï¼š

- æ•°æ®åº“æ–‡ä»¶è¿‡å¤§ï¼Œè¶…å‡ºå­˜å‚¨é™åˆ¶
- æ•°æ®ç±»å‹é€‰æ‹©ä¸å½“ï¼Œæµªè´¹å­˜å‚¨ç©ºé—´
- ç´¢å¼•å ç”¨å¤§é‡ç©ºé—´
- æ•°æ®å†—ä½™ä¸¥é‡

**å½±å“**ï¼š

- åº”ç”¨æ— æ³•å®‰è£…æˆ–è¿è¡Œ
- ç”¨æˆ·ä½“éªŒå·®
- å­˜å‚¨æˆæœ¬é«˜

### 3.2. æ•°æ®å†—ä½™æŒ‘æˆ˜

**æŒ‘æˆ˜**ï¼š

- é‡å¤æ•°æ®å ç”¨ç©ºé—´
- æœªä½¿ç”¨çš„åˆ—å ç”¨ç©ºé—´
- è¿‡é•¿çš„æ–‡æœ¬å­—æ®µ
- ä¸å¿…è¦çš„ç´¢å¼•

**å½±å“**ï¼š

- å­˜å‚¨ç©ºé—´æµªè´¹
- å¤‡ä»½å’Œæ¢å¤æ—¶é—´é•¿
- æ€§èƒ½ä¸‹é™

### 3.3. ç¢ç‰‡åŒ–æŒ‘æˆ˜

**æŒ‘æˆ˜**ï¼š

- åˆ é™¤æ“ä½œåç©ºé—´æ— æ³•å›æ”¶
- é¢‘ç¹æ›´æ–°å¯¼è‡´ç¢ç‰‡åŒ–
- ç¢ç‰‡åŒ–å½±å“æŸ¥è¯¢æ€§èƒ½
- VACUUMæ“ä½œè€—æ—¶

**å½±å“**ï¼š

- å­˜å‚¨ç©ºé—´æµªè´¹
- æŸ¥è¯¢æ€§èƒ½ä¸‹é™
- ç»´æŠ¤æˆæœ¬é«˜

---

## 4. ä¼˜åŒ–æ–¹æ¡ˆ

### 4.1. æ•°æ®ç±»å‹ä¼˜åŒ–

**ä¼˜åŒ–å‰è®¾è®¡**ï¼š

```sql
-- åŸå§‹è®¾è®¡ï¼šæµªè´¹ç©ºé—´
CREATE TABLE users (
    id TEXT PRIMARY KEY,              -- TEXTç±»å‹ï¼Œæµªè´¹ç©ºé—´
    name TEXT,                        -- æœªé™åˆ¶é•¿åº¦
    email TEXT,                       -- æœªé™åˆ¶é•¿åº¦
    age TEXT,                         -- åº”è¯¥ç”¨INTEGER
    created_at TEXT,                  -- åº”è¯¥ç”¨INTEGERæ—¶é—´æˆ³
    status TEXT                       -- åº”è¯¥ç”¨INTEGERæˆ–BOOLEAN
);
```

**ä¼˜åŒ–åè®¾è®¡**ï¼š

```sql
-- ä¼˜åŒ–åè®¾è®¡ï¼šèŠ‚çœç©ºé—´
CREATE TABLE users (
    id INTEGER PRIMARY KEY,           -- INTEGERç±»å‹ï¼ŒèŠ‚çœç©ºé—´
    name TEXT,                        -- ä¿æŒTEXTï¼ˆéœ€è¦ï¼‰
    email TEXT,                       -- ä¿æŒTEXTï¼ˆéœ€è¦ï¼‰
    age INTEGER,                      -- INTEGERç±»å‹ï¼ŒèŠ‚çœç©ºé—´
    created_at INTEGER,               -- INTEGERæ—¶é—´æˆ³ï¼ŒèŠ‚çœç©ºé—´
    status INTEGER                    -- INTEGERç±»å‹ï¼ŒèŠ‚çœç©ºé—´
);

-- ç©ºé—´èŠ‚çœå¯¹æ¯”
-- TEXT id (UUID): ~36å­—èŠ‚
-- INTEGER id: 8å­—èŠ‚
-- èŠ‚çœï¼š78%
```

### 4.2. è¡¨ç»“æ„ä¼˜åŒ–

**åˆ—é¡ºåºä¼˜åŒ–**ï¼š

```sql
-- ä¼˜åŒ–åˆ—é¡ºåºï¼šå¸¸ç”¨åˆ—åœ¨å‰ï¼Œä¸å¸¸ç”¨åˆ—åœ¨å
CREATE TABLE orders (
    id INTEGER PRIMARY KEY,           -- æœ€å¸¸ç”¨
    user_id INTEGER NOT NULL,         -- å¸¸ç”¨
    status INTEGER NOT NULL,          -- å¸¸ç”¨
    amount REAL NOT NULL,             -- å¸¸ç”¨
    created_at INTEGER NOT NULL,      -- å¸¸ç”¨
    updated_at INTEGER,               -- ä¸å¸¸ç”¨
    notes TEXT,                       -- å¾ˆå°‘ç”¨
    metadata TEXT                     -- å¾ˆå°‘ç”¨
);
```

**NULLå€¼ä¼˜åŒ–**ï¼š

```sql
-- é¿å…ä¸å¿…è¦çš„NULLå€¼
CREATE TABLE products (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,               -- å¿…é¡»å­—æ®µï¼ŒNOT NULL
    price REAL NOT NULL,              -- å¿…é¡»å­—æ®µï¼ŒNOT NULL
    description TEXT,                 -- å¯é€‰å­—æ®µï¼Œå…è®¸NULL
    category_id INTEGER               -- å¯é€‰å­—æ®µï¼Œå…è®¸NULL
);
```

### 4.3. ç´¢å¼•ä¼˜åŒ–

**ç´¢å¼•é€‰æ‹©ä¼˜åŒ–**ï¼š

```sql
-- åªåˆ›å»ºå¿…è¦çš„ç´¢å¼•
CREATE TABLE logs (
    id INTEGER PRIMARY KEY,
    user_id INTEGER NOT NULL,
    action TEXT NOT NULL,
    timestamp INTEGER NOT NULL,
    details TEXT
);

-- åªåˆ›å»ºå¸¸ç”¨æŸ¥è¯¢çš„ç´¢å¼•
CREATE INDEX idx_user_time ON logs(user_id, timestamp DESC);
-- ä¸åˆ›å»ºä¸å¸¸ç”¨çš„ç´¢å¼•ï¼ŒèŠ‚çœç©ºé—´

-- ä½¿ç”¨éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•å¸¸ç”¨æ•°æ®ï¼‰
CREATE INDEX idx_recent_logs ON logs(timestamp DESC)
WHERE timestamp > strftime('%s', 'now', '-7 days');
```

**è¦†ç›–ç´¢å¼•ä¼˜åŒ–**ï¼š

```sql
-- è¦†ç›–ç´¢å¼•ï¼šåŒ…å«æŸ¥è¯¢æ‰€éœ€çš„æ‰€æœ‰åˆ—
CREATE INDEX idx_user_action_time ON logs(user_id, action, timestamp DESC);

-- æŸ¥è¯¢æ—¶ä½¿ç”¨è¦†ç›–ç´¢å¼•ï¼Œé¿å…å›è¡¨
SELECT user_id, action, timestamp
FROM logs
WHERE user_id = 123
ORDER BY timestamp DESC
LIMIT 100;
```

### 4.4. æ•°æ®å‹ç¼©

**æ–‡æœ¬å‹ç¼©**ï¼š

```python
import sqlite3
import gzip
import json

def compress_text(text):
    """å‹ç¼©æ–‡æœ¬æ•°æ®"""
    if text is None:
        return None
    return gzip.compress(text.encode('utf-8'))

def decompress_text(compressed):
    """è§£å‹æ–‡æœ¬æ•°æ®"""
    if compressed is None:
        return None
    return gzip.decompress(compressed).decode('utf-8')

# ä½¿ç”¨ç¤ºä¾‹
def store_compressed_data(conn, data):
    """å­˜å‚¨å‹ç¼©æ•°æ®"""
    cursor = conn.cursor()
    compressed = compress_text(json.dumps(data))
    cursor.execute(
        'INSERT INTO data_table (compressed_data) VALUES (?)',
        (compressed,)
    )
    conn.commit()

# ç©ºé—´èŠ‚çœï¼šé€šå¸¸å¯èŠ‚çœ50-70%
```

**JSONæ•°æ®ä¼˜åŒ–**ï¼š

```sql
-- ä½¿ç”¨JSONå­˜å‚¨ç»“æ„åŒ–æ•°æ®ï¼Œé¿å…å¤šè¡¨JOIN
CREATE TABLE user_profiles (
    id INTEGER PRIMARY KEY,
    user_id INTEGER NOT NULL,
    profile_data TEXT,  -- JSONæ ¼å¼ï¼Œå‹ç¼©å­˜å‚¨
    -- é¿å…åˆ›å»ºå¤šä¸ªå…³è”è¡¨
);

-- ä½¿ç”¨ç”Ÿæˆåˆ—æå–å¸¸ç”¨å­—æ®µ
CREATE TABLE user_profiles (
    id INTEGER PRIMARY KEY,
    user_id INTEGER NOT NULL,
    profile_data TEXT,
    name TEXT GENERATED ALWAYS AS (json_extract(profile_data, '$.name')) VIRTUAL,
    email TEXT GENERATED ALWAYS AS (json_extract(profile_data, '$.email')) VIRTUAL
);
```

### 4.5. åˆ—å­˜å‚¨æ¨¡æ‹Ÿæ–¹æ¡ˆ

**è™½ç„¶SQLiteåŸç”Ÿä¸æ”¯æŒåˆ—å­˜å‚¨ï¼Œä½†å¯ä»¥é€šè¿‡è®¾è®¡æ¨¡å¼æ¨¡æ‹Ÿåˆ—å­˜å‚¨çš„ä¼˜åŠ¿**ï¼š

**æ–¹æ¡ˆ1ï¼šå‚ç›´åˆ†åŒºï¼ˆåˆ—åˆ†ç¦»ï¼‰**ï¼š

```sql
-- åœºæ™¯ï¼šç”¨æˆ·è¡¨æœ‰å¤§é‡åˆ—ï¼Œä½†æŸ¥è¯¢é€šå¸¸åªéœ€è¦éƒ¨åˆ†åˆ—
-- åŸå§‹è®¾è®¡ï¼šæ‰€æœ‰åˆ—åœ¨ä¸€ä¸ªè¡¨
CREATE TABLE users_full (
    id INTEGER PRIMARY KEY,
    name TEXT,
    email TEXT,
    phone TEXT,
    address TEXT,
    profile_text TEXT,      -- å¤§æ–‡æœ¬å­—æ®µ
    metadata_json TEXT,     -- JSONæ•°æ®
    created_at INTEGER,
    updated_at INTEGER
);

-- ä¼˜åŒ–è®¾è®¡ï¼šæ‹†åˆ†ä¸ºæ ¸å¿ƒè¡¨å’Œæ‰©å±•è¡¨
CREATE TABLE users_core (
    id INTEGER PRIMARY KEY,
    name TEXT,
    email TEXT,
    phone TEXT,
    created_at INTEGER,
    updated_at INTEGER
);

CREATE TABLE users_extended (
    id INTEGER PRIMARY KEY,
    address TEXT,
    profile_text TEXT,
    metadata_json TEXT,
    FOREIGN KEY (id) REFERENCES users_core(id)
);

-- æŸ¥è¯¢ä¼˜åŠ¿ï¼š
-- 1. åªæŸ¥è¯¢æ ¸å¿ƒåˆ—æ—¶ï¼Œåªæ‰«æusers_coreè¡¨ï¼ˆæ›´å°ï¼‰
-- 2. æ‰©å±•åˆ—æŒ‰éœ€JOINï¼Œå‡å°‘I/O
SELECT id, name, email FROM users_core WHERE id = 123;  -- åªæ‰«ææ ¸å¿ƒåˆ—
```

**æ–¹æ¡ˆ2ï¼šåˆ—å¼ç‰©åŒ–è§†å›¾ï¼ˆåˆ†ææŸ¥è¯¢ä¼˜åŒ–ï¼‰**ï¼š

```python
import sqlite3
import json
from collections import defaultdict

class ColumnarView:
    """åˆ—å¼ç‰©åŒ–è§†å›¾ï¼šæ¨¡æ‹Ÿåˆ—å­˜å‚¨ç”¨äºåˆ†ææŸ¥è¯¢"""

    def __init__(self, conn, table_name, columns):
        self.conn = conn
        self.table_name = table_name
        self.columns = columns
        self.column_tables = {}

    def create_column_tables(self):
        """ä¸ºæ¯åˆ—åˆ›å»ºå•ç‹¬çš„è¡¨ï¼ˆæ¨¡æ‹Ÿåˆ—å­˜å‚¨ï¼‰"""
        for col in self.columns:
            col_table = f"{self.table_name}_{col}_column"
            self.column_tables[col] = col_table

            self.conn.execute(f"""
                CREATE TABLE IF NOT EXISTS {col_table} (
                    row_id INTEGER PRIMARY KEY,
                    value TEXT
                )
            """)

    def populate_columns(self):
        """ä»åŸå§‹è¡¨å¡«å……åˆ—è¡¨"""
        cursor = self.conn.cursor()

        # è·å–æ‰€æœ‰è¡Œ
        cursor.execute(f"SELECT rowid, {', '.join(self.columns)} FROM {self.table_name}")
        rows = cursor.fetchall()

        # æŒ‰åˆ—å­˜å‚¨
        for col_idx, col in enumerate(self.columns):
            col_table = self.column_tables[col]
            col_values = [(row[0], str(row[col_idx + 1])) for row in rows]

            cursor.executemany(
                f"INSERT OR REPLACE INTO {col_table} (row_id, value) VALUES (?, ?)",
                col_values
            )

        self.conn.commit()

    def query_column(self, column, condition=None):
        """æŸ¥è¯¢å•åˆ—ï¼ˆåˆ—å­˜å‚¨ä¼˜åŠ¿ï¼‰"""
        col_table = self.column_tables[column]
        query = f"SELECT value FROM {col_table}"
        if condition:
            query += f" WHERE {condition}"
        return self.conn.execute(query).fetchall()

# ä½¿ç”¨ç¤ºä¾‹
conn = sqlite3.connect('analytics.db')

# åˆ›å»ºåˆ—å¼è§†å›¾
view = ColumnarView(conn, 'logs', ['user_id', 'action', 'timestamp', 'duration'])
view.create_column_tables()
view.populate_columns()

# åˆ—å­˜å‚¨æŸ¥è¯¢ä¼˜åŠ¿ï¼šåªæ‰«æéœ€è¦çš„åˆ—
results = view.query_column('action')  # åªè¯»å–actionåˆ—
```

**æ–¹æ¡ˆ3ï¼šåˆ—å‹ç¼©å­˜å‚¨**ï¼š

```python
import sqlite3
import zlib
import pickle
from collections import Counter

class ColumnCompressor:
    """åˆ—å‹ç¼©å™¨ï¼šå¯¹åˆ—æ•°æ®åº”ç”¨å‹ç¼©ç®—æ³•"""

    def __init__(self, conn):
        self.conn = conn

    def dictionary_encode(self, values):
        """å­—å…¸ç¼–ç ï¼šå°†é‡å¤å€¼æ˜ å°„åˆ°å­—å…¸ç´¢å¼•"""
        # æ„å»ºå­—å…¸
        unique_values = list(set(values))
        dictionary = {val: idx for idx, val in enumerate(unique_values)}

        # ç¼–ç 
        encoded = [dictionary[val] for val in values]

        return {
            'dictionary': unique_values,
            'encoded': encoded,
            'compression_ratio': len(encoded) / (len(values) * self._avg_value_size(values))
        }

    def run_length_encode(self, values):
        """æ¸¸ç¨‹ç¼–ç ï¼šå‹ç¼©è¿ç»­ç›¸åŒå€¼"""
        if not values:
            return {'encoded': [], 'compression_ratio': 1.0}

        encoded = []
        current_value = values[0]
        current_count = 1

        for val in values[1:]:
            if val == current_value:
                current_count += 1
            else:
                encoded.append((current_value, current_count))
                current_value = val
                current_count = 1
        encoded.append((current_value, current_count))

        original_size = len(values) * self._avg_value_size(values)
        encoded_size = len(encoded) * (self._avg_value_size([v[0] for v in encoded]) + 4)  # value + count

        return {
            'encoded': encoded,
            'compression_ratio': encoded_size / original_size if original_size > 0 else 1.0
        }

    def delta_encode(self, values):
        """å¢é‡ç¼–ç ï¼šå­˜å‚¨ç›¸é‚»å€¼çš„å·®å€¼"""
        if len(values) < 2:
            return {'base': values[0] if values else None, 'deltas': [], 'compression_ratio': 1.0}

        base = values[0]
        deltas = [values[i] - values[i-1] for i in range(1, len(values))]

        original_size = len(values) * 8  # å‡è®¾æ¯ä¸ªå€¼8å­—èŠ‚
        encoded_size = 8 + len(deltas) * self._avg_delta_size(deltas)  # base + deltas

        return {
            'base': base,
            'deltas': deltas,
            'compression_ratio': encoded_size / original_size if original_size > 0 else 1.0
        }

    def compress_column(self, table_name, column_name, method='dictionary'):
        """å‹ç¼©è¡¨ä¸­çš„åˆ—"""
        cursor = self.conn.cursor()
        cursor.execute(f"SELECT {column_name} FROM {table_name} ORDER BY rowid")
        values = [row[0] for row in cursor.fetchall()]

        if method == 'dictionary':
            result = self.dictionary_encode(values)
        elif method == 'rle':
            result = self.run_length_encode(values)
        elif method == 'delta':
            result = self.delta_encode(values)
        else:
            raise ValueError(f"Unknown compression method: {method}")

        # å­˜å‚¨å‹ç¼©åçš„æ•°æ®
        compressed_table = f"{table_name}_{column_name}_compressed"
        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {compressed_table} (
                id INTEGER PRIMARY KEY,
                compressed_data BLOB,
                metadata TEXT
            )
        """)

        compressed_blob = zlib.compress(pickle.dumps(result))
        metadata = json.dumps({
            'method': method,
            'compression_ratio': result['compression_ratio'],
            'original_count': len(values)
        })

        cursor.execute(
            f"INSERT INTO {compressed_table} (compressed_data, metadata) VALUES (?, ?)",
            (compressed_blob, metadata)
        )
        self.conn.commit()

        return result

    def _avg_value_size(self, values):
        """è®¡ç®—å¹³å‡å€¼çš„å­˜å‚¨å¤§å°"""
        if not values:
            return 0
        total_size = sum(len(str(v).encode('utf-8')) for v in values)
        return total_size / len(values)

    def _avg_delta_size(self, deltas):
        """è®¡ç®—å¹³å‡å·®å€¼çš„å­˜å‚¨å¤§å°ï¼ˆä½¿ç”¨varintï¼‰"""
        if not deltas:
            return 0
        # Varintç¼–ç ï¼šå°å€¼ç”¨æ›´å°‘å­—èŠ‚
        sizes = []
        for d in deltas:
            if abs(d) < 128:
                sizes.append(1)
            elif abs(d) < 16384:
                sizes.append(2)
            else:
                sizes.append(4)
        return sum(sizes) / len(sizes)

# ä½¿ç”¨ç¤ºä¾‹
conn = sqlite3.connect('analytics.db')
compressor = ColumnCompressor(conn)

# å¯¹ä½åŸºæ•°åˆ—ä½¿ç”¨å­—å…¸ç¼–ç 
result = compressor.compress_column('logs', 'action', method='dictionary')
print(f"å­—å…¸ç¼–ç å‹ç¼©ç‡: {result['compression_ratio']:.2%}")

# å¯¹æ’åºåçš„æ•°å€¼åˆ—ä½¿ç”¨å¢é‡ç¼–ç 
result = compressor.compress_column('logs', 'timestamp', method='delta')
print(f"å¢é‡ç¼–ç å‹ç¼©ç‡: {result['compression_ratio']:.2%}")
```

**åˆ—å­˜å‚¨æ¨¡æ‹Ÿæ•ˆæœå¯¹æ¯”**ï¼š

| æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | ç©ºé—´èŠ‚çœ | æŸ¥è¯¢æ€§èƒ½ | å®æ–½å¤æ‚åº¦ |
|------|---------|---------|---------|-----------|
| **å‚ç›´åˆ†åŒº** | å®½è¡¨ï¼ŒæŸ¥è¯¢éƒ¨åˆ†åˆ— | 20-40% | +10-30% | â­â­ |
| **åˆ—å¼ç‰©åŒ–è§†å›¾** | åˆ†ææŸ¥è¯¢ï¼Œè¯»å¤šå†™å°‘ | 30-60% | +50-200% | â­â­â­â­ |
| **åˆ—å‹ç¼©å­˜å‚¨** | ä½åŸºæ•°åˆ—ï¼Œé‡å¤å€¼å¤š | 40-80% | -5-10% | â­â­â­ |

## 5. ç¢ç‰‡æ•´ç†

**VACUUMæ“ä½œ**ï¼š

```python
def vacuum_database(conn):
    """æ‰§è¡ŒVACUUMæ•´ç†ç¢ç‰‡"""
# 1. æ‰§è¡ŒVACUUMå›æ”¶ç©ºé—´
    conn.execute('VACUUM')

# 2. é‡æ–°åˆ†æç»Ÿè®¡ä¿¡æ¯
    conn.execute('ANALYZE')

# 3. æ£€æŸ¥æ•°æ®åº“å®Œæ•´æ€§
    cursor = conn.cursor()
    cursor.execute('PRAGMA integrity_check')
    result = cursor.fetchone()
    if result[0] != 'ok':
        raise RuntimeError(f"æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {result[0]}")

# å®šæœŸæ‰§è¡ŒVACUUMï¼ˆå¦‚æ¯å‘¨ä¸€æ¬¡ï¼‰
# ç©ºé—´å›æ”¶ï¼šé€šå¸¸å¯å›æ”¶10-30%ç©ºé—´
```

**å¢é‡VACUUM**ï¼š

```sql
-- å¢é‡VACUUMï¼šé€æ­¥å›æ”¶ç©ºé—´ï¼Œä¸é˜»å¡æ“ä½œ
```

**å­˜å‚¨ç©ºé—´ä¼˜åŒ–å·¥å…·ç±»**ï¼š

```python
import sqlite3
import os
import gzip
import json
from typing import Dict, List

class StorageOptimizer:
    """å­˜å‚¨ç©ºé—´ä¼˜åŒ–å·¥å…·"""

    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)

    def analyze_storage_usage(self) -> Dict:
        """åˆ†æå­˜å‚¨ä½¿ç”¨æƒ…å†µ"""
        stats = {}

# æ–‡ä»¶å¤§å°
        stats['file_size'] = os.path.getsize(self.db_path)

# é¡µç»Ÿè®¡
        cursor = self.conn.execute('PRAGMA page_count')
        page_count = cursor.fetchone()[0]

        cursor = self.conn.execute('PRAGMA page_size')
        page_size = cursor.fetchone()[0]

        stats['total_pages'] = page_count
        stats['page_size'] = page_size
        stats['allocated_size'] = page_count * page_size

# ç©ºé—²é¡µ
        cursor = self.conn.execute('PRAGMA freelist_count')
        freelist_count = cursor.fetchone()[0]
        stats['free_pages'] = freelist_count
        stats['used_pages'] = page_count - freelist_count

# ç¢ç‰‡ç‡
        if page_count > 0:
            stats['fragmentation'] = (freelist_count / page_count) * 100
        else:
            stats['fragmentation'] = 0

# è¡¨å¤§å°åˆ†æ
        cursor = self.conn.execute("""
            SELECT name FROM sqlite_master
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
        """)
        tables = [row[0] for row in cursor.fetchall()]

        table_sizes = {}
        for table in tables:
            cursor.execute(f'SELECT COUNT(*) FROM {table}')
            row_count = cursor.fetchone()[0]
            table_sizes[table] = row_count

        stats['tables'] = table_sizes

        return stats

    def optimize_data_types(self, table_name, column_mapping):
        """ä¼˜åŒ–æ•°æ®ç±»å‹"""
# åˆ›å»ºæ–°è¡¨
        new_table = f"{table_name}_optimized"

# æ„å»ºCREATE TABLEè¯­å¥
        create_sql = f"CREATE TABLE {new_table} ("
# ... æ ¹æ®column_mappingæ„å»ºè¡¨ç»“æ„

# å¤åˆ¶æ•°æ®
        self.conn.execute(f"INSERT INTO {new_table} SELECT * FROM {table_name}")

# åˆ é™¤æ—§è¡¨ï¼Œé‡å‘½åæ–°è¡¨
        self.conn.execute(f"DROP TABLE {table_name}")
        self.conn.execute(f"ALTER TABLE {new_table} RENAME TO {table_name}")
        self.conn.commit()

    def compress_text_data(self, table_name, text_column):
        """å‹ç¼©æ–‡æœ¬æ•°æ®"""
        cursor = self.conn.cursor()

# è¯»å–æ‰€æœ‰æ•°æ®
        cursor.execute(f"SELECT id, {text_column} FROM {table_name}")
        rows = cursor.fetchall()

# å‹ç¼©å¹¶æ›´æ–°
        for row_id, text_data in rows:
            if text_data:
                compressed = gzip.compress(text_data.encode('utf-8'))
                cursor.execute(
                    f"UPDATE {table_name} SET {text_column} = ? WHERE id = ?",
                    (compressed, row_id)
                )

        self.conn.commit()
        print(f"âœ… è¡¨ {table_name} æ–‡æœ¬æ•°æ®å‹ç¼©å®Œæˆ")

    def remove_unused_indexes(self):
        """ç§»é™¤æœªä½¿ç”¨çš„ç´¢å¼•"""
        cursor = self.conn.cursor()

# è·å–æ‰€æœ‰ç´¢å¼•
        cursor.execute("""
            SELECT name, tbl_name FROM sqlite_master
            WHERE type='index' AND name NOT LIKE 'sqlite_%'
        """)
        indexes = cursor.fetchall()

# æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µï¼ˆéœ€è¦sqlite_statè¡¨ï¼‰
        unused_indexes = []
        for index_name, table_name in indexes:
# è¿™é‡Œå¯ä»¥æ·»åŠ ç´¢å¼•ä½¿ç”¨æƒ…å†µæ£€æŸ¥é€»è¾‘
# å¦‚æœç´¢å¼•æœªè¢«ä½¿ç”¨ï¼Œæ·»åŠ åˆ°unused_indexes
            pass

# åˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•
        for index_name in unused_indexes:
            cursor.execute(f"DROP INDEX {index_name}")
            print(f"  âœ… åˆ é™¤æœªä½¿ç”¨ç´¢å¼•: {index_name}")

        self.conn.commit()

    def optimize_storage(self):
        """ç»¼åˆå­˜å‚¨ä¼˜åŒ–"""
        print("å¼€å§‹å­˜å‚¨ä¼˜åŒ–...")

# 1. åˆ†æå½“å‰çŠ¶æ€
        print("\n1. åˆ†æå­˜å‚¨ä½¿ç”¨æƒ…å†µ...")
        stats = self.analyze_storage_usage()
        print(f"  æ–‡ä»¶å¤§å°: {stats['file_size'] / 1024 / 1024:.2f} MB")
        print(f"  ç¢ç‰‡ç‡: {stats['fragmentation']:.2f}%")

# 2. ç§»é™¤æœªä½¿ç”¨çš„ç´¢å¼•
        print("\n2. ç§»é™¤æœªä½¿ç”¨çš„ç´¢å¼•...")
        self.remove_unused_indexes()

# 3. æ‰§è¡ŒVACUUM
        print("\n3. æ‰§è¡ŒVACUUM...")
        self.conn.execute('VACUUM')
        self.conn.commit()

# 4. é‡æ–°åˆ†æ
        print("\n4. ä¼˜åŒ–åçŠ¶æ€...")
        new_stats = self.analyze_storage_usage()
        saved = stats['file_size'] - new_stats['file_size']
        print(f"  æ–‡ä»¶å¤§å°: {new_stats['file_size'] / 1024 / 1024:.2f} MB")
        print(f"  èŠ‚çœç©ºé—´: {saved / 1024 / 1024:.2f} MB ({(saved/stats['file_size']*100):.1f}%)")

        print("\nâœ… å­˜å‚¨ä¼˜åŒ–å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
optimizer = StorageOptimizer('mobile_app.db')

# åˆ†æå­˜å‚¨ä½¿ç”¨
stats = optimizer.analyze_storage_usage()
print(f"æ•°æ®åº“å¤§å°: {stats['file_size'] / 1024 / 1024:.2f} MB")
print(f"ç¢ç‰‡ç‡: {stats['fragmentation']:.2f}%")

# æ‰§è¡Œä¼˜åŒ–
optimizer.optimize_storage()
PRAGMA incremental_vacuum(1000);  -- å›æ”¶1000é¡µ

-- é€‚åˆå¤§æ•°æ®åº“ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
```

## 6. æ•°æ®å½’æ¡£

**æ•°æ®å½’æ¡£ç­–ç•¥**ï¼š

```python
def archive_old_data(conn, table, days=30):
    """å½’æ¡£æ—§æ•°æ®"""
    cutoff_timestamp = int(time.time()) - (days * 24 * 3600)

    cursor = conn.cursor()

# 1. å¯¼å‡ºæ—§æ•°æ®åˆ°å½’æ¡£è¡¨
    cursor.execute(f"""
        CREATE TABLE {table}_archive AS
        SELECT * FROM {table}
        WHERE created_at < ?
    """, (cutoff_timestamp,))

# 2. åˆ é™¤æ—§æ•°æ®
    cursor.execute(f"""
        DELETE FROM {table}
        WHERE created_at < ?
    """, (cutoff_timestamp,))

# 3. æ‰§è¡ŒVACUUMå›æ”¶ç©ºé—´
    conn.execute('VACUUM')

    conn.commit()

# å®šæœŸå½’æ¡£ï¼Œä¿æŒæ•°æ®åº“å¤§å°å¯æ§
```

---

## 7. ä¼˜åŒ–æ•ˆæœ

### 7.1. å­˜å‚¨ç©ºé—´èŠ‚çœ

**ä¼˜åŒ–å‰åå¯¹æ¯”**ï¼š

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | èŠ‚çœæ¯”ä¾‹ |
|--------|--------|--------|---------|
| æ•°æ®åº“æ–‡ä»¶å¤§å° | 50MB | 28MB | -44% |
| ç´¢å¼•å ç”¨ç©ºé—´ | 12MB | 6MB | -50% |
| æ•°æ®å ç”¨ç©ºé—´ | 35MB | 20MB | -43% |
| ç¢ç‰‡åŒ–ç©ºé—´ | 3MB | 0.5MB | -83% |

### 7.2. æ€§èƒ½å½±å“

**ä¼˜åŒ–å‰åå¯¹æ¯”**ï¼š

| æ“ä½œç±»å‹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | å˜åŒ– |
|---------|--------|--------|------|
| æŸ¥è¯¢æ€§èƒ½ | åŸºå‡† | +5% | ç•¥æœ‰æå‡ |
| æ’å…¥æ€§èƒ½ | åŸºå‡† | -2% | ç•¥æœ‰ä¸‹é™ |
| ç´¢å¼•ç»´æŠ¤ | åŸºå‡† | -10% | æ˜¾è‘—é™ä½ |

### 7.3. æˆæœ¬é™ä½

**ä¼˜åŒ–å‰åå¯¹æ¯”**ï¼š

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹å–„ |
|------|--------|--------|------|
| å­˜å‚¨æˆæœ¬ | åŸºå‡† | -44% | æ˜¾è‘—é™ä½ |
| å¤‡ä»½æ—¶é—´ | åŸºå‡† | -44% | æ˜¾è‘—é™ä½ |
| æ¢å¤æ—¶é—´ | åŸºå‡† | -44% | æ˜¾è‘—é™ä½ |

---

## 8. æœ€ä½³å®è·µ

### 8.1. æ•°æ®ç±»å‹é€‰æ‹©æœ€ä½³å®è·µ

1. **ä½¿ç”¨INTEGERä»£æ›¿TEXT**
   - IDå­—æ®µä½¿ç”¨INTEGER PRIMARY KEY
   - æ•°å€¼å­—æ®µä½¿ç”¨INTEGERæˆ–REAL
   - æ—¶é—´æˆ³ä½¿ç”¨INTEGER

2. **é¿å…è¿‡é•¿çš„TEXTå­—æ®µ**
   - é™åˆ¶TEXTå­—æ®µé•¿åº¦ï¼ˆå¦‚é€‚ç”¨ï¼‰
   - ä½¿ç”¨å‹ç¼©å­˜å‚¨å¤§æ–‡æœ¬
   - è€ƒè™‘å¤–éƒ¨å­˜å‚¨å¤§æ–‡ä»¶

3. **ä½¿ç”¨åˆé€‚çš„æ•°å€¼ç±»å‹**
   - å°æ•´æ•°ä½¿ç”¨INTEGER
   - æµ®ç‚¹æ•°ä½¿ç”¨REAL
   - å¸ƒå°”å€¼ä½¿ç”¨INTEGERï¼ˆ0/1ï¼‰

### 8.2. è¡¨è®¾è®¡æœ€ä½³å®è·µ

1. **åˆ—é¡ºåºä¼˜åŒ–**
   - å¸¸ç”¨åˆ—åœ¨å‰ï¼Œä¸å¸¸ç”¨åˆ—åœ¨å
   - å›ºå®šé•¿åº¦åˆ—åœ¨å‰ï¼Œå¯å˜é•¿åº¦åˆ—åœ¨å
   - å‡å°‘NULLå€¼ä½¿ç”¨

2. **é¿å…æ•°æ®å†—ä½™**
   - ä½¿ç”¨è§„èŒƒåŒ–è®¾è®¡
   - é¿å…é‡å¤æ•°æ®
   - ä½¿ç”¨å¤–é”®å…³è”

3. **åˆç†ä½¿ç”¨JSON**
   - ç»“æ„åŒ–æ•°æ®ä½¿ç”¨JSONå­˜å‚¨
   - ä½¿ç”¨ç”Ÿæˆåˆ—æå–å¸¸ç”¨å­—æ®µ
   - é¿å…è¿‡åº¦åµŒå¥—

### 8.3. ç´¢å¼•è®¾è®¡æœ€ä½³å®è·µ

1. **åªåˆ›å»ºå¿…è¦ç´¢å¼•**
   - åªä¸ºå¸¸ç”¨æŸ¥è¯¢åˆ›å»ºç´¢å¼•
   - é¿å…è¿‡åº¦ç´¢å¼•
   - å®šæœŸå®¡æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ

2. **ä½¿ç”¨éƒ¨åˆ†ç´¢å¼•**
   - åªç´¢å¼•å¸¸ç”¨æ•°æ®
   - å‡å°‘ç´¢å¼•å¤§å°
   - æå‡ç´¢å¼•ç»´æŠ¤æ€§èƒ½

3. **ä½¿ç”¨è¦†ç›–ç´¢å¼•**
   - åŒ…å«æŸ¥è¯¢æ‰€éœ€çš„æ‰€æœ‰åˆ—
   - é¿å…å›è¡¨æŸ¥è¯¢
   - æå‡æŸ¥è¯¢æ€§èƒ½

### 8.4. æ•°æ®ç®¡ç†æœ€ä½³å®è·µ

1. **å®šæœŸå½’æ¡£**
   - å®šæœŸå½’æ¡£æ—§æ•°æ®
   - ä¿æŒæ•°æ®åº“å¤§å°å¯æ§
   - æ‰§è¡ŒVACUUMå›æ”¶ç©ºé—´

2. **ç¢ç‰‡æ•´ç†**
   - å®šæœŸæ‰§è¡ŒVACUUM
   - å¤§æ•°æ®åº“ä½¿ç”¨å¢é‡VACUUM
   - ç›‘æ§ç¢ç‰‡åŒ–ç‡

3. **æ•°æ®å‹ç¼©**
   - å‹ç¼©å¤§æ–‡æœ¬æ•°æ®
   - ä½¿ç”¨JSONå­˜å‚¨ç»“æ„åŒ–æ•°æ®
   - è€ƒè™‘å¤–éƒ¨å­˜å‚¨å¤§æ–‡ä»¶

4. **åˆ—å­˜å‚¨æ¨¡æ‹Ÿä¼˜åŒ–** ğŸ†•
   - å¯¹äºå®½è¡¨ï¼Œè€ƒè™‘å‚ç›´åˆ†åŒºï¼ˆåˆ—åˆ†ç¦»ï¼‰
   - å¯¹äºåˆ†ææŸ¥è¯¢ï¼Œä½¿ç”¨åˆ—å¼ç‰©åŒ–è§†å›¾
   - å¯¹äºä½åŸºæ•°åˆ—ï¼Œåº”ç”¨åˆ—å‹ç¼©æŠ€æœ¯
   - å‚è€ƒ [4.5 åˆ—å­˜å‚¨æ¨¡æ‹Ÿæ–¹æ¡ˆ](#45-åˆ—å­˜å‚¨æ¨¡æ‹Ÿæ–¹æ¡ˆ) äº†è§£è¯¦ç»†å®ç°

---

## 9. ğŸ”— ç›¸å…³èµ„æº

- [02.01 æ•°æ®ç±»å‹ç³»ç»Ÿ](../02-æ•°æ®æ¨¡å‹/02.01-æ•°æ®ç±»å‹ç³»ç»Ÿ.md) - æ•°æ®ç±»å‹é€‰æ‹©
- [02.02 è¡¨ç»“æ„è®¾è®¡](../02-æ•°æ®æ¨¡å‹/02.02-è¡¨ç»“æ„è®¾è®¡.md) - è¡¨è®¾è®¡æœ€ä½³å®è·µ
- [03.02 ä¼˜åŒ–ç­–ç•¥](../03-æ€§èƒ½ä¼˜åŒ–/03.02-ä¼˜åŒ–ç­–ç•¥.md) - ç´¢å¼•ä¼˜åŒ–ç­–ç•¥ã€åˆ—å­˜å‚¨æ¨¡æ‹Ÿä¼˜åŒ–
- [01.03 å­˜å‚¨å¼•æ“](../01-æ ¸å¿ƒæ¶æ„/01.03-å­˜å‚¨å¼•æ“.md) - åˆ—å­˜å‚¨æ¶æ„åˆ†æã€åˆ—å‹ç¼©æŠ€æœ¯è¯¦è§£ ğŸ†•
- [08.04 PRAGMAé…ç½®](../08-ç¼–ç¨‹å®è·µ/08.04-PRAGMAé…ç½®.md) - PRAGMAé…ç½®è¯¦è§£

---

## 10. ğŸ”— äº¤å‰å¼•ç”¨

### 10.1. ç†è®ºæ¨¡å‹ ğŸ†•

- â­â­â­ [å­˜å‚¨ç†è®º](../11-ç†è®ºæ¨¡å‹/11.05-å­˜å‚¨ç†è®º.md) - å­˜å‚¨æ¨¡å‹ç†è®ºã€å­˜å‚¨æ•ˆç‡ç†è®ºã€åˆ—å­˜å‚¨ç†è®ºã€åˆ—å‹ç¼©ç†è®º ğŸ†•
- â­ [æ•°æ®æ¨¡å‹ç†è®º](../11-ç†è®ºæ¨¡å‹/11.02-æ•°æ®æ¨¡å‹ç†è®º.md) - æ•°æ®ç±»å‹é€‰æ‹©ç†è®º

### 10.2. è®¾è®¡æ¨¡å‹ ğŸ†•

- â­â­ [è®¾è®¡å†³ç­–](../12-è®¾è®¡æ¨¡å‹/12.04-è®¾è®¡å†³ç­–.md) - å­˜å‚¨è®¾è®¡å†³ç­–ã€ç©ºé—´ä¼˜åŒ–å†³ç­–
- â­ [è®¾è®¡åŸåˆ™](../12-è®¾è®¡æ¨¡å‹/12.02-è®¾è®¡åŸåˆ™.md) - ç©ºé—´æ•ˆç‡åŸåˆ™

### 10.3. æ ¸å¿ƒæ¶æ„ ğŸ†•

- â­â­â­ [å­˜å‚¨å¼•æ“](../01-æ ¸å¿ƒæ¶æ„/01.03-å­˜å‚¨å¼•æ“.md) - åˆ—å­˜å‚¨æ¶æ„åˆ†æã€åˆ—å‹ç¼©æŠ€æœ¯è¯¦è§£ã€è¡Œå­˜å‚¨vsåˆ—å­˜å‚¨å¯¹æ¯”

---

**ç»´æŠ¤è€…**ï¼šData-Science Team
**æœ€åæ›´æ–°**ï¼š2025-01-15
