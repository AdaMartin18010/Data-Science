# è¾¹ç¼˜è®¡ç®—æ•°æ®å­˜å‚¨æ¡ˆä¾‹

> **æ¡ˆä¾‹ç±»å‹**ï¼šè¾¹ç¼˜è®¡ç®—å®æˆ˜æ¡ˆä¾‹
> **åº”ç”¨åœºæ™¯**ï¼šIoTè¾¹ç¼˜è®¾å¤‡ã€ç¦»çº¿æ•°æ®å­˜å‚¨ã€è¾¹ç¼˜è®¡ç®—
> **æŠ€æœ¯ç‰¹ç‚¹**ï¼šèµ„æºå—é™ç¯å¢ƒã€ç¦»çº¿ä¼˜å…ˆã€æ•°æ®åŒæ­¥

---

## ğŸ“‘ ç›®å½•

- [è¾¹ç¼˜è®¡ç®—æ•°æ®å­˜å‚¨æ¡ˆä¾‹](#è¾¹ç¼˜è®¡ç®—æ•°æ®å­˜å‚¨æ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. åœºæ™¯æè¿°](#1-åœºæ™¯æè¿°)
    - [1.1 ä¸šåŠ¡èƒŒæ™¯](#11-ä¸šåŠ¡èƒŒæ™¯)
    - [1.2 ç³»ç»Ÿè§„æ¨¡](#12-ç³»ç»Ÿè§„æ¨¡)
  - [2. æŠ€æœ¯æŒ‘æˆ˜](#2-æŠ€æœ¯æŒ‘æˆ˜)
    - [2.1 èµ„æºå—é™](#21-èµ„æºå—é™)
    - [2.2 ç¦»çº¿ä¼˜å…ˆ](#22-ç¦»çº¿ä¼˜å…ˆ)
    - [2.3 æ•°æ®åŒæ­¥](#23-æ•°æ®åŒæ­¥)
  - [3. è§£å†³æ–¹æ¡ˆ](#3-è§£å†³æ–¹æ¡ˆ)
    - [3.1 è½»é‡çº§è®¾è®¡](#31-è½»é‡çº§è®¾è®¡)
    - [3.2 ç¦»çº¿å­˜å‚¨](#32-ç¦»çº¿å­˜å‚¨)
    - [3.3 å¢é‡åŒæ­¥](#33-å¢é‡åŒæ­¥)
  - [4. å®ç°ç»†èŠ‚](#4-å®ç°ç»†èŠ‚)
    - [4.1 Pythonå®ç°](#41-pythonå®ç°)
    - [4.2 æ•°æ®å‹ç¼©](#42-æ•°æ®å‹ç¼©)
  - [5. æ€§èƒ½ä¼˜åŒ–](#5-æ€§èƒ½ä¼˜åŒ–)
  - [6. ä¼˜åŒ–æ•ˆæœ](#6-ä¼˜åŒ–æ•ˆæœ)
  - [7. æœ€ä½³å®è·µ](#7-æœ€ä½³å®è·µ)
  - [8. ğŸ”— ç›¸å…³èµ„æº](#8--ç›¸å…³èµ„æº)

---

## 1. åœºæ™¯æè¿°

### 1.1 ä¸šåŠ¡èƒŒæ™¯

æŸè¾¹ç¼˜è®¡ç®—ç³»ç»Ÿéœ€è¦åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå­˜å‚¨æ•°æ®ï¼š

- **è®¾å¤‡ç±»å‹**ï¼šIoTä¼ æ„Ÿå™¨ã€è¾¹ç¼˜ç½‘å…³
- **å­˜å‚¨é™åˆ¶**ï¼š32MB-512MB RAMï¼Œ4GB-32GB å­˜å‚¨
- **ç½‘ç»œç¯å¢ƒ**ï¼šé—´æ­‡æ€§è¿æ¥ï¼Œå¸¦å®½å—é™
- **æ•°æ®é‡**ï¼šæ¯æ—¥ 10ä¸‡+ æ¡ä¼ æ„Ÿå™¨æ•°æ®
- **åŒæ­¥è¦æ±‚**ï¼šæ¯æ—¥åŒæ­¥åˆ°äº‘ç«¯

### 1.2 ç³»ç»Ÿè§„æ¨¡

- **è®¾å¤‡æ•°é‡**ï¼š10,000+ å°è®¾å¤‡
- **å•è®¾å¤‡æ•°æ®é‡**ï¼š100MB-1GB
- **æ€»æ•°æ®é‡**ï¼š1TB+
- **åŒæ­¥é¢‘ç‡**ï¼šæ¯æ—¥1æ¬¡
- **ç¦»çº¿æ—¶é—´**ï¼šæœ€é•¿7å¤©

---

## 2. æŠ€æœ¯æŒ‘æˆ˜

### 2.1 èµ„æºå—é™

**æŒ‘æˆ˜**ï¼š

- å†…å­˜é™åˆ¶
- å­˜å‚¨é™åˆ¶
- CPUé™åˆ¶

**è§£å†³æ–¹æ¡ˆ**ï¼š

- æœ€å°åŒ–å†…å­˜ä½¿ç”¨
- æ•°æ®å‹ç¼©
- æ‰¹é‡å¤„ç†

### 2.2 ç¦»çº¿ä¼˜å…ˆ

**æŒ‘æˆ˜**ï¼š

- ç½‘ç»œä¸ç¨³å®š
- ç¦»çº¿æ•°æ®å­˜å‚¨
- æ•°æ®å®Œæ•´æ€§

**è§£å†³æ–¹æ¡ˆ**ï¼š

- æœ¬åœ°SQLiteå­˜å‚¨
- äº‹åŠ¡ä¿è¯
- æ•°æ®æ ¡éªŒ

### 2.3 æ•°æ®åŒæ­¥

**æŒ‘æˆ˜**ï¼š

- å¢é‡åŒæ­¥
- å†²çªè§£å†³
- åŒæ­¥æ•ˆç‡

**è§£å†³æ–¹æ¡ˆ**ï¼š

- å˜æ›´è·Ÿè¸ª
- æ—¶é—´æˆ³åŒæ­¥
- å‹ç¼©ä¼ è¾“

---

## 3. è§£å†³æ–¹æ¡ˆ

### 3.1 è½»é‡çº§è®¾è®¡

```python
import sqlite3
import zlib
import json
from datetime import datetime

class EdgeStorage:
    def __init__(self, db_path: str, max_size_mb: int = 100):
        self.db_path = db_path
        self.max_size_mb = max_size_mb
        self.conn = sqlite3.connect(db_path)
        self.setup_optimized_schema()

    def setup_optimized_schema(self):
        """è®¾ç½®ä¼˜åŒ–çš„Schemaï¼ˆæœ€å°åŒ–å­˜å‚¨ï¼‰"""
        cursor = self.conn.cursor()

        # å¯ç”¨å‹ç¼©å­˜å‚¨
        cursor.execute("PRAGMA journal_mode = WAL")
        cursor.execute("PRAGMA synchronous = NORMAL")
        cursor.execute("PRAGMA cache_size = -2000")  # 2MBç¼“å­˜
        cursor.execute("PRAGMA temp_store = MEMORY")

        # åˆ›å»ºç´§å‡‘çš„è¡¨ç»“æ„
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sensor_data (
                id INTEGER PRIMARY KEY,
                sensor_id INTEGER NOT NULL,
                timestamp INTEGER NOT NULL,
                value REAL NOT NULL,
                compressed_data BLOB
            )
        """)

        # åˆ›å»ºè¦†ç›–ç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_sensor_time
            ON sensor_data(sensor_id, timestamp)
        """)

        self.conn.commit()

    def insert_sensor_data(self, sensor_id: int, value: float,
                          metadata: dict = None):
        """æ’å…¥ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå‹ç¼©å­˜å‚¨ï¼‰"""
        cursor = self.conn.cursor()
        timestamp = int(datetime.now().timestamp())

        # å‹ç¼©å…ƒæ•°æ®
        compressed = None
        if metadata:
            json_data = json.dumps(metadata).encode()
            compressed = zlib.compress(json_data)

        cursor.execute("""
            INSERT INTO sensor_data (sensor_id, timestamp, value, compressed_data)
            VALUES (?, ?, ?, ?)
        """, (sensor_id, timestamp, value, compressed))

        self.conn.commit()

        # æ£€æŸ¥å­˜å‚¨å¤§å°
        self.check_storage_size()

    def check_storage_size(self):
        """æ£€æŸ¥å­˜å‚¨å¤§å°ï¼Œå¿…è¦æ—¶æ¸…ç†"""
        import os
        file_size_mb = os.path.getsize(self.db_path) / (1024 * 1024)

        if file_size_mb > self.max_size_mb:
            self.cleanup_old_data()

    def cleanup_old_data(self, days_to_keep: int = 7):
        """æ¸…ç†æ—§æ•°æ®"""
        cursor = self.conn.cursor()
        cutoff_time = int(datetime.now().timestamp()) - (days_to_keep * 86400)

        cursor.execute("""
            DELETE FROM sensor_data
            WHERE timestamp < ?
        """, (cutoff_time,))

        # å‹ç¼©æ•°æ®åº“
        cursor.execute("VACUUM")
        self.conn.commit()
```

### 3.2 ç¦»çº¿å­˜å‚¨

```python
class OfflineFirstStorage:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.setup_sync_tracking()

    def setup_sync_tracking(self):
        """è®¾ç½®åŒæ­¥è·Ÿè¸ª"""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sync_status (
                table_name TEXT PRIMARY KEY,
                last_sync_time INTEGER NOT NULL,
                sync_state TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS change_log (
                id INTEGER PRIMARY KEY,
                table_name TEXT NOT NULL,
                record_id INTEGER NOT NULL,
                operation TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                synced INTEGER DEFAULT 0
            )
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_change_log_sync
            ON change_log(synced, timestamp)
        """)

        self.conn.commit()

    def track_change(self, table_name: str, record_id: int, operation: str):
        """è·Ÿè¸ªå˜æ›´"""
        cursor = self.conn.cursor()
        timestamp = int(datetime.now().timestamp())

        cursor.execute("""
            INSERT INTO change_log (table_name, record_id, operation, timestamp)
            VALUES (?, ?, ?, ?)
        """, (table_name, record_id, operation, timestamp))

        self.conn.commit()

    def get_unsynced_changes(self, limit: int = 1000):
        """è·å–æœªåŒæ­¥çš„å˜æ›´"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT * FROM change_log
            WHERE synced = 0
            ORDER BY timestamp
            LIMIT ?
        """, (limit,))

        return cursor.fetchall()

    def mark_synced(self, change_ids: list):
        """æ ‡è®°ä¸ºå·²åŒæ­¥"""
        cursor = self.conn.cursor()
        placeholders = ','.join(['?'] * len(change_ids))
        cursor.execute(f"UPDATE change_log SET synced = 1 WHERE id IN ({placeholders})", change_ids)
        self.conn.commit()
```

### 3.3 å¢é‡åŒæ­¥

```python
class IncrementalSync:
    def __init__(self, local_db: str, remote_url: str):
        self.local_db = local_db
        self.remote_url = remote_url
        self.conn = sqlite3.connect(local_db)

    def sync_to_cloud(self):
        """åŒæ­¥åˆ°äº‘ç«¯"""
        offline_storage = OfflineFirstStorage(self.local_db)
        changes = offline_storage.get_unsynced_changes()

        if not changes:
            return

        # å‡†å¤‡åŒæ­¥æ•°æ®
        sync_data = []
        for change in changes:
            sync_data.append({
                'id': change[0],
                'table': change[1],
                'record_id': change[2],
                'operation': change[3],
                'timestamp': change[4]
            })

        # å‘é€åˆ°äº‘ç«¯ï¼ˆç®€åŒ–å®ç°ï¼‰
        try:
            self.send_to_cloud(sync_data)
            # æ ‡è®°ä¸ºå·²åŒæ­¥
            change_ids = [c['id'] for c in sync_data]
            offline_storage.mark_synced(change_ids)
        except Exception as e:
            print(f"åŒæ­¥å¤±è´¥: {e}")
            # ä¿æŒæœªåŒæ­¥çŠ¶æ€ï¼Œä¸‹æ¬¡é‡è¯•

    def send_to_cloud(self, data: list):
        """å‘é€æ•°æ®åˆ°äº‘ç«¯"""
        import requests
        import gzip

        # å‹ç¼©æ•°æ®
        json_data = json.dumps(data).encode()
        compressed = gzip.compress(json_data)

        # å‘é€ï¼ˆç®€åŒ–å®ç°ï¼‰
        response = requests.post(
            self.remote_url,
            data=compressed,
            headers={'Content-Encoding': 'gzip'}
        )
        response.raise_for_status()

    def sync_from_cloud(self, updates: list):
        """ä»äº‘ç«¯åŒæ­¥æ›´æ–°"""
        cursor = self.conn.cursor()

        for update in updates:
            if update['operation'] == 'INSERT':
                cursor.execute(update['sql'], update['params'])
            elif update['operation'] == 'UPDATE':
                cursor.execute(update['sql'], update['params'])
            elif update['operation'] == 'DELETE':
                cursor.execute(update['sql'], update['params'])

        self.conn.commit()
```

---

## 4. å®ç°ç»†èŠ‚

### 4.1 Pythonå®ç°

```python
class EdgeDeviceStorage:
    def __init__(self, device_id: str, db_path: str):
        self.device_id = device_id
        self.storage = EdgeStorage(db_path)
        self.offline_storage = OfflineFirstStorage(db_path)
        self.sync = IncrementalSync(db_path, f"https://api.example.com/sync/{device_id}")

    def record_sensor_data(self, sensor_id: int, value: float, metadata: dict = None):
        """è®°å½•ä¼ æ„Ÿå™¨æ•°æ®"""
        # æ’å…¥æ•°æ®
        self.storage.insert_sensor_data(sensor_id, value, metadata)

        # è·Ÿè¸ªå˜æ›´
        record_id = self.storage.conn.lastrowid
        self.offline_storage.track_change('sensor_data', record_id, 'INSERT')

    def sync_data(self):
        """åŒæ­¥æ•°æ®"""
        try:
            self.sync.sync_to_cloud()
        except Exception as e:
            print(f"åŒæ­¥å¤±è´¥ï¼Œå°†åœ¨ä¸‹æ¬¡é‡è¯•: {e}")
```

### 4.2 æ•°æ®å‹ç¼©

```python
class DataCompression:
    @staticmethod
    def compress_json(data: dict) -> bytes:
        """å‹ç¼©JSONæ•°æ®"""
        json_str = json.dumps(data, separators=(',', ':'))
        return zlib.compress(json_str.encode())

    @staticmethod
    def decompress_json(compressed: bytes) -> dict:
        """è§£å‹JSONæ•°æ®"""
        decompressed = zlib.decompress(compressed)
        return json.loads(decompressed.decode())

    @staticmethod
    def compress_batch(data_list: list) -> bytes:
        """æ‰¹é‡å‹ç¼©æ•°æ®"""
        json_str = json.dumps(data_list, separators=(',', ':'))
        return zlib.compress(json_str.encode())
```

---

## 5. æ€§èƒ½ä¼˜åŒ–

- **å†…å­˜ä¼˜åŒ–**ï¼šæœ€å°åŒ–ç¼“å­˜å¤§å°
- **å­˜å‚¨ä¼˜åŒ–**ï¼šæ•°æ®å‹ç¼©ã€å®šæœŸæ¸…ç†
- **åŒæ­¥ä¼˜åŒ–**ï¼šå¢é‡åŒæ­¥ã€æ‰¹é‡ä¼ è¾“
- **æŸ¥è¯¢ä¼˜åŒ–**ï¼šè¦†ç›–ç´¢å¼•ã€æ—¶é—´åˆ†åŒº

---

## 6. ä¼˜åŒ–æ•ˆæœ

- **å­˜å‚¨ç©ºé—´**ï¼šå‹ç¼©åå‡å°‘ 60%+
- **å†…å­˜ä½¿ç”¨**ï¼š< 10MB
- **åŒæ­¥æ•ˆç‡**ï¼šå¢é‡åŒæ­¥å‡å°‘ 80%+ ä¼ è¾“é‡
- **ç¦»çº¿èƒ½åŠ›**ï¼šæ”¯æŒ 7å¤©+ ç¦»çº¿å­˜å‚¨

---

## 7. æœ€ä½³å®è·µ

1. **èµ„æºç®¡ç†**
   - é™åˆ¶æ•°æ®åº“å¤§å°
   - å®šæœŸæ¸…ç†æ—§æ•°æ®
   - ä¼˜åŒ–ç¼“å­˜é…ç½®

2. **ç¦»çº¿ç­–ç•¥**
   - æœ¬åœ°ä¼˜å…ˆå­˜å‚¨
   - å˜æ›´è·Ÿè¸ª
   - å†²çªè§£å†³

3. **åŒæ­¥ä¼˜åŒ–**
   - å¢é‡åŒæ­¥
   - æ•°æ®å‹ç¼©
   - æ‰¹é‡ä¼ è¾“

---

## 8. ğŸ”— ç›¸å…³èµ„æº

- [ç§»åŠ¨åº”ç”¨ç¦»çº¿æ•°æ®åŒæ­¥](./09-ç§»åŠ¨åº”ç”¨ç¦»çº¿æ•°æ®åŒæ­¥æ¡ˆä¾‹.md) - ç¦»çº¿åŒæ­¥
- [æ€§èƒ½ä¼˜åŒ–](../03-æ€§èƒ½ä¼˜åŒ–/) - æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- [å­˜å‚¨ç©ºé—´ä¼˜åŒ–æ¡ˆä¾‹](./08-å­˜å‚¨ç©ºé—´ä¼˜åŒ–æ¡ˆä¾‹.md) - å­˜å‚¨ä¼˜åŒ–

---

**æœ€åæ›´æ–°**: 2025-12-05
**ç»´æŠ¤è€…**: Data-Science Team
