# DataOpsé©±åŠ¨çš„SQLiteæ•°æ®åº“CI/CDå®è·µ

> **åˆ›å»ºæ—¥æœŸ**: 2025-12-05
> **ç‰ˆæœ¬**: SQLite 3.47.x
> **éš¾åº¦**: â­â­â­â­â­
> **é€‚ç”¨åœºæ™¯**: DataOpsæ–¹æ³•è®ºã€æ•°æ®è´¨é‡ä¿è¯ã€æ•°æ®æ²»ç†ã€åˆ†æç®¡é“CI/CD

---

## ğŸ“‘ ç›®å½•

- [DataOpsé©±åŠ¨çš„SQLiteæ•°æ®åº“CI/CDå®è·µ](#dataopsé©±åŠ¨çš„sqliteæ•°æ®åº“cicdå®è·µ)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [DataOpsæ€»è§ˆ](#dataopsæ€»è§ˆ)
  - [1. DataOpsæ–¹æ³•è®º](#1-dataopsæ–¹æ³•è®º)
    - [1.1 DataOpsæ ¸å¿ƒåŸåˆ™](#11-dataopsæ ¸å¿ƒåŸåˆ™)
    - [1.2 DataOpsæµç¨‹](#12-dataopsæµç¨‹)
    - [1.3 DataOpså·¥å…·é“¾](#13-dataopså·¥å…·é“¾)
  - [2. æ•°æ®è´¨é‡ä¿è¯](#2-æ•°æ®è´¨é‡ä¿è¯)
    - [2.1 æ•°æ®è´¨é‡åº¦é‡](#21-æ•°æ®è´¨é‡åº¦é‡)
    - [2.2 æ•°æ®è´¨é‡æ£€æŸ¥](#22-æ•°æ®è´¨é‡æ£€æŸ¥)
    - [2.3 æ•°æ®è´¨é‡ç›‘æ§](#23-æ•°æ®è´¨é‡ç›‘æ§)
  - [3. æ•°æ®æ²»ç†æµç¨‹](#3-æ•°æ®æ²»ç†æµç¨‹)
    - [3.1 æ•°æ®è¡€ç¼˜è¿½è¸ª](#31-æ•°æ®è¡€ç¼˜è¿½è¸ª)
    - [3.2 æ•°æ®åˆ†ç±»åˆ†çº§](#32-æ•°æ®åˆ†ç±»åˆ†çº§)
    - [3.3 æ•°æ®è®¿é—®æ§åˆ¶](#33-æ•°æ®è®¿é—®æ§åˆ¶)
  - [4. åˆ†æç®¡é“CI/CD](#4-åˆ†æç®¡é“cicd)
    - [4.1 æ•°æ®ç®¡é“ç‰ˆæœ¬æ§åˆ¶](#41-æ•°æ®ç®¡é“ç‰ˆæœ¬æ§åˆ¶)
    - [4.2 æ•°æ®ç®¡é“æµ‹è¯•](#42-æ•°æ®ç®¡é“æµ‹è¯•)
    - [4.3 æ•°æ®ç®¡é“éƒ¨ç½²](#43-æ•°æ®ç®¡é“éƒ¨ç½²)
  - [5. å®ç°ç»†èŠ‚](#5-å®ç°ç»†èŠ‚)
    - [5.1 Pythonå®ç°](#51-pythonå®ç°)
    - [5.2 é›†æˆæ–¹æ¡ˆ](#52-é›†æˆæ–¹æ¡ˆ)
  - [6. æœ€ä½³å®è·µ](#6-æœ€ä½³å®è·µ)
    - [6.1 DataOpsæœ€ä½³å®è·µ](#61-dataopsæœ€ä½³å®è·µ)
    - [6.2 æ•°æ®æ²»ç†æœ€ä½³å®è·µ](#62-æ•°æ®æ²»ç†æœ€ä½³å®è·µ)
  - [7. ğŸ”— ç›¸å…³èµ„æº](#7--ç›¸å…³èµ„æº)
    - [å†…éƒ¨èµ„æº](#å†…éƒ¨èµ„æº)
    - [å¤–éƒ¨èµ„æº](#å¤–éƒ¨èµ„æº)

---

## DataOpsæ€»è§ˆ

```text
DataOpsä½“ç³»
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DataOpsæ ¸å¿ƒç»„ä»¶                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. æ•°æ®è´¨é‡ä¿è¯                                                          â”‚
â”‚    â€¢ æ•°æ®è´¨é‡åº¦é‡                                                        â”‚
â”‚    â€¢ æ•°æ®è´¨é‡æ£€æŸ¥                                                        â”‚
â”‚    â€¢ æ•°æ®è´¨é‡ç›‘æ§                                                        â”‚
â”‚                                                                         â”‚
â”‚ 2. æ•°æ®æ²»ç†æµç¨‹                                                          â”‚
â”‚    â€¢ æ•°æ®è¡€ç¼˜è¿½è¸ª                                                        â”‚
â”‚    â€¢ æ•°æ®åˆ†ç±»åˆ†çº§                                                        â”‚
â”‚    â€¢ æ•°æ®è®¿é—®æ§åˆ¶                                                        â”‚
â”‚                                                                         â”‚
â”‚ 3. åˆ†æç®¡é“CI/CD                                                         â”‚
â”‚    â€¢ æ•°æ®ç®¡é“ç‰ˆæœ¬æ§åˆ¶                                                    â”‚
â”‚    â€¢ æ•°æ®ç®¡é“æµ‹è¯•                                                        â”‚
â”‚    â€¢ æ•°æ®ç®¡é“éƒ¨ç½²                                                        â”‚
â”‚                                                                         â”‚
â”‚ 4. åä½œä¸é€æ˜åº¦                                                          â”‚
â”‚    â€¢ æ•°æ®ç›®å½•                                                            â”‚
â”‚    â€¢ æ•°æ®æ–‡æ¡£                                                            â”‚
â”‚    â€¢ åä½œå·¥å…·                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. DataOpsæ–¹æ³•è®º

### 1.1 DataOpsæ ¸å¿ƒåŸåˆ™

```python
class DataOpsPrinciples:
    """
    DataOpsæ ¸å¿ƒåŸåˆ™ï¼š
    1. æŒç»­é›†æˆï¼šæ•°æ®ç®¡é“æŒç»­é›†æˆå’Œæµ‹è¯•
    2. æŒç»­äº¤ä»˜ï¼šæ•°æ®äº§å“æŒç»­äº¤ä»˜
    3. ç‰ˆæœ¬æ§åˆ¶ï¼šæ•°æ®å’Œä»£ç ç‰ˆæœ¬æ§åˆ¶
    4. è‡ªåŠ¨åŒ–ï¼šè‡ªåŠ¨åŒ–æµ‹è¯•å’Œéƒ¨ç½²
    5. ç›‘æ§ï¼šå®æ—¶ç›‘æ§å’Œå‘Šè­¦
    6. åä½œï¼šè·¨å›¢é˜Ÿåä½œå’Œæ²Ÿé€š
    7. è´¨é‡ï¼šæ•°æ®è´¨é‡ä¿è¯
    8. æ²»ç†ï¼šæ•°æ®æ²»ç†å’Œåˆè§„
    """

    def __init__(self):
        self.principles = {
            'continuous_integration': 'æ•°æ®ç®¡é“æŒç»­é›†æˆå’Œæµ‹è¯•',
            'continuous_delivery': 'æ•°æ®äº§å“æŒç»­äº¤ä»˜',
            'version_control': 'æ•°æ®å’Œä»£ç ç‰ˆæœ¬æ§åˆ¶',
            'automation': 'è‡ªåŠ¨åŒ–æµ‹è¯•å’Œéƒ¨ç½²',
            'monitoring': 'å®æ—¶ç›‘æ§å’Œå‘Šè­¦',
            'collaboration': 'è·¨å›¢é˜Ÿåä½œå’Œæ²Ÿé€š',
            'quality': 'æ•°æ®è´¨é‡ä¿è¯',
            'governance': 'æ•°æ®æ²»ç†å’Œåˆè§„'
        }
```

### 1.2 DataOpsæµç¨‹

```python
import sqlite3
from typing import List, Dict
from datetime import datetime

class DataOpsPipeline:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self.setup_dataops_tables()

    def setup_dataops_tables(self):
        """è®¾ç½®DataOpsç›¸å…³è¡¨"""
        cursor = self.conn.cursor()

        # æ•°æ®ç®¡é“è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_pipelines (
                pipeline_id TEXT PRIMARY KEY,
                pipeline_name TEXT NOT NULL,
                version TEXT NOT NULL,
                status TEXT NOT NULL,
                created_at INTEGER NOT NULL,
                updated_at INTEGER NOT NULL
            )
        """)

        # æ•°æ®è´¨é‡æ£€æŸ¥è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_quality_checks (
                check_id TEXT PRIMARY KEY,
                pipeline_id TEXT NOT NULL,
                check_type TEXT NOT NULL,
                check_config TEXT NOT NULL,
                result TEXT,
                passed INTEGER,
                executed_at INTEGER,
                FOREIGN KEY (pipeline_id) REFERENCES data_pipelines(pipeline_id)
            )
        """)

        # æ•°æ®è¡€ç¼˜è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_lineage (
                lineage_id TEXT PRIMARY KEY,
                source_table TEXT NOT NULL,
                target_table TEXT NOT NULL,
                transformation TEXT,
                pipeline_id TEXT,
                created_at INTEGER NOT NULL
            )
        """)

        self.conn.commit()

    def create_pipeline(self, pipeline_name: str, version: str = "1.0.0") -> str:
        """åˆ›å»ºæ•°æ®ç®¡é“"""
        pipeline_id = f"{pipeline_name}_{version}_{int(datetime.now().timestamp())}"
        cursor = self.conn.cursor()

        cursor.execute("""
            INSERT INTO data_pipelines
            (pipeline_id, pipeline_name, version, status, created_at, updated_at)
            VALUES (?, ?, ?, 'created', strftime('%s', 'now'), strftime('%s', 'now'))
        """, (pipeline_id, pipeline_name, version))

        self.conn.commit()
        return pipeline_id

    def update_pipeline_status(self, pipeline_id: str, status: str):
        """æ›´æ–°ç®¡é“çŠ¶æ€"""
        cursor = self.conn.cursor()
        cursor.execute("""
            UPDATE data_pipelines
            SET status = ?, updated_at = strftime('%s', 'now')
            WHERE pipeline_id = ?
        """, (status, pipeline_id))
        self.conn.commit()
```

### 1.3 DataOpså·¥å…·é“¾

```python
class DataOpsToolchain:
    """DataOpså·¥å…·é“¾"""

    def __init__(self):
        self.tools = {
            'version_control': ['Git', 'DVC'],
            'testing': ['Great Expectations', 'dbt test'],
            'orchestration': ['Airflow', 'Prefect', 'Dagster'],
            'monitoring': ['DataDog', 'Prometheus', 'Grafana'],
            'quality': ['Great Expectations', 'Pandera'],
            'governance': ['Collibra', 'Alation', 'DataHub']
        }

    def get_recommended_tools(self, use_case: str) -> List[str]:
        """è·å–æ¨èå·¥å…·"""
        recommendations = {
            'data_quality': ['Great Expectations', 'Pandera'],
            'data_testing': ['dbt test', 'Great Expectations'],
            'orchestration': ['Airflow', 'Prefect'],
            'monitoring': ['DataDog', 'Grafana']
        }
        return recommendations.get(use_case, [])
```

---

## 2. æ•°æ®è´¨é‡ä¿è¯

### 2.1 æ•°æ®è´¨é‡åº¦é‡

```python
class DataQualityMetrics:
    def __init__(self, conn):
        self.conn = conn

    def calculate_completeness(self, table_name: str, column_name: str) -> float:
        """è®¡ç®—å®Œæ•´æ€§"""
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT
                COUNT(*) as total,
                COUNT({column_name}) as non_null
            FROM {table_name}
        """)

        total, non_null = cursor.fetchone()
        return non_null / total if total > 0 else 0.0

    def calculate_accuracy(self, table_name: str, validation_query: str) -> float:
        """è®¡ç®—å‡†ç¡®æ€§"""
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT COUNT(*) FROM {table_name}
            WHERE {validation_query}
        """)

        valid_count = cursor.fetchone()[0]

        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        total_count = cursor.fetchone()[0]

        return valid_count / total_count if total_count > 0 else 0.0

    def calculate_consistency(self, table1: str, table2: str,
                             join_condition: str) -> float:
        """è®¡ç®—ä¸€è‡´æ€§"""
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT COUNT(*) FROM {table1} t1
            LEFT JOIN {table2} t2 ON {join_condition}
            WHERE t2.id IS NULL
        """)

        inconsistent = cursor.fetchone()[0]

        cursor.execute(f"SELECT COUNT(*) FROM {table1}")
        total = cursor.fetchone()[0]

        return 1.0 - (inconsistent / total if total > 0 else 0.0)

    def calculate_timeliness(self, table_name: str,
                            timestamp_column: str,
                            expected_frequency_hours: int) -> float:
        """è®¡ç®—åŠæ—¶æ€§"""
        cursor = self.conn.cursor()
        cursor.execute(f"""
            SELECT MAX({timestamp_column}) FROM {table_name}
        """)

        last_update = cursor.fetchone()[0]
        if not last_update:
            return 0.0

        hours_since_update = (datetime.now().timestamp() - last_update) / 3600
        timeliness = max(0, 1.0 - (hours_since_update / expected_frequency_hours))

        return timeliness

    def calculate_quality_score(self, table_name: str,
                               metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        weights = {
            'completeness': 0.3,
            'accuracy': 0.3,
            'consistency': 0.2,
            'timeliness': 0.2
        }

        score = sum(metrics.get(metric, 0) * weight
                   for metric, weight in weights.items())
        return score
```

### 2.2 æ•°æ®è´¨é‡æ£€æŸ¥

```python
class DataQualityChecker:
    def __init__(self, conn):
        self.conn = conn
        self.metrics = DataQualityMetrics(conn)

    def run_quality_checks(self, pipeline_id: str,
                          table_name: str) -> Dict:
        """è¿è¡Œè´¨é‡æ£€æŸ¥"""
        checks = {}

        # å®Œæ•´æ€§æ£€æŸ¥
        cursor = self.conn.cursor()
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = [row[1] for row in cursor.fetchall()]

        for column in columns:
            completeness = self.metrics.calculate_completeness(table_name, column)
            checks[f'{column}_completeness'] = {
                'metric': 'completeness',
                'value': completeness,
                'threshold': 0.95,
                'passed': completeness >= 0.95
            }

        # å‡†ç¡®æ€§æ£€æŸ¥ï¼ˆç¤ºä¾‹ï¼šæ£€æŸ¥æ•°å€¼èŒƒå›´ï¼‰
        numeric_columns = self.get_numeric_columns(table_name)
        for column in numeric_columns:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            cursor.execute(f"""
                SELECT COUNT(*) FROM {table_name}
                WHERE {column} < 0 OR {column} > 1000000
            """)
            outlier_count = cursor.fetchone()[0]

            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            total = cursor.fetchone()[0]

            accuracy = 1.0 - (outlier_count / total if total > 0 else 0)
            checks[f'{column}_accuracy'] = {
                'metric': 'accuracy',
                'value': accuracy,
                'threshold': 0.99,
                'passed': accuracy >= 0.99
            }

        # è®°å½•æ£€æŸ¥ç»“æœ
        self.record_quality_checks(pipeline_id, checks)

        return checks

    def get_numeric_columns(self, table_name: str) -> List[str]:
        """è·å–æ•°å€¼åˆ—"""
        cursor = self.conn.cursor()
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = []
        for row in cursor.fetchall():
            col_name, col_type = row[1], row[2]
            if 'INT' in col_type.upper() or 'REAL' in col_type.upper():
                columns.append(col_name)
        return columns

    def record_quality_checks(self, pipeline_id: str, checks: Dict):
        """è®°å½•è´¨é‡æ£€æŸ¥ç»“æœ"""
        cursor = self.conn.cursor()

        for check_name, check_result in checks.items():
            cursor.execute("""
                INSERT INTO data_quality_checks
                (check_id, pipeline_id, check_type, check_config, result, passed, executed_at)
                VALUES (?, ?, ?, ?, ?, ?, strftime('%s', 'now'))
            """, (
                f"{pipeline_id}_{check_name}",
                pipeline_id,
                check_result['metric'],
                str(check_result),
                'passed' if check_result['passed'] else 'failed',
                1 if check_result['passed'] else 0
            ))

        self.conn.commit()
```

### 2.3 æ•°æ®è´¨é‡ç›‘æ§

```python
class DataQualityMonitor:
    def __init__(self, conn):
        self.conn = conn
        self.checker = DataQualityChecker(conn)

    def monitor_pipeline(self, pipeline_id: str,
                        tables: List[str]) -> Dict:
        """ç›‘æ§æ•°æ®ç®¡é“è´¨é‡"""
        results = {}

        for table in tables:
            checks = self.checker.run_quality_checks(pipeline_id, table)

            # è®¡ç®—é€šè¿‡ç‡
            passed = sum(1 for c in checks.values() if c['passed'])
            total = len(checks)
            pass_rate = passed / total if total > 0 else 0

            results[table] = {
                'checks': checks,
                'pass_rate': pass_rate,
                'status': 'healthy' if pass_rate >= 0.95 else 'degraded'
            }

            # å¦‚æœè´¨é‡ä¸‹é™ï¼Œè§¦å‘å‘Šè­¦
            if pass_rate < 0.95:
                self.trigger_alert(pipeline_id, table, pass_rate)

        return results

    def trigger_alert(self, pipeline_id: str, table: str, pass_rate: float):
        """è§¦å‘è´¨é‡å‘Šè­¦"""
        print(f"âš ï¸ è´¨é‡å‘Šè­¦: {pipeline_id}/{table} é€šè¿‡ç‡: {pass_rate:.2%}")
        # å®é™…åº”å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
```

---

## 3. æ•°æ®æ²»ç†æµç¨‹

### 3.1 æ•°æ®è¡€ç¼˜è¿½è¸ª

```python
class DataLineageTracker:
    def __init__(self, conn):
        self.conn = conn

    def track_transformation(self, source_table: str,
                           target_table: str,
                           transformation: str,
                           pipeline_id: str = None):
        """è¿½è¸ªæ•°æ®è½¬æ¢"""
        lineage_id = f"{source_table}_{target_table}_{int(datetime.now().timestamp())}"
        cursor = self.conn.cursor()

        cursor.execute("""
            INSERT INTO data_lineage
            (lineage_id, source_table, target_table, transformation, pipeline_id, created_at)
            VALUES (?, ?, ?, ?, ?, strftime('%s', 'now'))
        """, (lineage_id, source_table, target_table, transformation, pipeline_id))

        self.conn.commit()

    def get_lineage(self, table_name: str, direction: str = 'both') -> List[Dict]:
        """è·å–æ•°æ®è¡€ç¼˜"""
        cursor = self.conn.cursor()

        if direction == 'upstream' or direction == 'both':
            cursor.execute("""
                SELECT * FROM data_lineage
                WHERE target_table = ?
            """, (table_name,))
            upstream = cursor.fetchall()
        else:
            upstream = []

        if direction == 'downstream' or direction == 'both':
            cursor.execute("""
                SELECT * FROM data_lineage
                WHERE source_table = ?
            """, (table_name,))
            downstream = cursor.fetchall()
        else:
            downstream = []

        return {
            'upstream': [dict(row) for row in upstream],
            'downstream': [dict(row) for row in downstream]
        }

    def visualize_lineage(self, table_name: str) -> str:
        """å¯è§†åŒ–æ•°æ®è¡€ç¼˜"""
        lineage = self.get_lineage(table_name)

        # ç”Ÿæˆç®€å•çš„æ–‡æœ¬å›¾
        graph = f"æ•°æ®è¡€ç¼˜å›¾: {table_name}\n"
        graph += "=" * 50 + "\n"

        if lineage['upstream']:
            graph += "ä¸Šæ¸¸è¡¨:\n"
            for item in lineage['upstream']:
                graph += f"  {item['source_table']} -> {item['target_table']}\n"

        if lineage['downstream']:
            graph += "ä¸‹æ¸¸è¡¨:\n"
            for item in lineage['downstream']:
                graph += f"  {item['source_table']} -> {item['target_table']}\n"

        return graph
```

### 3.2 æ•°æ®åˆ†ç±»åˆ†çº§

```python
class DataClassification:
    def __init__(self, conn):
        self.conn = conn
        self.setup_classification_tables()

    def setup_classification_tables(self):
        """è®¾ç½®åˆ†ç±»è¡¨"""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_classification (
                table_name TEXT PRIMARY KEY,
                classification TEXT NOT NULL,
                sensitivity_level TEXT NOT NULL,
                owner TEXT,
                classification_date INTEGER NOT NULL
            )
        """)
        self.conn.commit()

    def classify_table(self, table_name: str,
                      classification: str,
                      sensitivity_level: str,
                      owner: str):
        """åˆ†ç±»è¡¨"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO data_classification
            (table_name, classification, sensitivity_level, owner, classification_date)
            VALUES (?, ?, ?, ?, strftime('%s', 'now'))
        """, (table_name, classification, sensitivity_level, owner))
        self.conn.commit()

    def get_classification(self, table_name: str) -> Dict:
        """è·å–åˆ†ç±»ä¿¡æ¯"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT * FROM data_classification
            WHERE table_name = ?
        """, (table_name,))

        result = cursor.fetchone()
        if result:
            return {
                'table_name': result[0],
                'classification': result[1],
                'sensitivity_level': result[2],
                'owner': result[3]
            }
        return None
```

### 3.3 æ•°æ®è®¿é—®æ§åˆ¶

```python
class DataAccessControl:
    def __init__(self, conn):
        self.conn = conn
        self.setup_access_control_tables()

    def setup_access_control_tables(self):
        """è®¾ç½®è®¿é—®æ§åˆ¶è¡¨"""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_access_control (
                table_name TEXT NOT NULL,
                user_role TEXT NOT NULL,
                permission TEXT NOT NULL,
                granted_at INTEGER NOT NULL,
                PRIMARY KEY (table_name, user_role)
            )
        """)
        self.conn.commit()

    def grant_permission(self, table_name: str, user_role: str, permission: str):
        """æˆäºˆæƒé™"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO data_access_control
            (table_name, user_role, permission, granted_at)
            VALUES (?, ?, ?, strftime('%s', 'now'))
        """, (table_name, user_role, permission))
        self.conn.commit()

    def check_permission(self, table_name: str, user_role: str,
                        requested_permission: str) -> bool:
        """æ£€æŸ¥æƒé™"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT permission FROM data_access_control
            WHERE table_name = ? AND user_role = ?
        """, (table_name, user_role))

        result = cursor.fetchone()
        if not result:
            return False

        granted_permission = result[0]

        # æƒé™å±‚çº§ï¼šread < write < admin
        permission_levels = {'read': 1, 'write': 2, 'admin': 3}
        granted_level = permission_levels.get(granted_permission, 0)
        requested_level = permission_levels.get(requested_permission, 0)

        return granted_level >= requested_level
```

---

## 4. åˆ†æç®¡é“CI/CD

### 4.1 æ•°æ®ç®¡é“ç‰ˆæœ¬æ§åˆ¶

```python
class DataPipelineVersionControl:
    def __init__(self, conn):
        self.conn = conn

    def version_pipeline(self, pipeline_id: str,
                        pipeline_config: Dict) -> str:
        """ç‰ˆæœ¬åŒ–æ•°æ®ç®¡é“"""
        # è®¡ç®—é…ç½®å“ˆå¸Œä½œä¸ºç‰ˆæœ¬
        import hashlib
        import json

        config_str = json.dumps(pipeline_config, sort_keys=True)
        version_hash = hashlib.sha256(config_str.encode()).hexdigest()[:8]

        # æ›´æ–°ç®¡é“ç‰ˆæœ¬
        cursor = self.conn.cursor()
        cursor.execute("""
            UPDATE data_pipelines
            SET version = ?, updated_at = strftime('%s', 'now')
            WHERE pipeline_id = ?
        """, (version_hash, pipeline_id))

        self.conn.commit()
        return version_hash

    def get_pipeline_versions(self, pipeline_name: str) -> List[Dict]:
        """è·å–ç®¡é“ç‰ˆæœ¬å†å²"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT pipeline_id, version, status, created_at, updated_at
            FROM data_pipelines
            WHERE pipeline_name = ?
            ORDER BY updated_at DESC
        """, (pipeline_name,))

        return [dict(row) for row in cursor.fetchall()]
```

### 4.2 æ•°æ®ç®¡é“æµ‹è¯•

```python
class DataPipelineTester:
    def __init__(self, conn):
        self.conn = conn
        self.quality_checker = DataQualityChecker(conn)

    def test_pipeline(self, pipeline_id: str,
                     test_config: Dict) -> Dict:
        """æµ‹è¯•æ•°æ®ç®¡é“"""
        results = {
            'unit_tests': [],
            'integration_tests': [],
            'quality_checks': []
        }

        # å•å…ƒæµ‹è¯•
        for test in test_config.get('unit_tests', []):
            result = self.run_unit_test(pipeline_id, test)
            results['unit_tests'].append(result)

        # é›†æˆæµ‹è¯•
        for test in test_config.get('integration_tests', []):
            result = self.run_integration_test(pipeline_id, test)
            results['integration_tests'].append(result)

        # è´¨é‡æ£€æŸ¥
        tables = test_config.get('tables', [])
        for table in tables:
            checks = self.quality_checker.run_quality_checks(pipeline_id, table)
            results['quality_checks'].extend(checks.values())

        # è®¡ç®—æ€»ä½“é€šè¿‡ç‡
        all_tests = (results['unit_tests'] +
                    results['integration_tests'] +
                    results['quality_checks'])
        passed = sum(1 for t in all_tests if t.get('passed', False))
        total = len(all_tests)

        results['summary'] = {
            'total': total,
            'passed': passed,
            'failed': total - passed,
            'pass_rate': passed / total if total > 0 else 0
        }

        return results

    def run_unit_test(self, pipeline_id: str, test: Dict) -> Dict:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        # ç®€åŒ–å®ç°
        test_name = test.get('name', 'unknown')
        test_query = test.get('query', '')

        try:
            cursor = self.conn.cursor()
            cursor.execute(test_query)
            result = cursor.fetchone()

            expected = test.get('expected')
            passed = result == expected if expected else True

            return {
                'name': test_name,
                'passed': passed,
                'result': result
            }
        except Exception as e:
            return {
                'name': test_name,
                'passed': False,
                'error': str(e)
            }

    def run_integration_test(self, pipeline_id: str, test: Dict) -> Dict:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        # ç®€åŒ–å®ç°
        return {
            'name': test.get('name', 'unknown'),
            'passed': True
        }
```

### 4.3 æ•°æ®ç®¡é“éƒ¨ç½²

```python
class DataPipelineDeployer:
    def __init__(self, conn):
        self.conn = conn
        self.pipeline = DataOpsPipeline(conn)
        self.tester = DataPipelineTester(conn)

    def deploy_pipeline(self, pipeline_config: Dict) -> Dict:
        """éƒ¨ç½²æ•°æ®ç®¡é“"""
        pipeline_name = pipeline_config['name']
        version = pipeline_config.get('version', '1.0.0')

        # 1. åˆ›å»ºç®¡é“
        pipeline_id = self.pipeline.create_pipeline(pipeline_name, version)

        # 2. è¿è¡Œæµ‹è¯•
        test_results = self.tester.test_pipeline(pipeline_id, pipeline_config.get('tests', {}))

        # 3. æ£€æŸ¥æµ‹è¯•ç»“æœ
        if test_results['summary']['pass_rate'] < 0.95:
            self.pipeline.update_pipeline_status(pipeline_id, 'failed')
            return {
                'success': False,
                'pipeline_id': pipeline_id,
                'reason': 'æµ‹è¯•æœªé€šè¿‡',
                'test_results': test_results
            }

        # 4. æ‰§è¡Œéƒ¨ç½²
        try:
            self.execute_pipeline(pipeline_config)
            self.pipeline.update_pipeline_status(pipeline_id, 'deployed')

            return {
                'success': True,
                'pipeline_id': pipeline_id,
                'test_results': test_results
            }
        except Exception as e:
            self.pipeline.update_pipeline_status(pipeline_id, 'failed')
            return {
                'success': False,
                'pipeline_id': pipeline_id,
                'error': str(e)
            }

    def execute_pipeline(self, pipeline_config: Dict):
        """æ‰§è¡Œæ•°æ®ç®¡é“"""
        # ç®€åŒ–å®ç°
        steps = pipeline_config.get('steps', [])
        for step in steps:
            step_type = step.get('type')
            if step_type == 'sql':
                cursor = self.conn.cursor()
                cursor.execute(step['query'])
                self.conn.commit()
            elif step_type == 'python':
                # æ‰§è¡ŒPythonè„šæœ¬
                exec(step['code'])
```

---

## 5. å®ç°ç»†èŠ‚

### 5.1 Pythonå®ç°

```python
class DataOpsSystem:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.pipeline = DataOpsPipeline(db_path)
        self.quality_checker = DataQualityChecker(self.conn)
        self.lineage_tracker = DataLineageTracker(self.conn)
        self.tester = DataPipelineTester(self.conn)
        self.deployer = DataPipelineDeployer(self.conn)

    def run_dataops_pipeline(self, pipeline_config: Dict) -> Dict:
        """è¿è¡ŒDataOpsç®¡é“"""
        # 1. åˆ›å»ºç®¡é“
        pipeline_id = self.pipeline.create_pipeline(
            pipeline_config['name'],
            pipeline_config.get('version', '1.0.0')
        )

        # 2. è¿è¡Œæµ‹è¯•
        test_results = self.tester.test_pipeline(
            pipeline_id,
            pipeline_config.get('tests', {})
        )

        # 3. è´¨é‡æ£€æŸ¥
        tables = pipeline_config.get('tables', [])
        quality_results = {}
        for table in tables:
            checks = self.quality_checker.run_quality_checks(pipeline_id, table)
            quality_results[table] = checks

        # 4. éƒ¨ç½²å†³ç­–
        if test_results['summary']['pass_rate'] >= 0.95:
            deployment_result = self.deployer.deploy_pipeline(pipeline_config)
        else:
            deployment_result = {
                'success': False,
                'reason': 'æµ‹è¯•æœªé€šè¿‡'
            }

        return {
            'pipeline_id': pipeline_id,
            'test_results': test_results,
            'quality_results': quality_results,
            'deployment_result': deployment_result
        }
```

### 5.2 é›†æˆæ–¹æ¡ˆ

```yaml
# .github/workflows/dataops-pipeline.yml
name: DataOps Pipeline

on:
  push:
    branches: [ main ]
    paths:
      - 'data/pipelines/**'

jobs:
  dataops:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run data quality checks
        run: |
          python scripts/data_quality_checker.py

      - name: Run data pipeline tests
        run: |
          python scripts/pipeline_tester.py

      - name: Deploy pipeline
        if: success()
        run: |
          python scripts/pipeline_deployer.py
```

---

## 6. æœ€ä½³å®è·µ

### 6.1 DataOpsæœ€ä½³å®è·µ

1. **æ•°æ®è´¨é‡ä¼˜å…ˆ**
   - å®šä¹‰è´¨é‡æ ‡å‡†
   - è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥
   - æŒç»­ç›‘æ§

2. **ç‰ˆæœ¬æ§åˆ¶**
   - æ•°æ®å’Œä»£ç ç‰ˆæœ¬åŒ–
   - å˜æ›´è¿½è¸ª
   - å›æ»šæœºåˆ¶

3. **è‡ªåŠ¨åŒ–**
   - è‡ªåŠ¨åŒ–æµ‹è¯•
   - è‡ªåŠ¨åŒ–éƒ¨ç½²
   - è‡ªåŠ¨åŒ–ç›‘æ§

### 6.2 æ•°æ®æ²»ç†æœ€ä½³å®è·µ

1. **æ•°æ®åˆ†ç±»**
   - æ˜ç¡®åˆ†ç±»æ ‡å‡†
   - å®šæœŸå®¡æŸ¥
   - è®¿é—®æ§åˆ¶

2. **è¡€ç¼˜è¿½è¸ª**
   - è®°å½•æ‰€æœ‰è½¬æ¢
   - å¯è§†åŒ–è¡€ç¼˜
   - å½±å“åˆ†æ

---

## 7. ğŸ”— ç›¸å…³èµ„æº

### å†…éƒ¨èµ„æº

- [å¼€å‘å·¥ä½œæµä¸CI/CDé›†æˆ](./08.17-SQLiteå¼€å‘å·¥ä½œæµä¸CI-CDé›†æˆ.md) - CI/CDåŸºç¡€
- [æ™ºèƒ½åŒ–CI/CDå®è·µ](./08.18-æ™ºèƒ½åŒ–CI-CDå®è·µæŒ‡å—.md) - æ™ºèƒ½åŒ–å®è·µ
- [ç”Ÿäº§ç¯å¢ƒç›‘æ§](./08.13-SQLiteç”Ÿäº§ç¯å¢ƒç›‘æ§ä¸è¯Šæ–­.md) - ç›‘æ§å®è·µ

### å¤–éƒ¨èµ„æº

- [DataOpsæ–¹æ³•è®º](https://www.dataopsmanifesto.org/) - DataOpså®£è¨€
- [Great Expectations](https://greatexpectations.io/) - æ•°æ®è´¨é‡å·¥å…·

---

**æœ€åæ›´æ–°**: 2025-12-05
**ç»´æŠ¤è€…**: Data-Science Team
