# SQL数据迁移完整方案

> **创建日期**: 2025-12-04
> **场景**: 跨数据库迁移、版本升级、数据同步
> **难度**: ⭐⭐⭐⭐⭐

---

## 迁移策略总览

```text
数据迁移四大策略
══════════════════════════════════════════════════════════════════════════════

┌──────────────────┬──────────┬──────────┬──────────┬──────────┐
│ 策略             │ 停机时间 │ 数据一致 │ 复杂度   │ 适用场景 │
├──────────────────┼──────────┼──────────┼──────────┼──────────┤
│ 1. 停机迁移      │ 长       │ 强       │ 低       │ 小数据库 │
│ 2. 双写迁移      │ 短       │ 最终一致 │ 中       │ 中等规模 │
│ 3. 逻辑复制      │ 无       │ 实时     │ 高       │ 大规模   │
│ 4. 物理复制      │ 极短     │ 强       │ 中       │ 同构DB   │
└──────────────────┴──────────┴──────────┴──────────┴──────────┘
```

---

## 策略1：停机迁移（Stop-The-World）

### 流程图

```text
停机迁移流程
═══════════════════════════════════════════════════════════════

1. 准备阶段
   ├─ 备份源数据库
   ├─ 创建目标数据库结构
   └─ 验证迁移脚本

2. 迁移阶段（应用停机）
   ├─ 停止应用服务
   ├─ 最后一次备份
   ├─ 执行数据导出
   ├─ 数据转换（如需要）
   ├─ 导入目标数据库
   └─ 验证数据完整性

3. 切换阶段
   ├─ 更新应用配置（新数据库）
   ├─ 启动应用服务
   └─ 监控运行状态

4. 回滚计划
   └─ 如果失败 → 恢复备份 → 重新上线
```

### 实现（PostgreSQL → MySQL）

```python
import psycopg2
import pymysql
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StopTheWorldMigration:
    def __init__(self, pg_config, mysql_config):
        self.pg_conn = psycopg2.connect(**pg_config)
        self.mysql_conn = pymysql.connect(**mysql_config)
        self.batch_size = 10000

    def migrate(self):
        """执行完整迁移"""
        try:
            logger.info("1. 备份源数据库...")
            self.backup_source()

            logger.info("2. 创建目标表结构...")
            self.create_target_schema()

            logger.info("3. 迁移数据...")
            tables = ['users', 'orders', 'order_items', 'products']
            for table in tables:
                self.migrate_table(table)

            logger.info("4. 验证数据一致性...")
            self.verify_consistency(tables)

            logger.info("✅ 迁移完成！")
        except Exception as e:
            logger.error(f"❌ 迁移失败: {e}")
            raise

    def backup_source(self):
        """备份源数据库"""
        import subprocess
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"backup_{timestamp}.sql"

        subprocess.run([
            'pg_dump',
            '-h', 'localhost',
            '-U', 'postgres',
            '-d', 'source_db',
            '-f', backup_file
        ], check=True)

        logger.info(f"备份文件: {backup_file}")

    def create_target_schema(self):
        """创建目标表结构"""
        with self.mysql_conn.cursor() as cursor:
            # 用户表
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS users (
                    id BIGINT PRIMARY KEY,
                    username VARCHAR(50) NOT NULL UNIQUE,
                    email VARCHAR(100) NOT NULL UNIQUE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # 订单表
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS orders (
                    id BIGINT PRIMARY KEY,
                    user_id BIGINT NOT NULL,
                    order_no VARCHAR(50) NOT NULL UNIQUE,
                    total_amount DECIMAL(10, 2) NOT NULL,
                    status VARCHAR(20) NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (user_id) REFERENCES users(id)
                )
            ''')

            # 订单项表
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS order_items (
                    id BIGINT PRIMARY KEY,
                    order_id BIGINT NOT NULL,
                    product_id BIGINT NOT NULL,
                    quantity INT NOT NULL,
                    price DECIMAL(10, 2) NOT NULL,
                    FOREIGN KEY (order_id) REFERENCES orders(id)
                )
            ''')

            self.mysql_conn.commit()

    def migrate_table(self, table_name):
        """迁移单个表"""
        logger.info(f"  迁移表: {table_name}")

        # 获取总行数
        with self.pg_conn.cursor() as pg_cursor:
            pg_cursor.execute(f'SELECT COUNT(*) FROM {table_name}')
            total_rows = pg_cursor.fetchone()[0]
            logger.info(f"    总行数: {total_rows}")

        # 分批迁移
        offset = 0
        migrated = 0

        while offset < total_rows:
            with self.pg_conn.cursor() as pg_cursor:
                # 读取源数据
                pg_cursor.execute(
                    f'SELECT * FROM {table_name} ORDER BY id LIMIT %s OFFSET %s',
                    (self.batch_size, offset)
                )
                rows = pg_cursor.fetchall()
                columns = [desc[0] for desc in pg_cursor.description]

            if not rows:
                break

            # 写入目标数据库
            with self.mysql_conn.cursor() as mysql_cursor:
                placeholders = ', '.join(['%s'] * len(columns))
                insert_sql = f'''
                    INSERT IGNORE INTO {table_name} ({', '.join(columns)})
                    VALUES ({placeholders})
                '''
                mysql_cursor.executemany(insert_sql, rows)
                self.mysql_conn.commit()

            migrated += len(rows)
            offset += self.batch_size
            logger.info(f"    进度: {migrated}/{total_rows} ({migrated*100//total_rows}%)")

    def verify_consistency(self, tables):
        """验证数据一致性"""
        for table in tables:
            with self.pg_conn.cursor() as pg_cursor:
                pg_cursor.execute(f'SELECT COUNT(*) FROM {table}')
                pg_count = pg_cursor.fetchone()[0]

            with self.mysql_conn.cursor() as mysql_cursor:
                mysql_cursor.execute(f'SELECT COUNT(*) FROM {table}')
                mysql_count = mysql_cursor.fetchone()[0]

            if pg_count != mysql_count:
                raise ValueError(f"表{table}行数不一致: PG={pg_count}, MySQL={mysql_count}")

            logger.info(f"  ✓ {table}: {pg_count} 行")

# 使用
migration = StopTheWorldMigration(
    pg_config={
        'host': 'localhost',
        'database': 'source_db',
        'user': 'postgres',
        'password': 'password'
    },
    mysql_config={
        'host': 'localhost',
        'database': 'target_db',
        'user': 'root',
        'password': 'password'
    }
)

migration.migrate()
```

---

## 策略2：双写迁移（Dual-Write）

### 流程图

```text
双写迁移流程
═══════════════════════════════════════════════════════════════

阶段1: 历史数据迁移
   └─ 批量复制旧数据到新DB（应用继续运行）

阶段2: 双写（应用升级）
   ├─ 写操作同时写旧DB和新DB
   ├─ 读操作从旧DB读取
   └─ 持续一段时间（确保数据同步）

阶段3: 读写切换
   ├─ 写操作仍然双写
   ├─ 读操作切换到新DB
   └─ 监控新DB性能

阶段4: 停止双写
   ├─ 只写新DB
   └─ 旧DB标记为只读（保留一段时间）

阶段5: 下线旧DB
   └─ 验证无问题后，下线旧DB
```

### 实现（应用层双写）

```python
class DualWriteService:
    """双写服务"""

    def __init__(self, old_db, new_db, mode='dual_write'):
        self.old_db = old_db  # 旧数据库连接
        self.new_db = new_db  # 新数据库连接
        self.mode = mode  # dual_write, read_new, write_new

    def create_user(self, username, email):
        """创建用户"""
        if self.mode in ['dual_write', 'write_new']:
            # 写新DB
            try:
                new_id = self._write_new_db(username, email)
            except Exception as e:
                logging.error(f"写新DB失败: {e}")
                # 继续写旧DB

        if self.mode == 'dual_write':
            # 写旧DB
            old_id = self._write_old_db(username, email)
            return old_id
        else:
            return new_id

    def get_user(self, user_id):
        """获取用户"""
        if self.mode in ['dual_write']:
            # 从旧DB读取
            return self._read_old_db(user_id)
        elif self.mode in ['read_new', 'write_new']:
            # 从新DB读取
            try:
                return self._read_new_db(user_id)
            except Exception as e:
                logging.error(f"读新DB失败: {e}")
                # 降级到旧DB
                return self._read_old_db(user_id)

    def _write_old_db(self, username, email):
        """写旧数据库"""
        with self.old_db.cursor() as cursor:
            cursor.execute(
                'INSERT INTO users (username, email) VALUES (?, ?)',
                (username, email)
            )
            self.old_db.commit()
            return cursor.lastrowid

    def _write_new_db(self, username, email):
        """写新数据库"""
        with self.new_db.cursor() as cursor:
            cursor.execute(
                'INSERT INTO users (username, email) VALUES (%s, %s)',
                (username, email)
            )
            self.new_db.commit()
            return cursor.lastrowid

    def _read_old_db(self, user_id):
        """从旧数据库读取"""
        with self.old_db.cursor() as cursor:
            cursor.execute('SELECT * FROM users WHERE id = ?', (user_id,))
            return cursor.fetchone()

    def _read_new_db(self, user_id):
        """从新数据库读取"""
        with self.new_db.cursor() as cursor:
            cursor.execute('SELECT * FROM users WHERE id = %s', (user_id,))
            return cursor.fetchone()

# 使用（配合配置切换）
import os

mode = os.getenv('DB_MODE', 'dual_write')  # dual_write, read_new, write_new
service = DualWriteService(old_db, new_db, mode=mode)

# 应用代码无需改变
user_id = service.create_user('alice', 'alice@example.com')
user = service.get_user(user_id)
```

---

## 策略3：逻辑复制（Logical Replication）

### 架构图

```text
逻辑复制架构
═══════════════════════════════════════════════════════════════

┌─────────────┐                ┌─────────────┐
│  Source DB  │                │  Target DB  │
│ (PostgreSQL)│                │  (MySQL)    │
└──────┬──────┘                └──────▲──────┘
       │                              │
       │ WAL日志                      │
       ▼                              │
┌────────────────┐                    │
│ Logical Decode │                    │
│  (pgoutput)    │                    │
└──────┬─────────┘                    │
       │                              │
       │ 变更事件                     │
       ▼                              │
┌────────────────┐                    │
│ Replication    │                    │
│ Worker         │────变更SQL─────────┘
└────────────────┘

特点:
• 实时同步
• 支持异构数据库
• 可自定义转换逻辑
```

### 实现（基于Debezium）

```yaml
# docker-compose.yml
version: '3'
services:
  # PostgreSQL源数据库
  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: source_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"  # 启用逻辑复制

  # Kafka（消息队列）
  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

  # Debezium Connector（捕获变更）
  debezium:
    image: debezium/connect:latest
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      CONFIG_STORAGE_TOPIC: debezium_configs
      OFFSET_STORAGE_TOPIC: debezium_offsets

  # MySQL目标数据库
  mysql:
    image: mysql:8.4
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: target_db

  # 自定义Worker（消费Kafka，写MySQL）
  worker:
    build: ./worker
    environment:
      KAFKA_BROKERS: kafka:9092
      MYSQL_HOST: mysql
```

```python
# worker/main.py
from kafka import KafkaConsumer
import pymysql
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReplicationWorker:
    def __init__(self, kafka_brokers, mysql_config):
        self.consumer = KafkaConsumer(
            'postgres.public.users',  # Topic: {db}.{schema}.{table}
            bootstrap_servers=kafka_brokers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        self.mysql_conn = pymysql.connect(**mysql_config)

    def run(self):
        """消费变更事件"""
        for message in self.consumer:
            event = message.value
            self.process_event(event)

    def process_event(self, event):
        """处理变更事件"""
        op = event['payload']['op']  # c=INSERT, u=UPDATE, d=DELETE

        if op == 'c':  # INSERT
            self.handle_insert(event)
        elif op == 'u':  # UPDATE
            self.handle_update(event)
        elif op == 'd':  # DELETE
            self.handle_delete(event)

    def handle_insert(self, event):
        """处理INSERT事件"""
        after = event['payload']['after']
        table = event['payload']['source']['table']

        columns = ', '.join(after.keys())
        placeholders = ', '.join(['%s'] * len(after))
        values = list(after.values())

        sql = f'INSERT IGNORE INTO {table} ({columns}) VALUES ({placeholders})'

        with self.mysql_conn.cursor() as cursor:
            cursor.execute(sql, values)
            self.mysql_conn.commit()

        logger.info(f"INSERT {table}: {after['id']}")

    def handle_update(self, event):
        """处理UPDATE事件"""
        after = event['payload']['after']
        before = event['payload']['before']
        table = event['payload']['source']['table']

        # 构造SET子句
        set_clause = ', '.join([f"{k} = %s" for k in after.keys() if k != 'id'])
        values = [v for k, v in after.items() if k != 'id']
        values.append(after['id'])

        sql = f'UPDATE {table} SET {set_clause} WHERE id = %s'

        with self.mysql_conn.cursor() as cursor:
            cursor.execute(sql, values)
            self.mysql_conn.commit()

        logger.info(f"UPDATE {table}: {after['id']}")

    def handle_delete(self, event):
        """处理DELETE事件"""
        before = event['payload']['before']
        table = event['payload']['source']['table']

        sql = f'DELETE FROM {table} WHERE id = %s'

        with self.mysql_conn.cursor() as cursor:
            cursor.execute(sql, (before['id'],))
            self.mysql_conn.commit()

        logger.info(f"DELETE {table}: {before['id']}")

# 运行
worker = ReplicationWorker(
    kafka_brokers=['kafka:9092'],
    mysql_config={
        'host': 'mysql',
        'database': 'target_db',
        'user': 'root',
        'password': 'password'
    }
)
worker.run()
```

---

## 策略4：物理复制（Physical Replication）

### 流程（PostgreSQL主从复制）

```bash
# 1. 主库配置（postgresql.conf）
wal_level = replica
max_wal_senders = 3
wal_keep_size = 1GB

# 2. 创建复制用户
CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'password';

# 3. 配置pg_hba.conf
host replication replicator 192.168.1.0/24 md5

# 4. 从库：基础备份
pg_basebackup -h master_host -D /var/lib/postgresql/data -U replicator -P

# 5. 从库配置（postgresql.conf）
hot_standby = on

# 6. 从库配置（postgresql.auto.conf）
primary_conninfo = 'host=master_host port=5432 user=replicator password=password'

# 7. 启动从库
pg_ctl start

# 8. 验证复制状态
-- 主库
SELECT * FROM pg_stat_replication;

-- 从库
SELECT pg_is_in_recovery();  -- 应返回t
```

---

**文档版本**: v1.0.0
**最后更新**: 2025-12-04
