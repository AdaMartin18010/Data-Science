# æ•°æ®åº“ç›‘æ§ä¸è¯Šæ–­å®æˆ˜

> **åˆ›å»ºæ—¥æœŸ**: 2025-12-04
> **é€‚ç”¨ç‰ˆæœ¬**: PostgreSQL 18, MySQL 8.4, SQLite 3.47
> **éš¾åº¦**: â­â­â­â­â­

---

## PostgreSQLç›‘æ§ä½“ç³»

### ç³»ç»Ÿè§†å›¾ç›‘æ§

```sql
-- 1. è¿æ¥æ•°ç›‘æ§
SELECT
    datname,
    count(*) as connections,
    count(*) FILTER (WHERE state = 'active') as active,
    count(*) FILTER (WHERE state = 'idle') as idle,
    count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_txn
FROM pg_stat_activity
WHERE pid <> pg_backend_pid()
GROUP BY datname
ORDER BY connections DESC;

-- 2. æ…¢æŸ¥è¯¢å®æ—¶ç›‘æ§
SELECT
    pid,
    usename,
    application_name,
    client_addr,
    state,
    EXTRACT(EPOCH FROM (now() - query_start)) as duration_sec,
    query
FROM pg_stat_activity
WHERE state != 'idle'
    AND query NOT LIKE '%pg_stat_activity%'
    AND (now() - query_start) > interval '5 seconds'
ORDER BY duration_sec DESC;

-- 3. é”ç­‰å¾…ç›‘æ§
SELECT
    blocked_locks.pid AS blocked_pid,
    blocked_activity.usename AS blocked_user,
    blocking_locks.pid AS blocking_pid,
    blocking_activity.usename AS blocking_user,
    blocked_activity.query AS blocked_query,
    blocking_activity.query AS blocking_query,
    blocked_locks.mode AS blocked_mode,
    blocking_locks.mode AS blocking_mode
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks
    ON blocking_locks.locktype = blocked_locks.locktype
    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
    AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted;

-- 4. è¡¨IOç»Ÿè®¡
SELECT
    schemaname,
    relname,
    heap_blks_read,
    heap_blks_hit,
    CASE
        WHEN heap_blks_hit + heap_blks_read = 0 THEN 0
        ELSE ROUND(100.0 * heap_blks_hit / (heap_blks_hit + heap_blks_read), 2)
    END AS cache_hit_ratio,
    idx_blks_read,
    idx_blks_hit
FROM pg_statio_user_tables
WHERE heap_blks_read + heap_blks_hit > 0
ORDER BY heap_blks_read DESC
LIMIT 20;

-- 5. ç´¢å¼•ä½¿ç”¨ç‡
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
    AND pg_relation_size(indexrelid) > 1024 * 1024  -- > 1MB
ORDER BY pg_relation_size(indexrelid) DESC;

-- 6. VACUUMç›‘æ§
SELECT
    schemaname,
    relname,
    n_live_tup,
    n_dead_tup,
    ROUND(100.0 * n_dead_tup / NULLIF(n_live_tup + n_dead_tup, 0), 2) as dead_ratio,
    last_vacuum,
    last_autovacuum,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE n_dead_tup > 1000
ORDER BY dead_ratio DESC;
```

### Pythonç›‘æ§è„šæœ¬

```python
import psycopg2
import time
import json
from datetime import datetime

class PostgreSQLMonitor:
    def __init__(self, dsn):
        self.conn = psycopg2.connect(dsn)
        self.conn.autocommit = True

    def get_current_connections(self):
        """è¿æ¥æ•°ç›‘æ§"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT
                    datname,
                    count(*) as total,
                    count(*) FILTER (WHERE state = 'active') as active,
                    count(*) FILTER (WHERE state = 'idle') as idle
                FROM pg_stat_activity
                WHERE pid <> pg_backend_pid()
                GROUP BY datname
            """)
            return cur.fetchall()

    def get_slow_queries(self, threshold_sec=5):
        """æ…¢æŸ¥è¯¢ç›‘æ§"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT
                    pid,
                    usename,
                    EXTRACT(EPOCH FROM (now() - query_start)) as duration,
                    query
                FROM pg_stat_activity
                WHERE state != 'idle'
                    AND (now() - query_start) > interval '%s seconds'
                ORDER BY duration DESC
            """, (threshold_sec,))
            return cur.fetchall()

    def get_blocking_queries(self):
        """é”ç­‰å¾…ç›‘æ§"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT
                    blocked_locks.pid AS blocked_pid,
                    blocking_locks.pid AS blocking_pid,
                    blocked_activity.query AS blocked_query,
                    blocking_activity.query AS blocking_query
                FROM pg_catalog.pg_locks blocked_locks
                JOIN pg_catalog.pg_stat_activity blocked_activity
                    ON blocked_activity.pid = blocked_locks.pid
                JOIN pg_catalog.pg_locks blocking_locks
                    ON blocking_locks.locktype = blocked_locks.locktype
                    AND blocking_locks.pid != blocked_locks.pid
                JOIN pg_catalog.pg_stat_activity blocking_activity
                    ON blocking_activity.pid = blocking_locks.pid
                WHERE NOT blocked_locks.granted
            """)
            return cur.fetchall()

    def get_cache_hit_ratio(self):
        """ç¼“å­˜å‘½ä¸­ç‡"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT
                    ROUND(100.0 * sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit) + sum(heap_blks_read), 0), 2) as ratio
                FROM pg_statio_user_tables
            """)
            return cur.fetchone()[0]

    def get_table_bloat(self):
        """è¡¨è†¨èƒ€ç›‘æ§"""
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT
                    schemaname,
                    tablename,
                    n_dead_tup,
                    ROUND(100.0 * n_dead_tup / NULLIF(n_live_tup, 0), 2) as bloat_pct
                FROM pg_stat_user_tables
                WHERE n_dead_tup > 1000
                ORDER BY bloat_pct DESC
                LIMIT 10
            """)
            return cur.fetchall()

    def monitor_loop(self, interval=60):
        """ç›‘æ§å¾ªç¯"""
        while True:
            report = {
                'timestamp': datetime.now().isoformat(),
                'connections': self.get_current_connections(),
                'slow_queries': self.get_slow_queries(),
                'blocking_queries': self.get_blocking_queries(),
                'cache_hit_ratio': self.get_cache_hit_ratio(),
                'table_bloat': self.get_table_bloat()
            }

            # è¾“å‡ºæŠ¥å‘Š
            print(json.dumps(report, indent=2, default=str))

            # å‘Šè­¦æ£€æµ‹
            if report['cache_hit_ratio'] < 95:
                print(f"âš ï¸  LOW CACHE HIT RATIO: {report['cache_hit_ratio']}%")

            if len(report['slow_queries']) > 0:
                print(f"âš ï¸  {len(report['slow_queries'])} SLOW QUERIES DETECTED")

            if len(report['blocking_queries']) > 0:
                print(f"ğŸ”´ {len(report['blocking_queries'])} BLOCKING QUERIES DETECTED")

            time.sleep(interval)

# ä½¿ç”¨
monitor = PostgreSQLMonitor('postgresql://user:pass@localhost/mydb')
monitor.monitor_loop(interval=60)
```

---

## MySQLç›‘æ§ä½“ç³»

### Performance Schemaç›‘æ§

```sql
-- 1. æœ€æ…¢è¯­å¥TOP 10
SELECT
    DIGEST_TEXT as query,
    COUNT_STAR as exec_count,
    ROUND(AVG_TIMER_WAIT / 1000000000, 2) as avg_sec,
    ROUND(SUM_TIMER_WAIT / 1000000000, 2) as total_sec,
    ROUND(MAX_TIMER_WAIT / 1000000000, 2) as max_sec
FROM performance_schema.events_statements_summary_by_digest
ORDER BY avg_sec DESC
LIMIT 10;

-- 2. è¡¨IOç»Ÿè®¡
SELECT
    OBJECT_SCHEMA,
    OBJECT_NAME,
    COUNT_READ,
    COUNT_WRITE,
    ROUND(SUM_TIMER_WAIT / 1000000000, 2) as total_sec
FROM performance_schema.table_io_waits_summary_by_table
WHERE OBJECT_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema')
ORDER BY total_sec DESC
LIMIT 20;

-- 3. æ–‡ä»¶IOç»Ÿè®¡
SELECT
    EVENT_NAME,
    COUNT_READ,
    COUNT_WRITE,
    ROUND(SUM_TIMER_READ / 1000000000, 2) as read_sec,
    ROUND(SUM_TIMER_WRITE / 1000000000, 2) as write_sec
FROM performance_schema.file_summary_by_event_name
WHERE EVENT_NAME LIKE 'wait/io/file/%'
ORDER BY (SUM_TIMER_READ + SUM_TIMER_WRITE) DESC
LIMIT 10;

-- 4. InnoDBçŠ¶æ€
SHOW ENGINE INNODB STATUS\G

-- 5. æœªä½¿ç”¨çš„ç´¢å¼•
SELECT
    t.TABLE_SCHEMA,
    t.TABLE_NAME,
    s.INDEX_NAME,
    ROUND(s.index_length / 1024 / 1024, 2) as size_mb
FROM information_schema.STATISTICS s
JOIN information_schema.TABLES t
    ON s.TABLE_SCHEMA = t.TABLE_SCHEMA
    AND s.TABLE_NAME = t.TABLE_NAME
LEFT JOIN performance_schema.table_io_waits_summary_by_index_usage u
    ON s.TABLE_SCHEMA = u.OBJECT_SCHEMA
    AND s.TABLE_NAME = u.OBJECT_NAME
    AND s.INDEX_NAME = u.INDEX_NAME
WHERE t.TABLE_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema')
    AND s.INDEX_NAME != 'PRIMARY'
    AND (u.COUNT_STAR IS NULL OR u.COUNT_STAR = 0)
GROUP BY t.TABLE_SCHEMA, t.TABLE_NAME, s.INDEX_NAME;
```

---

## SQLiteè¯Šæ–­å·¥å…·

```python
import sqlite3
import time
from collections import defaultdict

class SQLiteProfiler:
    def __init__(self, db_path):
        self.db_path = db_path
        self.queries = defaultdict(list)

    def profile(self):
        """åˆ†æSQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)

        # 1. æ•°æ®åº“åŸºæœ¬ä¿¡æ¯
        print("=== Database Info ===")
        page_count = conn.execute("PRAGMA page_count").fetchone()[0]
        page_size = conn.execute("PRAGMA page_size").fetchone()[0]
        db_size_mb = (page_count * page_size) / 1024 / 1024
        print(f"Database size: {db_size_mb:.2f} MB")
        print(f"Page size: {page_size} bytes")
        print(f"Page count: {page_count}")

        # 2. è¡¨å¤§å°ç»Ÿè®¡
        print("\n=== Table Sizes ===")
        tables = conn.execute("""
            SELECT name FROM sqlite_master
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
        """).fetchall()

        for (table,) in tables:
            size_query = f"SELECT SUM(pgsize) FROM dbstat WHERE name='{table}'"
            size = conn.execute(size_query).fetchone()[0] or 0
            row_count = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
            print(f"{table}: {size / 1024:.2f} KB, {row_count} rows")

        # 3. ç´¢å¼•åˆ†æ
        print("\n=== Indexes ===")
        indexes = conn.execute("""
            SELECT name, tbl_name FROM sqlite_master
            WHERE type='index' AND name NOT LIKE 'sqlite_%'
        """).fetchall()

        for idx_name, tbl_name in indexes:
            size_query = f"SELECT SUM(pgsize) FROM dbstat WHERE name='{idx_name}'"
            size = conn.execute(size_query).fetchone()[0] or 0
            print(f"{idx_name} on {tbl_name}: {size / 1024:.2f} KB")

        # 4. ç¢ç‰‡åˆ†æ
        print("\n=== Fragmentation ===")
        freelist_count = conn.execute("PRAGMA freelist_count").fetchone()[0]
        fragmentation = (freelist_count / page_count * 100) if page_count > 0 else 0
        print(f"Free pages: {freelist_count} ({fragmentation:.2f}%)")
        if fragmentation > 5:
            print("âš ï¸  Consider running VACUUM")

        # 5. é…ç½®æ£€æŸ¥
        print("\n=== Configuration ===")
        journal_mode = conn.execute("PRAGMA journal_mode").fetchone()[0]
        synchronous = conn.execute("PRAGMA synchronous").fetchone()[0]
        cache_size = conn.execute("PRAGMA cache_size").fetchone()[0]
        print(f"Journal mode: {journal_mode}")
        print(f"Synchronous: {synchronous}")
        print(f"Cache size: {abs(cache_size)} pages")

        if journal_mode != 'wal':
            print("ğŸ’¡ Recommend: PRAGMA journal_mode=WAL")

        conn.close()

    def trace_queries(self, duration_sec=60):
        """è·Ÿè¸ªæŸ¥è¯¢ï¼ˆéœ€è¦å¼€å¯traceï¼‰"""
        conn = sqlite3.connect(self.db_path)

        def trace_callback(statement):
            start = time.time()
            def trace_commit(statement):
                elapsed = (time.time() - start) * 1000
                self.queries[statement].append(elapsed)
                print(f"[{elapsed:.2f}ms] {statement[:80]}")
            return trace_commit

        conn.set_trace_callback(trace_callback)

        # è¿è¡Œä¸€æ®µæ—¶é—´
        print(f"Tracing queries for {duration_sec} seconds...")
        time.sleep(duration_sec)

        conn.close()

        # è¾“å‡ºç»Ÿè®¡
        print("\n=== Query Statistics ===")
        for query, times in sorted(self.queries.items(), key=lambda x: sum(x[1]), reverse=True)[:10]:
            print(f"Total: {sum(times):.2f}ms, Avg: {sum(times)/len(times):.2f}ms, Count: {len(times)}")
            print(f"  {query[:80]}\n")

# ä½¿ç”¨
profiler = SQLiteProfiler('app.db')
profiler.profile()
```
