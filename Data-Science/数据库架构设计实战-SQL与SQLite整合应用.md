# æ•°æ®åº“æ¶æ„è®¾è®¡å®æˆ˜ï¼šSQLä¸SQLiteæ•´åˆåº”ç”¨

> **åˆ›å»ºæ—¥æœŸ**ï¼š2025-12-04
> **éš¾åº¦**ï¼šâ­â­â­â­â­
> **å‰ç½®çŸ¥è¯†**ï¼šSQLæ ‡å‡†ã€SQLiteæ ¸å¿ƒæœºåˆ¶ã€ç³»ç»Ÿæ¶æ„è®¾è®¡
> **é€‚ç”¨å¯¹è±¡**ï¼šæ¶æ„å¸ˆã€é«˜çº§å¼€å‘è€…

---

## ğŸ“‹ æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£å±•ç¤ºå¦‚ä½•åœ¨å®é™…ç³»ç»Ÿæ¶æ„ä¸­æ•´åˆä½¿ç”¨PostgreSQLå’ŒSQLiteï¼Œå……åˆ†å‘æŒ¥å„è‡ªä¼˜åŠ¿ã€‚

---

## ğŸ“‘ ç›®å½•

- [æ•°æ®åº“æ¶æ„è®¾è®¡å®æˆ˜ï¼šSQLä¸SQLiteæ•´åˆåº”ç”¨](#æ•°æ®åº“æ¶æ„è®¾è®¡å®æˆ˜sqlä¸sqliteæ•´åˆåº”ç”¨)
  - [ğŸ“‹ æ–‡æ¡£è¯´æ˜](#-æ–‡æ¡£è¯´æ˜)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [ä¸€ã€æ··åˆæ¶æ„è®¾è®¡æ¨¡å¼](#ä¸€æ··åˆæ¶æ„è®¾è®¡æ¨¡å¼)
    - [1.1 ä¸­å¿ƒåŒ–+è¾¹ç¼˜åŒ–æ¶æ„](#11-ä¸­å¿ƒåŒ–è¾¹ç¼˜åŒ–æ¶æ„)
    - [1.2 è¯»å†™åˆ†ç¦»æ¶æ„](#12-è¯»å†™åˆ†ç¦»æ¶æ„)
    - [1.3 ç¼“å­˜å±‚æ¶æ„](#13-ç¼“å­˜å±‚æ¶æ„)
  - [äºŒã€å…¸å‹åœºæ™¯æ¶æ„æ–¹æ¡ˆ](#äºŒå…¸å‹åœºæ™¯æ¶æ„æ–¹æ¡ˆ)
    - [2.1 ç§»åŠ¨åº”ç”¨æ¶æ„ï¼ˆåœ¨çº¿+ç¦»çº¿ï¼‰](#21-ç§»åŠ¨åº”ç”¨æ¶æ„åœ¨çº¿ç¦»çº¿)
    - [2.2 IoTè¾¹ç¼˜è®¡ç®—æ¶æ„](#22-iotè¾¹ç¼˜è®¡ç®—æ¶æ„)
    - [2.3 åˆ†å¸ƒå¼ç³»ç»Ÿæœ¬åœ°ç¼“å­˜](#23-åˆ†å¸ƒå¼ç³»ç»Ÿæœ¬åœ°ç¼“å­˜)
  - [ä¸‰ã€æ•°æ®åŒæ­¥ç­–ç•¥](#ä¸‰æ•°æ®åŒæ­¥ç­–ç•¥)
    - [3.1 å…¨é‡åŒæ­¥](#31-å…¨é‡åŒæ­¥)
    - [3.2 å¢é‡åŒæ­¥](#32-å¢é‡åŒæ­¥)
    - [3.3 å†²çªè§£å†³](#33-å†²çªè§£å†³)
  - [å››ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#å››æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
    - [4.1 æŸ¥è¯¢è·¯ç”±](#41-æŸ¥è¯¢è·¯ç”±)
    - [4.2 ç¼“å­˜ç­–ç•¥](#42-ç¼“å­˜ç­–ç•¥)
  - [äº”ã€å®Œæ•´å®ç°æ¡ˆä¾‹](#äº”å®Œæ•´å®ç°æ¡ˆä¾‹)
    - [5.1 æ–°é—»Appæ¶æ„](#51-æ–°é—»appæ¶æ„)
    - [5.2 ååŒåŠå…¬ç³»ç»Ÿ](#52-ååŒåŠå…¬ç³»ç»Ÿ)

---

## ä¸€ã€æ··åˆæ¶æ„è®¾è®¡æ¨¡å¼

### 1.1 ä¸­å¿ƒåŒ–+è¾¹ç¼˜åŒ–æ¶æ„

```text
ä¸­å¿ƒåŒ–+è¾¹ç¼˜åŒ–æ··åˆæ¶æ„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    ä¸­å¿ƒæ•°æ®åº“ï¼ˆPostgreSQL 18ï¼‰
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  â€¢ ä¸»æ•°æ®å­˜å‚¨           â”‚
                    â”‚  â€¢ MVCCå¹¶å‘æ§åˆ¶         â”‚
                    â”‚  â€¢ äº‹åŠ¡ä¸€è‡´æ€§ä¿è¯       â”‚
                    â”‚  â€¢ å¤æ‚æŸ¥è¯¢åˆ†æ         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           â”‚           â”‚
                    â–¼           â–¼           â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ ç§»åŠ¨ç«¯1  â”‚  â”‚ ç§»åŠ¨ç«¯2  â”‚  â”‚ ç§»åŠ¨ç«¯N  â”‚
            â”‚ SQLite   â”‚  â”‚ SQLite   â”‚  â”‚ SQLite   â”‚
            â”‚          â”‚  â”‚          â”‚  â”‚          â”‚
            â”‚ â€¢ æœ¬åœ°æ•°æ®â”‚  â”‚ â€¢ æœ¬åœ°æ•°æ®â”‚  â”‚ â€¢ æœ¬åœ°æ•°æ®â”‚
            â”‚ â€¢ ç¦»çº¿æ”¯æŒâ”‚  â”‚ â€¢ ç¦»çº¿æ”¯æŒâ”‚  â”‚ â€¢ ç¦»çº¿æ”¯æŒâ”‚
            â”‚ â€¢ å¿«é€Ÿå“åº”â”‚  â”‚ â€¢ å¿«é€Ÿå“åº”â”‚  â”‚ â€¢ å¿«é€Ÿå“åº”â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ•°æ®æµï¼š
â€¢ ä¸‹è¡Œï¼ˆServer â†’ Clientï¼‰ï¼šPostgreSQLå¯¼å‡º â†’ SQLiteå¯¼å…¥
â€¢ ä¸Šè¡Œï¼ˆClient â†’ Serverï¼‰ï¼šSQLiteæ”¶é›† â†’ PostgreSQLåˆå¹¶
â€¢ å†²çªè§£å†³ï¼šæ—¶é—´æˆ³/ç‰ˆæœ¬å·/Last-Write-Wins

ä¼˜åŠ¿ï¼š
âœ… ä¸­å¿ƒç«¯å¼ºä¸€è‡´æ€§ï¼ˆPostgreSQL ACID + MVCCï¼‰
âœ… è¾¹ç¼˜ç«¯é«˜å¯ç”¨æ€§ï¼ˆSQLiteç¦»çº¿å·¥ä½œï¼‰
âœ… ç½‘ç»œä¸­æ–­å®¹å¿
âœ… é™ä½ä¸­å¿ƒæ•°æ®åº“å‹åŠ›
```

### 1.2 è¯»å†™åˆ†ç¦»æ¶æ„

```text
è¯»å†™åˆ†ç¦»æ¶æ„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                        åº”ç”¨æœåŠ¡å™¨
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                         â”‚
                â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PostgreSQL   â”‚          â”‚ SQLite       â”‚
        â”‚ (Master)     â”‚â”€â”€â”€â”€å¤åˆ¶â”€>â”‚ (Read Replica)â”‚
        â”‚              â”‚          â”‚              â”‚
        â”‚ å†™æ“ä½œ       â”‚          â”‚ è¯»æ“ä½œ       â”‚
        â”‚ â€¢ INSERT     â”‚          â”‚ â€¢ SELECT     â”‚
        â”‚ â€¢ UPDATE     â”‚          â”‚ â€¢ æŠ¥è¡¨æŸ¥è¯¢   â”‚
        â”‚ â€¢ DELETE     â”‚          â”‚ â€¢ æ•°æ®å¯¼å‡º   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â–²
              â”‚ å®šæœŸå¯¼å‡º                 â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®ç°æ–¹æ¡ˆï¼š

-- PostgreSQLå†™å…¥
BEGIN;
INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com');
COMMIT;

-- å®šæœŸå¯¼å‡ºåˆ°SQLiteï¼ˆå¢é‡ï¼‰
pg_dump --data-only --inserts \
        --table=users \
        --where="updated_at >= '2025-12-04 00:00:00'" \
        | sqlite3 replica.db

-- åº”ç”¨å±‚è¯»å–ï¼ˆä»SQLiteï¼‰
SELECT * FROM users WHERE id = ?;
-- é›¶ç½‘ç»œå»¶è¿Ÿï¼Œæå¿«å“åº”

ä¼˜åŠ¿ï¼š
âœ… å†™æ“ä½œé›†ä¸­åˆ°PostgreSQLï¼ˆMVCCå¹¶å‘ä¼˜åŠ¿ï¼‰
âœ… è¯»æ“ä½œåˆ†æµåˆ°SQLiteï¼ˆå‡è½»ä¸»åº“å‹åŠ›ï¼‰
âœ… æŠ¥è¡¨æŸ¥è¯¢æœ¬åœ°åŒ–ï¼ˆä¸å½±å“ä¸»åº“æ€§èƒ½ï¼‰
âœ… æˆæœ¬ä½ï¼ˆSQLiteæ— éœ€é¢å¤–æœåŠ¡å™¨ï¼‰
```

### 1.3 ç¼“å­˜å±‚æ¶æ„

```text
SQLiteä½œä¸ºç¼“å­˜å±‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                        åº”ç”¨ç¨‹åº
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ç¼“å­˜æŸ¥è¯¢é€»è¾‘   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚            â”‚            â”‚
                â–¼            â–¼            â–¼
            åœ¨å†…å­˜ï¼Ÿ      åœ¨SQLiteï¼Ÿ    åœ¨PostgreSQLï¼Ÿ
                â”‚            â”‚            â”‚
              YES           YES           YES
                â”‚            â”‚            â”‚
                â–¼            â–¼            â–¼
            è¿”å›ç»“æœ    è¿”å›ç»“æœ+æ›´æ–°   æŸ¥è¯¢+å†™SQLite+è¿”å›
                         å†…å­˜ç¼“å­˜

æ•°æ®å±‚æ¬¡ï¼š
L1ç¼“å­˜ï¼ˆå†…å­˜ï¼‰: çƒ­ç‚¹æ•°æ®ï¼ŒTTL=1åˆ†é’Ÿ
L2ç¼“å­˜ï¼ˆSQLiteï¼‰: å¸¸ç”¨æ•°æ®ï¼ŒTTL=1å°æ—¶
L3æŒä¹…åŒ–ï¼ˆPostgreSQLï¼‰: å…¨é‡æ•°æ®

-- Pythonå®ç°
class HybridCache:
    def __init__(self):
        self.memory_cache = {}  # L1
        self.sqlite_conn = sqlite3.connect(':memory:')  # L2
        self.pg_conn = psycopg2.connect(...)  # L3

    def get_user(self, user_id):
        # L1: å†…å­˜ç¼“å­˜
        if user_id in self.memory_cache:
            return self.memory_cache[user_id]

        # L2: SQLiteç¼“å­˜
        row = self.sqlite_conn.execute(
            "SELECT * FROM users WHERE id=?", (user_id,)
        ).fetchone()
        if row:
            self.memory_cache[user_id] = row  # å†™å›L1
            return row

        # L3: PostgreSQLä¸»åº“
        cursor = self.pg_conn.execute(
            "SELECT * FROM users WHERE id=%s", (user_id,)
        )
        row = cursor.fetchone()
        if row:
            # å†™å›L2
            self.sqlite_conn.execute(
                "INSERT OR REPLACE INTO users VALUES (?,?,?)", row
            )
            # å†™å›L1
            self.memory_cache[user_id] = row

        return row

æ€§èƒ½å¯¹æ¯”ï¼š
â€¢ L1å‘½ä¸­: ~0.1ms
â€¢ L2å‘½ä¸­: ~1ms
â€¢ L3å‘½ä¸­: ~5-10msï¼ˆç½‘ç»œï¼‰
â€¢ å‘½ä¸­ç‡: L1(60%) + L2(35%) + L3(5%)
â€¢ å¹³å‡å»¶è¿Ÿ: 0.1*0.6 + 1*0.35 + 7*0.05 = 0.76ms
```

---

## äºŒã€å…¸å‹åœºæ™¯æ¶æ„æ–¹æ¡ˆ

### 2.1 ç§»åŠ¨åº”ç”¨æ¶æ„ï¼ˆåœ¨çº¿+ç¦»çº¿ï¼‰

```text
ç§»åŠ¨Appæ··åˆæ¶æ„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ‰‹æœºAppï¼ˆFlutter/React Nativeï¼‰
â”œâ”€â”€ æœ¬åœ°SQLiteæ•°æ®åº“
â”‚   â”œâ”€â”€ ç”¨æˆ·ä¸ªäººæ•°æ®ï¼ˆprofiles.dbï¼‰
â”‚   â”œâ”€â”€ ç¦»çº¿å†…å®¹ç¼“å­˜ï¼ˆcache.dbï¼‰
â”‚   â””â”€â”€ å¾…åŒæ­¥æ“ä½œé˜Ÿåˆ—ï¼ˆsync_queue.dbï¼‰
â”‚
â”œâ”€â”€ ç½‘ç»œå±‚
â”‚   â”œâ”€â”€ HTTP APIå®¢æˆ·ç«¯
â”‚   â””â”€â”€ WebSocketï¼ˆå®æ—¶é€šçŸ¥ï¼‰
â”‚
â””â”€â”€ äº‘ç«¯PostgreSQL
    â”œâ”€â”€ ä¸­å¿ƒç”¨æˆ·æ•°æ®åº“
    â”œâ”€â”€ å†…å®¹ç®¡ç†ç³»ç»Ÿ
    â””â”€â”€ åˆ†ææ•°æ®ä»“åº“

æ•°æ®æµå‘ï¼š

1. å¯åŠ¨æ—¶å…¨é‡åŒæ­¥ï¼ˆé¦–æ¬¡ï¼‰
   PostgreSQL â†’ JSON API â†’ è§£æ â†’ SQLiteæ‰¹é‡INSERT

2. è¿è¡Œæ—¶å¢é‡åŒæ­¥ï¼ˆåå°ï¼‰
   SQLiteæŸ¥è¯¢last_sync_timestamp
   â†’ APIè¯·æ±‚WHERE updated_at > last_sync
   â†’ SQLite UPSERT

3. ç¦»çº¿æ“ä½œé˜Ÿåˆ—
   ç”¨æˆ·ä¿®æ”¹ â†’ INSERT INTO sync_queue
   â†’ ç½‘ç»œæ¢å¤æ—¶æ‰¹é‡ä¸Šä¼  â†’ PostgreSQLå¤„ç†
   â†’ æ¸…ç†sync_queue

4. å®æ—¶æ¨é€
   PostgreSQLè§¦å‘å™¨ â†’ é€šçŸ¥æœåŠ¡ â†’ WebSocket â†’ Appæ›´æ–°SQLite
```

**å®Œæ•´å®ç°ï¼ˆFlutter + Pythonï¼‰**ï¼š

```dart
// Flutterå®¢æˆ·ç«¯
class DatabaseService {
  late Database _db;

  Future<void> init() async {
    _db = await openDatabase(
      'app.db',
      version: 1,
      onCreate: (db, version) async {
        await db.execute('''
          CREATE TABLE users (
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            email TEXT NOT NULL,
            updated_at TEXT NOT NULL,
            synced INTEGER DEFAULT 0
          )
        ''');
        await db.execute('''
          CREATE TABLE sync_queue (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            table_name TEXT NOT NULL,
            operation TEXT NOT NULL,
            data TEXT NOT NULL,
            created_at TEXT NOT NULL,
            synced INTEGER DEFAULT 0
          )
        ''');

        // é…ç½®WALæ¨¡å¼
        await db.execute('PRAGMA journal_mode=WAL');
        await db.execute('PRAGMA synchronous=NORMAL');
      },
    );
  }

  // ç¦»çº¿æ“ä½œï¼šåŠ å…¥åŒæ­¥é˜Ÿåˆ—
  Future<void> updateUserOffline(int id, String name) async {
    await _db.transaction((txn) async {
      // æ›´æ–°æœ¬åœ°æ•°æ®
      await txn.update('users', {'name': name}, where: 'id = ?', whereArgs: [id]);

      // åŠ å…¥åŒæ­¥é˜Ÿåˆ—
      await txn.insert('sync_queue', {
        'table_name': 'users',
        'operation': 'UPDATE',
        'data': json.encode({'id': id, 'name': name}),
        'created_at': DateTime.now().toIso8601String(),
      });
    });
  }

  // åå°åŒæ­¥
  Future<void> syncToServer() async {
    if (!await isOnline()) return;

    // è·å–å¾…åŒæ­¥æ“ä½œ
    final pending = await _db.query(
      'sync_queue',
      where: 'synced = 0',
      orderBy: 'id ASC',
    );

    for (final op in pending) {
      try {
        // HTTPè¯·æ±‚åˆ°æœåŠ¡å™¨
        await api.syncOperation(
          op['table_name'],
          op['operation'],
          json.decode(op['data']),
        );

        // æ ‡è®°å·²åŒæ­¥
        await _db.update(
          'sync_queue',
          {'synced': 1},
          where: 'id = ?',
          whereArgs: [op['id']],
        );
      } catch (e) {
        print('åŒæ­¥å¤±è´¥: $e');
        break;  // åœæ­¢åŒæ­¥ï¼Œç­‰å¾…ä¸‹æ¬¡é‡è¯•
      }
    }

    // æ¸…ç†å·²åŒæ­¥è®°å½•ï¼ˆä¿ç•™7å¤©ï¼‰
    await _db.delete(
      'sync_queue',
      where: 'synced = 1 AND created_at < ?',
      whereArgs: [DateTime.now().subtract(Duration(days: 7)).toIso8601String()],
    );
  }
}
```

```python
# PythonæœåŠ¡ç«¯ï¼ˆFastAPIï¼‰
from fastapi import FastAPI, HTTPException
from sqlalchemy import create_engine
from sqlalchemy.orm import Session
import psycopg2

app = FastAPI()

# PostgreSQLè¿æ¥
pg_engine = create_engine('postgresql://user:pass@localhost/mydb')

@app.post("/sync/pull")
async def pull_sync(last_sync: str, user_id: int):
    """å®¢æˆ·ç«¯æ‹‰å–æ›´æ–°"""
    with Session(pg_engine) as session:
        # æŸ¥è¯¢è‡ªlast_syncä»¥æ¥çš„æ›´æ–°
        users = session.execute("""
            SELECT id, name, email, updated_at
            FROM users
            WHERE user_id = :user_id
                AND updated_at > :last_sync
            ORDER BY updated_at
        """, {"user_id": user_id, "last_sync": last_sync}).fetchall()

        return {"users": [dict(u) for u in users]}

@app.post("/sync/push")
async def push_sync(operations: list):
    """å®¢æˆ·ç«¯æ¨é€æ›´æ–°"""
    with Session(pg_engine) as session:
        try:
            for op in operations:
                if op['operation'] == 'UPDATE':
                    session.execute("""
                        UPDATE users
                        SET name = :name, updated_at = CURRENT_TIMESTAMP
                        WHERE id = :id
                    """, op['data'])
                elif op['operation'] == 'INSERT':
                    session.execute("""
                        INSERT INTO users (id, name, email)
                        VALUES (:id, :name, :email)
                        ON CONFLICT (id) DO UPDATE
                        SET name = EXCLUDED.name,
                            updated_at = CURRENT_TIMESTAMP
                    """, op['data'])

            session.commit()
            return {"status": "success"}
        except Exception as e:
            session.rollback()
            raise HTTPException(status_code=500, detail=str(e))
```

### 2.2 IoTè¾¹ç¼˜è®¡ç®—æ¶æ„

```text
IoTè¾¹ç¼˜è®¡ç®—æ··åˆæ¶æ„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

äº‘ç«¯ï¼ˆPostgreSQL + TimescaleDBï¼‰
â”œâ”€â”€ å†å²æ•°æ®å­˜å‚¨
â”œâ”€â”€ å¤§æ•°æ®åˆ†æ
â””â”€â”€ æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ

        â†• äº’è”ç½‘/4G/5G

è¾¹ç¼˜ç½‘å…³ï¼ˆRaspberry Pi + SQLiteï¼‰
â”œâ”€â”€ æœ¬åœ°SQLiteæ•°æ®åº“
â”‚   â”œâ”€â”€ ä¼ æ„Ÿå™¨æ•°æ®ç¼“å­˜ï¼ˆæœ€è¿‘7å¤©ï¼‰
â”‚   â”œâ”€â”€ è®¾å¤‡çŠ¶æ€ï¼ˆå®æ—¶ï¼‰
â”‚   â””â”€â”€ å‘Šè­¦è§„åˆ™ï¼ˆæœ¬åœ°å¤„ç†ï¼‰
â”œâ”€â”€ è¾¹ç¼˜è®¡ç®—
â”‚   â”œâ”€â”€ å®æ—¶æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ æœ¬åœ°å‘Šè­¦æ£€æµ‹
â”‚   â””â”€â”€ æ•°æ®é¢„èšåˆ
â””â”€â”€ å®šæ—¶åŒæ­¥
    â”œâ”€â”€ ä¸Šä¼ èšåˆæ•°æ®åˆ°äº‘ç«¯
    â””â”€â”€ ä¸‹è½½è§„åˆ™æ›´æ–°

        â†• Zigbee/BLE/WiFi

ç»ˆç«¯è®¾å¤‡ï¼ˆä¼ æ„Ÿå™¨ï¼‰
â””â”€â”€ æ¸©åº¦ã€æ¹¿åº¦ã€è¿åŠ¨ç­‰ä¼ æ„Ÿå™¨
```

**è¾¹ç¼˜ç½‘å…³SQLite Schema**ï¼š

```sql
-- SQLiteè¾¹ç¼˜æ•°æ®åº“è®¾è®¡

-- ä¼ æ„Ÿå™¨åŸå§‹æ•°æ®
CREATE TABLE sensor_data (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    sensor_id INTEGER NOT NULL,
    sensor_type TEXT NOT NULL,  -- 'temperature', 'humidity', etc.
    value REAL NOT NULL,
    timestamp TEXT NOT NULL,
    synced INTEGER DEFAULT 0,

    CHECK (timestamp = datetime(timestamp))  -- ç¡®ä¿ISO8601æ ¼å¼
) STRICT;

CREATE INDEX idx_sensor_timestamp ON sensor_data(sensor_id, timestamp DESC);
CREATE INDEX idx_sensor_synced ON sensor_data(synced) WHERE synced = 0;

-- é¢„èšåˆæ•°æ®ï¼ˆ5åˆ†é’Ÿç²’åº¦ï¼‰
CREATE TABLE sensor_aggregates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    sensor_id INTEGER NOT NULL,
    time_bucket TEXT NOT NULL,  -- '2025-12-04 10:00:00'
    avg_value REAL NOT NULL,
    min_value REAL NOT NULL,
    max_value REAL NOT NULL,
    count INTEGER NOT NULL,
    synced INTEGER DEFAULT 0,

    UNIQUE(sensor_id, time_bucket)
) STRICT;

-- è§¦å‘å™¨ï¼šè‡ªåŠ¨èšåˆ
CREATE TRIGGER auto_aggregate AFTER INSERT ON sensor_data
BEGIN
    INSERT INTO sensor_aggregates (
        sensor_id, time_bucket, avg_value, min_value, max_value, count
    )
    SELECT
        NEW.sensor_id,
        datetime(NEW.timestamp, 'start of hour',
                 '+' || (CAST(strftime('%M', NEW.timestamp) AS INT) / 5) * 5 || ' minutes'),
        NEW.value, NEW.value, NEW.value, 1
    ON CONFLICT (sensor_id, time_bucket) DO UPDATE SET
        avg_value = (avg_value * count + NEW.value) / (count + 1),
        min_value = MIN(min_value, NEW.value),
        max_value = MAX(max_value, NEW.value),
        count = count + 1;
END;

-- æœ¬åœ°å‘Šè­¦æ£€æµ‹
CREATE VIEW sensor_alerts AS
SELECT
    s.sensor_id,
    s.sensor_type,
    s.value,
    s.timestamp,
    CASE
        WHEN s.sensor_type = 'temperature' AND s.value > 35 THEN 'HIGH_TEMP'
        WHEN s.sensor_type = 'humidity' AND s.value < 30 THEN 'LOW_HUMIDITY'
        ELSE NULL
    END AS alert_type
FROM sensor_data s
WHERE timestamp >= datetime('now', '-1 hour')
HAVING alert_type IS NOT NULL;
```

**è¾¹ç¼˜è®¡ç®—Pythonä»£ç **ï¼š

```python
import sqlite3
import time
from datetime import datetime, timedelta

class EdgeGateway:
    def __init__(self):
        self.conn = sqlite3.connect('edge.db')
        self.conn.execute("PRAGMA journal_mode=WAL")
        self.conn.execute("PRAGMA synchronous=NORMAL")

    def process_sensor_data(self, sensor_id, sensor_type, value):
        """å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®"""
        # 1. å­˜å‚¨åŸå§‹æ•°æ®
        self.conn.execute("""
            INSERT INTO sensor_data (sensor_id, sensor_type, value, timestamp)
            VALUES (?, ?, ?, ?)
        """, (sensor_id, sensor_type, value, datetime.now().isoformat()))
        self.conn.commit()

        # 2. æ£€æŸ¥å‘Šè­¦ï¼ˆè§¦å‘å™¨å·²è‡ªåŠ¨èšåˆï¼‰
        alerts = self.conn.execute("""
            SELECT sensor_id, sensor_type, value, alert_type
            FROM sensor_alerts
            WHERE sensor_id = ?
        """, (sensor_id,)).fetchall()

        for alert in alerts:
            self.send_local_alert(alert)

    def sync_to_cloud(self, api_endpoint):
        """åŒæ­¥æ•°æ®åˆ°äº‘ç«¯"""
        # 1. ä¸Šä¼ èšåˆæ•°æ®ï¼ˆè€ŒéåŸå§‹æ•°æ®ï¼ŒèŠ‚çœå¸¦å®½ï¼‰
        aggregates = self.conn.execute("""
            SELECT sensor_id, time_bucket, avg_value, min_value, max_value, count
            FROM sensor_aggregates
            WHERE synced = 0
            ORDER BY time_bucket
            LIMIT 1000
        """).fetchall()

        if not aggregates:
            return

        try:
            # HTTP POSTåˆ°äº‘ç«¯
            response = requests.post(f"{api_endpoint}/ingest", json={
                'aggregates': [dict(zip(['sensor_id', 'time_bucket', 'avg', 'min', 'max', 'count'], row))
                              for row in aggregates]
            })

            if response.status_code == 200:
                # æ ‡è®°å·²åŒæ­¥
                ids = [row[0] for row in aggregates]
                placeholders = ','.join('?' * len(ids))
                self.conn.execute(f"""
                    UPDATE sensor_aggregates
                    SET synced = 1
                    WHERE id IN ({placeholders})
                """, ids)
                self.conn.commit()

        except Exception as e:
            print(f"åŒæ­¥å¤±è´¥: {e}")

    def cleanup_old_data(self):
        """æ¸…ç†è¶…è¿‡7å¤©çš„åŸå§‹æ•°æ®"""
        cutoff = (datetime.now() - timedelta(days=7)).isoformat()
        self.conn.execute("""
            DELETE FROM sensor_data
            WHERE timestamp < ? AND synced = 1
        """, (cutoff,))
        self.conn.commit()
```

### 2.3 åˆ†å¸ƒå¼ç³»ç»Ÿæœ¬åœ°ç¼“å­˜

```text
å¾®æœåŠ¡æ¶æ„ä¸­çš„SQLiteåº”ç”¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         API Gateway                 â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æœåŠ¡A        â”‚      â”‚ æœåŠ¡B        â”‚      â”‚ æœåŠ¡C        â”‚
â”‚ + SQLiteç¼“å­˜ â”‚      â”‚ + SQLiteç¼“å­˜ â”‚      â”‚ + SQLiteç¼“å­˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                         â”‚
                â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PostgreSQL   â”‚          â”‚ Redis        â”‚
        â”‚ (ä¸»æ•°æ®åº“)   â”‚          â”‚ (åˆ†å¸ƒå¼ç¼“å­˜) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¯ä¸ªå¾®æœåŠ¡çš„SQLiteç¼“å­˜ï¼š
â€¢ ç¼“å­˜è¯¥æœåŠ¡å¸¸ç”¨çš„æ•°æ®
â€¢ å‡å°‘æ•°æ®åº“æŸ¥è¯¢å‹åŠ›
â€¢ é™ä½æœåŠ¡é—´è°ƒç”¨
â€¢ æé«˜å“åº”é€Ÿåº¦

ä¼˜åŠ¿ï¼š
âœ… æœåŠ¡ç‹¬ç«‹æ€§ï¼ˆç¼“å­˜å¤±æ•ˆä¸å½±å“å…¶ä»–æœåŠ¡ï¼‰
âœ… é›¶é…ç½®ï¼ˆSQLiteæ— éœ€é¢å¤–éƒ¨ç½²ï¼‰
âœ… æˆæœ¬ä½ï¼ˆæ— éœ€Redisé›†ç¾¤ï¼‰
âœ… æ•…éšœéš”ç¦»ï¼ˆæ•°æ®åº“å®•æœºä»å¯ç”¨ç¼“å­˜æ•°æ®ï¼‰
```

---

## ä¸‰ã€æ•°æ®åŒæ­¥ç­–ç•¥

### 3.1 å…¨é‡åŒæ­¥

```python
def full_sync_postgres_to_sqlite():
    """å…¨é‡åŒæ­¥ï¼šPostgreSQL â†’ SQLite"""
    pg_conn = psycopg2.connect(...)
    sqlite_conn = sqlite3.connect('local.db')

    # 1. æ¸…ç©ºSQLite
    sqlite_conn.execute("DELETE FROM users")

    # 2. ä»PostgreSQLå¯¼å‡º
    pg_cursor = pg_conn.cursor('server_cursor')  # æœåŠ¡ç«¯æ¸¸æ ‡ï¼Œé¿å…å†…å­˜æº¢å‡º
    pg_cursor.execute("SELECT id, name, email, updated_at FROM users")

    # 3. æ‰¹é‡æ’å…¥SQLite
    batch_size = 1000
    batch = []

    for row in pg_cursor:
        batch.append(row)
        if len(batch) >= batch_size:
            sqlite_conn.executemany(
                "INSERT INTO users VALUES (?, ?, ?, ?)", batch
            )
            sqlite_conn.commit()
            batch = []

    if batch:
        sqlite_conn.executemany("INSERT INTO users VALUES (?, ?, ?, ?)", batch)
        sqlite_conn.commit()

    # 4. è®°å½•åŒæ­¥æ—¶é—´
    sqlite_conn.execute("""
        INSERT OR REPLACE INTO sync_metadata (key, value)
        VALUES ('last_full_sync', ?)
    """, (datetime.now().isoformat(),))
    sqlite_conn.commit()
```

### 3.2 å¢é‡åŒæ­¥

```python
def incremental_sync():
    """å¢é‡åŒæ­¥ï¼šåªåŒæ­¥å˜æ›´æ•°æ®"""
    sqlite_conn = sqlite3.connect('local.db')
    pg_conn = psycopg2.connect(...)

    # 1. è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
    last_sync = sqlite_conn.execute("""
        SELECT value FROM sync_metadata WHERE key = 'last_sync'
    """).fetchone()

    last_sync_time = last_sync[0] if last_sync else '1970-01-01'

    # 2. æŸ¥è¯¢PostgreSQLå˜æ›´æ•°æ®
    pg_cursor = pg_conn.cursor()
    pg_cursor.execute("""
        SELECT id, name, email, updated_at, deleted
        FROM users
        WHERE updated_at > %s
        ORDER BY updated_at
    """, (last_sync_time,))

    # 3. åº”ç”¨å˜æ›´åˆ°SQLite
    for row in pg_cursor:
        if row[4]:  # deleted=True
            sqlite_conn.execute("DELETE FROM users WHERE id = ?", (row[0],))
        else:
            sqlite_conn.execute("""
                INSERT OR REPLACE INTO users (id, name, email, updated_at)
                VALUES (?, ?, ?, ?)
            """, row[:4])

    sqlite_conn.execute("""
        INSERT OR REPLACE INTO sync_metadata (key, value)
        VALUES ('last_sync', ?)
    """, (datetime.now().isoformat(),))

    sqlite_conn.commit()
```

### 3.3 å†²çªè§£å†³

```sql
-- å†²çªè§£å†³ç­–ç•¥

-- ç­–ç•¥1: Last-Write-Winsï¼ˆæœ€åå†™å…¥èƒœå‡ºï¼‰
INSERT OR REPLACE INTO users (id, name, updated_at)
VALUES (?, ?, ?)
WHERE updated_at < ?;  -- åªæœ‰æ›´æ–°çš„æ•°æ®æ‰è¦†ç›–

-- ç­–ç•¥2: ç‰ˆæœ¬å·å†²çªæ£€æµ‹
UPDATE users
SET name = ?, version = version + 1
WHERE id = ? AND version = ?;
-- å¦‚æœversionä¸åŒ¹é…ï¼Œæ›´æ–°å¤±è´¥ï¼Œéœ€è¦åˆå¹¶

-- ç­–ç•¥3: å­—æ®µçº§åˆå¹¶
UPDATE users
SET
    name = CASE WHEN ? > name_updated_at THEN ? ELSE name END,
    email = CASE WHEN ? > email_updated_at THEN ? ELSE email END,
    name_updated_at = ?,
    email_updated_at = ?
WHERE id = ?;
```

---

## å››ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 4.1 æŸ¥è¯¢è·¯ç”±

```python
class SmartQueryRouter:
    """æ™ºèƒ½æŸ¥è¯¢è·¯ç”±"""

    def __init__(self):
        self.pg_pool = create_pg_pool()  # PostgreSQLè¿æ¥æ± 
        self.sqlite_conn = sqlite3.connect('cache.db')

    def query(self, sql, params, force_master=False):
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è·¯ç”±åˆ°ä¸åŒæ•°æ®åº“"""

        # 1. å†™æ“ä½œâ†’PostgreSQL
        if sql.strip().upper().startswith(('INSERT', 'UPDATE', 'DELETE')):
            return self._query_postgres(sql, params)

        # 2. å¼ºåˆ¶ä¸»åº“æŸ¥è¯¢
        if force_master:
            return self._query_postgres(sql, params)

        # 3. ç®€å•SELECTâ†’SQLiteç¼“å­˜
        if self._is_cacheable(sql):
            # å…ˆæŸ¥SQLite
            result = self._query_sqlite(sql, params)
            if result:
                return result

            # SQLiteæœªå‘½ä¸­ï¼ŒæŸ¥PostgreSQLå¹¶ç¼“å­˜
            result = self._query_postgres(sql, params)
            self._cache_result(sql, params, result)
            return result

        # 4. å¤æ‚æŸ¥è¯¢â†’PostgreSQL
        return self._query_postgres(sql, params)

    def _is_cacheable(self, sql):
        """åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦å¯ç¼“å­˜"""
        # ç®€å•SELECT + ä¸»é”®/ç´¢å¼•æŸ¥è¯¢
        return ('SELECT' in sql.upper() and
                'WHERE' in sql.upper() and
                'JOIN' not in sql.upper())
```

### 4.2 ç¼“å­˜ç­–ç•¥

```text
å¤šçº§ç¼“å­˜ç­–ç•¥
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

L1: åº”ç”¨å†…å­˜ç¼“å­˜ï¼ˆè¿›ç¨‹çº§ï¼‰
â”œâ”€â”€ å®¹é‡: 100MB
â”œâ”€â”€ TTL: 1åˆ†é’Ÿ
â”œâ”€â”€ å‘½ä¸­ç‡: 60%
â””â”€â”€ å»¶è¿Ÿ: 0.1ms

L2: SQLiteè¿›ç¨‹ç¼“å­˜ï¼ˆæœºå™¨çº§ï¼‰
â”œâ”€â”€ å®¹é‡: 1GB
â”œâ”€â”€ TTL: 1å°æ—¶
â”œâ”€â”€ å‘½ä¸­ç‡: 35%
â””â”€â”€ å»¶è¿Ÿ: 1ms

L3: PostgreSQLä¸»åº“ï¼ˆé›†ç¾¤çº§ï¼‰
â”œâ”€â”€ å®¹é‡: æ— é™
â”œâ”€â”€ TTL: æ°¸ä¹…
â”œâ”€â”€ å‘½ä¸­ç‡: 5%
â””â”€â”€ å»¶è¿Ÿ: 5-10ms

ç¼“å­˜æ›´æ–°ç­–ç•¥ï¼š
â€¢ Write-Through: å†™å…¥æ—¶åŒæ­¥æ›´æ–°æ‰€æœ‰ç¼“å­˜å±‚
â€¢ Write-Back: å†™å…¥ç¼“å­˜ï¼Œå¼‚æ­¥åˆ·æ–°æ•°æ®åº“
â€¢ Cache-Aside: åº”ç”¨è´Ÿè´£ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°
```

---

## äº”ã€å®Œæ•´å®ç°æ¡ˆä¾‹

### 5.1 æ–°é—»Appæ¶æ„

```python
# æ–°é—»Appå®Œæ•´æ¶æ„å®ç°

class NewsApp:
    def __init__(self):
        # æœ¬åœ°SQLiteï¼ˆWALæ¨¡å¼ï¼‰
        self.local_db = sqlite3.connect('news.db')
        self.local_db.execute("PRAGMA journal_mode=WAL")
        self.local_db.execute("PRAGMA synchronous=NORMAL")
        self.setup_local_schema()

        # äº‘ç«¯PostgreSQL
        self.cloud_db = psycopg2.connect(
            "postgresql://user:pass@cloud-db/news"
        )

    def setup_local_schema(self):
        """è®¾ç½®æœ¬åœ°æ•°æ®åº“Schema"""
        self.local_db.executescript("""
            -- æ–‡ç« ç¼“å­˜
            CREATE TABLE IF NOT EXISTS articles (
                article_id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                author TEXT NOT NULL,
                published_at TEXT NOT NULL,
                cached_at TEXT NOT NULL,
                read_count INTEGER DEFAULT 0
            ) STRICT;

            -- ç”¨æˆ·é˜…è¯»å†å²
            CREATE TABLE IF NOT EXISTS reading_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_id INTEGER NOT NULL,
                read_at TEXT NOT NULL,
                duration_seconds INTEGER,
                synced INTEGER DEFAULT 0
            ) STRICT;

            -- ç¦»çº¿æ”¶è—
            CREATE TABLE IF NOT EXISTS bookmarks (
                article_id INTEGER PRIMARY KEY,
                bookmarked_at TEXT NOT NULL,
                synced INTEGER DEFAULT 0
            ) STRICT;

            CREATE INDEX idx_reading_history_synced
            ON reading_history(synced) WHERE synced = 0;
        """)

    def fetch_articles(self, limit=20):
        """è·å–æ–‡ç« åˆ—è¡¨ï¼ˆç¦»çº¿ä¼˜å…ˆï¼‰"""
        # 1. å…ˆæŸ¥æœ¬åœ°ç¼“å­˜
        articles = self.local_db.execute("""
            SELECT article_id, title, author, published_at
            FROM articles
            WHERE cached_at >= datetime('now', '-1 day')
            ORDER BY published_at DESC
            LIMIT ?
        """, (limit,)).fetchall()

        if len(articles) >= limit:
            return articles  # ç¼“å­˜å……è¶³ï¼Œç›´æ¥è¿”å›

        # 2. ç¼“å­˜ä¸è¶³ï¼Œä»äº‘ç«¯æ‹‰å–
        if self.is_online():
            cursor = self.cloud_db.cursor()
            cursor.execute("""
                SELECT article_id, title, content, author, published_at
                FROM articles
                ORDER BY published_at DESC
                LIMIT %s
            """, (limit,))

            cloud_articles = cursor.fetchall()

            # 3. å†™å…¥æœ¬åœ°ç¼“å­˜
            self.local_db.executemany("""
                INSERT OR REPLACE INTO articles
                (article_id, title, content, author, published_at, cached_at)
                VALUES (?, ?, ?, ?, ?, ?)
            """, [(row[0], row[1], row[2], row[3], row[4], datetime.now().isoformat())
                  for row in cloud_articles])
            self.local_db.commit()

            return cloud_articles

        return articles  # ç¦»çº¿çŠ¶æ€ï¼Œè¿”å›ç¼“å­˜æ•°æ®

    def record_reading(self, article_id, duration):
        """è®°å½•é˜…è¯»è¡Œä¸º"""
        self.local_db.execute("""
            INSERT INTO reading_history (article_id, read_at, duration_seconds)
            VALUES (?, ?, ?)
        """, (article_id, datetime.now().isoformat(), duration))

        # æ›´æ–°æœ¬åœ°é˜…è¯»è®¡æ•°
        self.local_db.execute("""
            UPDATE articles SET read_count = read_count + 1
            WHERE article_id = ?
        """, (article_id,))

        self.local_db.commit()

    def sync_reading_history(self):
        """åŒæ­¥é˜…è¯»å†å²åˆ°äº‘ç«¯"""
        if not self.is_online():
            return

        # è·å–æœªåŒæ­¥çš„é˜…è¯»è®°å½•
        records = self.local_db.execute("""
            SELECT id, article_id, read_at, duration_seconds
            FROM reading_history
            WHERE synced = 0
            LIMIT 100
        """).fetchall()

        if not records:
            return

        try:
            cursor = self.cloud_db.cursor()
            # PostgreSQLæ‰¹é‡æ’å…¥
            cursor.executemany("""
                INSERT INTO reading_history (article_id, user_id, read_at, duration_seconds)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (article_id, user_id, read_at) DO NOTHING
            """, [(r[1], self.user_id, r[2], r[3]) for r in records])

            self.cloud_db.commit()

            # æ ‡è®°SQLiteä¸­å·²åŒæ­¥
            ids = [r[0] for r in records]
            placeholders = ','.join('?' * len(ids))
            self.local_db.execute(f"""
                UPDATE reading_history
                SET synced = 1
                WHERE id IN ({placeholders})
            """, ids)
            self.local_db.commit()

        except Exception as e:
            self.cloud_db.rollback()
            print(f"åŒæ­¥å¤±è´¥: {e}")
```

### 5.2 ååŒåŠå…¬ç³»ç»Ÿ

```text
ååŒåŠå…¬æ··åˆæ¶æ„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

äº‘ç«¯PostgreSQL:
â€¢ æ–‡æ¡£ä¸»ç‰ˆæœ¬ï¼ˆauthoritativeï¼‰
â€¢ å®æ—¶åä½œï¼ˆå¤šäººç¼–è¾‘ï¼‰
â€¢ ç‰ˆæœ¬å†å²ï¼ˆå®Œæ•´å®¡è®¡ï¼‰

æœ¬åœ°SQLite:
â€¢ æ–‡æ¡£è‰ç¨¿ï¼ˆç¦»çº¿ç¼–è¾‘ï¼‰
â€¢ å¿«é€Ÿæœç´¢ç´¢å¼•
â€¢ é™„ä»¶ç¼“å­˜

æ•°æ®ä¸€è‡´æ€§ä¿è¯ï¼š
â€¢ æ“ä½œè½¬æ¢ï¼ˆOperational Transformationï¼‰
â€¢ CRDTï¼ˆConflict-free Replicated Data Typeï¼‰
â€¢ ç‰ˆæœ¬å‘é‡ï¼ˆVector Clockï¼‰
```

```sql
-- PostgreSQL: æ–‡æ¡£ç‰ˆæœ¬è¡¨

CREATE TABLE documents (
    doc_id BIGSERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    version INTEGER NOT NULL DEFAULT 1,
    created_by BIGINT NOT NULL,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    vector_clock JSONB NOT NULL DEFAULT '{}'::jsonb  -- å‘é‡æ—¶é’Ÿ
);

CREATE TABLE document_operations (
    op_id BIGSERIAL PRIMARY KEY,
    doc_id BIGINT NOT NULL REFERENCES documents(doc_id),
    operation JSONB NOT NULL,  -- {type: 'insert', pos: 10, text: 'hello'}
    user_id BIGINT NOT NULL,
    applied_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    vector_clock JSONB NOT NULL
);

-- SQLite: æœ¬åœ°è‰ç¨¿

CREATE TABLE local_drafts (
    doc_id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    local_version INTEGER NOT NULL DEFAULT 1,
    server_version INTEGER NOT NULL,  -- åŸºäºå“ªä¸ªæœåŠ¡å™¨ç‰ˆæœ¬
    pending_ops TEXT NOT NULL DEFAULT '[]',  -- JSONæ•°ç»„
    last_sync_at TEXT
) STRICT;
```

---

**æ¶æ„è®¾è®¡å®Œæˆï¼**

æœ¬æ–‡æ¡£å±•ç¤ºäº†ï¼š

- âœ… 3ç§æ··åˆæ¶æ„è®¾è®¡æ¨¡å¼
- âœ… 3ç§å…¸å‹åœºæ™¯å®Œæ•´æ–¹æ¡ˆï¼ˆç§»åŠ¨App/IoT/å¾®æœåŠ¡ï¼‰
- âœ… å®Œæ•´çš„æ•°æ®åŒæ­¥ç­–ç•¥ï¼ˆå…¨é‡/å¢é‡/å†²çªè§£å†³ï¼‰
- âœ… å¤šçº§ç¼“å­˜ä¼˜åŒ–ç­–ç•¥
- âœ… 2ä¸ªç«¯åˆ°ç«¯å®ç°æ¡ˆä¾‹ï¼ˆæ–°é—»App/ååŒåŠå…¬ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-12-04
**ç»´æŠ¤è€…**: Data-Science Architecture Team
