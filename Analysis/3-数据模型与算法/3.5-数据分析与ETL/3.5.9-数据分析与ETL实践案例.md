# 数据分析与ETL实践案例

## 1. 数据分析实践案例

### 1.1 探索性数据分析(EDA)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

class ExploratoryDataAnalysis:
    def __init__(self):
        self.data = None
        self.numeric_columns = []
        self.categorical_columns = []
    
    def load_sample_data(self):
        """加载示例数据集"""
        np.random.seed(42)
        n_samples = 1000
        
        # 生成示例数据
        data = {
            'age': np.random.normal(35, 10, n_samples),
            'income': np.random.lognormal(10, 0.5, n_samples),
            'education_years': np.random.poisson(12, n_samples),
            'city': np.random.choice(['北京', '上海', '广州', '深圳'], n_samples),
            'gender': np.random.choice(['男', '女'], n_samples),
            'satisfaction_score': np.random.uniform(1, 10, n_samples),
            'purchase_amount': np.random.exponential(1000, n_samples)
        }
        
        self.data = pd.DataFrame(data)
        self._identify_column_types()
        return self.data
    
    def _identify_column_types(self):
        """识别数值型和分类型列"""
        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_columns = self.data.select_dtypes(include=['object']).columns.tolist()
    
    def basic_statistics(self):
        """基本统计信息"""
        print("=== 基本统计信息 ===")
        print(f"数据集形状: {self.data.shape}")
        print(f"\n数值型列: {self.numeric_columns}")
        print(f"分类型列: {self.categorical_columns}")
        
        print("\n数值型变量统计:")
        print(self.data[self.numeric_columns].describe())
        
        print("\n分类型变量统计:")
        for col in self.categorical_columns:
            print(f"\n{col}:")
            print(self.data[col].value_counts())
    
    def missing_value_analysis(self):
        """缺失值分析"""
        print("=== 缺失值分析 ===")
        missing_data = self.data.isnull().sum()
        missing_percentage = (missing_data / len(self.data)) * 100
        
        missing_df = pd.DataFrame({
            '缺失数量': missing_data,
            '缺失百分比': missing_percentage
        })
        
        print(missing_df[missing_df['缺失数量'] > 0])
    
    def correlation_analysis(self):
        """相关性分析"""
        print("=== 相关性分析 ===")
        correlation_matrix = self.data[self.numeric_columns].corr()
        
        # 绘制相关性热力图
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('数值变量相关性热力图')
        plt.tight_layout()
        plt.show()
        
        # 找出高相关性变量对
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:
                    high_corr_pairs.append({
                        '变量1': correlation_matrix.columns[i],
                        '变量2': correlation_matrix.columns[j],
                        '相关系数': corr_value
                    })
        
        if high_corr_pairs:
            print("\n高相关性变量对 (|r| > 0.7):")
            for pair in high_corr_pairs:
                print(f"{pair['变量1']} - {pair['变量2']}: {pair['相关系数']:.3f}")
    
    def distribution_analysis(self):
        """分布分析"""
        print("=== 分布分析 ===")
        
        # 数值型变量分布
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.ravel()
        
        for i, col in enumerate(self.numeric_columns[:6]):
            # 直方图
            axes[i].hist(self.data[col], bins=30, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'{col} 分布')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('频数')
        
        plt.tight_layout()
        plt.show()
        
        # 正态性检验
        print("\n正态性检验 (Shapiro-Wilk):")
        for col in self.numeric_columns:
            stat, p_value = stats.shapiro(self.data[col])
            print(f"{col}: 统计量={stat:.4f}, p值={p_value:.4f}")
    
    def outlier_analysis(self):
        """异常值分析"""
        print("=== 异常值分析 ===")
        
        for col in self.numeric_columns:
            Q1 = self.data[col].quantile(0.25)
            Q3 = self.data[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = self.data[(self.data[col] < lower_bound) | (self.data[col] > upper_bound)]
            
            print(f"\n{col}:")
            print(f"  异常值数量: {len(outliers)}")
            print(f"  异常值比例: {len(outliers)/len(self.data)*100:.2f}%")
            print(f"  异常值范围: [{outliers[col].min():.2f}, {outliers[col].max():.2f}]")

# 使用示例
print("=== 探索性数据分析示例 ===")
eda = ExploratoryDataAnalysis()
data = eda.load_sample_data()

eda.basic_statistics()
eda.missing_value_analysis()
eda.correlation_analysis()
eda.distribution_analysis()
eda.outlier_analysis()
```

### 1.2 时间序列分析

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

class TimeSeriesAnalysis:
    def __init__(self):
        self.data = None
        self.time_column = None
    
    def generate_sample_data(self, n_days=365):
        """生成示例时间序列数据"""
        # 生成日期序列
        start_date = datetime(2023, 1, 1)
        dates = [start_date + timedelta(days=i) for i in range(n_days)]
        
        # 生成时间序列数据
        np.random.seed(42)
        trend = np.linspace(100, 150, n_days)  # 趋势
        seasonal = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365)  # 季节性
        noise = np.random.normal(0, 5, n_days)  # 噪声
        
        values = trend + seasonal + noise
        
        self.data = pd.DataFrame({
            'date': dates,
            'value': values
        })
        self.time_column = 'date'
        
        return self.data
    
    def set_time_index(self):
        """设置时间索引"""
        self.data = self.data.set_index(self.time_column)
        self.data = self.data.sort_index()
    
    def plot_time_series(self):
        """绘制时间序列图"""
        plt.figure(figsize=(12, 6))
        plt.plot(self.data.index, self.data['value'])
        plt.title('时间序列数据')
        plt.xlabel('时间')
        plt.ylabel('数值')
        plt.grid(True)
        plt.show()
    
    def seasonal_decomposition(self):
        """季节性分解"""
        decomposition = seasonal_decompose(self.data['value'], period=365, extrapolate_trend='freq')
        
        fig, axes = plt.subplots(4, 1, figsize=(12, 10))
        
        # 原始数据
        axes[0].plot(self.data.index, self.data['value'])
        axes[0].set_title('原始数据')
        axes[0].grid(True)
        
        # 趋势
        axes[1].plot(self.data.index, decomposition.trend)
        axes[1].set_title('趋势')
        axes[1].grid(True)
        
        # 季节性
        axes[2].plot(self.data.index, decomposition.seasonal)
        axes[2].set_title('季节性')
        axes[2].grid(True)
        
        # 残差
        axes[3].plot(self.data.index, decomposition.resid)
        axes[3].set_title('残差')
        axes[3].grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def stationarity_test(self):
        """平稳性检验"""
        print("=== ADF平稳性检验 ===")
        result = adfuller(self.data['value'])
        
        print(f'ADF统计量: {result[0]:.4f}')
        print(f'p值: {result[1]:.4f}')
        print('临界值:')
        for key, value in result[4].items():
            print(f'\t{key}: {value:.4f}')
        
        if result[1] <= 0.05:
            print("结论: 时间序列是平稳的 (p值 <= 0.05)")
        else:
            print("结论: 时间序列是非平稳的 (p值 > 0.05)")
    
    def autocorrelation_analysis(self):
        """自相关分析"""
        fig, axes = plt.subplots(2, 1, figsize=(12, 8))
        
        # 自相关函数
        plot_acf(self.data['value'], ax=axes[0], lags=40)
        axes[0].set_title('自相关函数(ACF)')
        
        # 偏自相关函数
        plot_pacf(self.data['value'], ax=axes[1], lags=40)
        axes[1].set_title('偏自相关函数(PACF)')
        
        plt.tight_layout()
        plt.show()
    
    def rolling_statistics(self):
        """滚动统计"""
        # 计算滚动均值和标准差
        rolling_mean = self.data['value'].rolling(window=30).mean()
        rolling_std = self.data['value'].rolling(window=30).std()
        
        plt.figure(figsize=(12, 6))
        plt.plot(self.data.index, self.data['value'], label='原始数据', alpha=0.7)
        plt.plot(self.data.index, rolling_mean, label='30天滚动均值', linewidth=2)
        plt.fill_between(self.data.index, 
                        rolling_mean - 2*rolling_std, 
                        rolling_mean + 2*rolling_std, 
                        alpha=0.3, label='±2σ区间')
        plt.title('时间序列滚动统计')
        plt.xlabel('时间')
        plt.ylabel('数值')
        plt.legend()
        plt.grid(True)
        plt.show()

# 使用示例
print("=== 时间序列分析示例 ===")
tsa = TimeSeriesAnalysis()
data = tsa.generate_sample_data()
tsa.set_time_index()

tsa.plot_time_series()
tsa.seasonal_decomposition()
tsa.stationarity_test()
tsa.autocorrelation_analysis()
tsa.rolling_statistics()
```

## 2. ETL流程实践案例

### 2.1 数据提取与转换

```python
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import json
import xml.etree.ElementTree as ET
from datetime import datetime

class ETLProcessor:
    def __init__(self):
        self.extracted_data = {}
        self.transformed_data = None
    
    def extract_from_csv(self, file_path):
        """从CSV文件提取数据"""
        try:
            data = pd.read_csv(file_path)
            self.extracted_data['csv'] = data
            print(f"成功从CSV提取数据: {data.shape}")
            return data
        except Exception as e:
            print(f"CSV提取失败: {e}")
            return None
    
    def extract_from_json(self, file_path):
        """从JSON文件提取数据"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # 假设JSON是列表格式
            if isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = pd.DataFrame([data])
            
            self.extracted_data['json'] = df
            print(f"成功从JSON提取数据: {df.shape}")
            return df
        except Exception as e:
            print(f"JSON提取失败: {e}")
            return None
    
    def extract_from_database(self, connection_string, query):
        """从数据库提取数据"""
        try:
            engine = create_engine(connection_string)
            with engine.connect() as conn:
                data = pd.read_sql(text(query), conn)
            
            self.extracted_data['database'] = data
            print(f"成功从数据库提取数据: {data.shape}")
            return data
        except Exception as e:
            print(f"数据库提取失败: {e}")
            return None
    
    def data_cleaning(self, data):
        """数据清洗"""
        print("=== 数据清洗 ===")
        
        # 处理缺失值
        print(f"清洗前缺失值:\n{data.isnull().sum()}")
        
        # 数值型列用中位数填充
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if data[col].isnull().sum() > 0:
                median_value = data[col].median()
                data[col].fillna(median_value, inplace=True)
                print(f"列 {col}: 用中位数 {median_value:.2f} 填充缺失值")
        
        # 分类型列用众数填充
        categorical_columns = data.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            if data[col].isnull().sum() > 0:
                mode_value = data[col].mode()[0]
                data[col].fillna(mode_value, inplace=True)
                print(f"列 {col}: 用众数 '{mode_value}' 填充缺失值")
        
        print(f"清洗后缺失值:\n{data.isnull().sum()}")
        return data
    
    def data_transformation(self, data):
        """数据转换"""
        print("=== 数据转换 ===")
        
        # 数据类型转换
        print("数据类型转换:")
        for col in data.columns:
            if data[col].dtype == 'object':
                # 尝试转换为数值型
                try:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
                    if not data[col].isnull().all():
                        print(f"列 {col}: object -> numeric")
                except:
                    pass
        
        # 创建新特征
        print("\n特征工程:")
        
        # 如果有日期列，提取时间特征
        date_columns = data.select_dtypes(include=['datetime64']).columns
        for col in date_columns:
            data[f'{col}_year'] = data[col].dt.year
            data[f'{col}_month'] = data[col].dt.month
            data[f'{col}_day'] = data[col].dt.day
            data[f'{col}_weekday'] = data[col].dt.weekday
            print(f"从 {col} 提取时间特征")
        
        # 数值型列的统计特征
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col not in [f'{dc}_year' for dc in date_columns]:
                data[f'{col}_log'] = np.log1p(data[col])
                data[f'{col}_squared'] = data[col] ** 2
                print(f"为 {col} 创建对数和平方特征")
        
        return data
    
    def data_validation(self, data):
        """数据验证"""
        print("=== 数据验证 ===")
        
        # 基本验证
        print(f"数据形状: {data.shape}")
        print(f"数据类型:\n{data.dtypes}")
        
        # 数值型数据验证
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            print(f"\n{col}:")
            print(f"  范围: [{data[col].min():.2f}, {data[col].max():.2f}]")
            print(f"  均值: {data[col].mean():.2f}")
            print(f"  标准差: {data[col].std():.2f}")
            
            # 异常值检测
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = data[(data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)]
            print(f"  异常值数量: {len(outliers)}")
        
        # 分类型数据验证
        categorical_columns = data.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            print(f"\n{col}:")
            print(f"  唯一值数量: {data[col].nunique()}")
            print(f"  前5个值: {data[col].value_counts().head().to_dict()}")
    
    def run_etl_pipeline(self, data_sources):
        """运行完整的ETL流程"""
        print("=== ETL流程开始 ===")
        
        # 1. 提取阶段
        print("\n1. 数据提取阶段")
        for source_type, source_info in data_sources.items():
            if source_type == 'csv':
                self.extract_from_csv(source_info['path'])
            elif source_type == 'json':
                self.extract_from_json(source_info['path'])
            elif source_type == 'database':
                self.extract_from_database(source_info['connection'], source_info['query'])
        
        # 2. 转换阶段
        print("\n2. 数据转换阶段")
        if self.extracted_data:
            # 合并所有数据源
            all_data = []
            for source_name, data in self.extracted_data.items():
                data['source'] = source_name
                all_data.append(data)
            
            self.transformed_data = pd.concat(all_data, ignore_index=True)
            
            # 数据清洗
            self.transformed_data = self.data_cleaning(self.transformed_data)
            
            # 数据转换
            self.transformed_data = self.data_transformation(self.transformed_data)
            
            # 数据验证
            self.data_validation(self.transformed_data)
        
        print("\n=== ETL流程完成 ===")
        return self.transformed_data

# 使用示例
print("=== ETL流程示例 ===")

# 创建示例数据源
data_sources = {
    'csv': {'path': 'sample_data.csv'},
    'json': {'path': 'sample_data.json'},
    'database': {
        'connection': 'sqlite:///sample.db',
        'query': 'SELECT * FROM users'
    }
}

etl = ETLProcessor()

# 注意：实际使用时需要提供真实的数据文件
# result = etl.run_etl_pipeline(data_sources)
print("ETL流程示例代码已准备就绪，需要真实数据文件才能运行")
```

### 2.2 数据加载与存储

```python
import pandas as pd
import sqlite3
import json
import pickle
from sqlalchemy import create_engine, text
import os

class DataLoader:
    def __init__(self):
        self.data = None
    
    def load_to_csv(self, data, file_path):
        """加载数据到CSV文件"""
        try:
            data.to_csv(file_path, index=False, encoding='utf-8')
            print(f"数据成功保存到CSV: {file_path}")
            return True
        except Exception as e:
            print(f"保存CSV失败: {e}")
            return False
    
    def load_to_json(self, data, file_path):
        """加载数据到JSON文件"""
        try:
            data.to_json(file_path, orient='records', force_ascii=False, indent=2)
            print(f"数据成功保存到JSON: {file_path}")
            return True
        except Exception as e:
            print(f"保存JSON失败: {e}")
            return False
    
    def load_to_database(self, data, connection_string, table_name):
        """加载数据到数据库"""
        try:
            engine = create_engine(connection_string)
            data.to_sql(table_name, engine, if_exists='replace', index=False)
            print(f"数据成功保存到数据库表: {table_name}")
            return True
        except Exception as e:
            print(f"保存到数据库失败: {e}")
            return False
    
    def load_to_parquet(self, data, file_path):
        """加载数据到Parquet文件"""
        try:
            data.to_parquet(file_path, index=False)
            print(f"数据成功保存到Parquet: {file_path}")
            return True
        except Exception as e:
            print(f"保存Parquet失败: {e}")
            return False
    
    def load_to_pickle(self, data, file_path):
        """加载数据到Pickle文件"""
        try:
            with open(file_path, 'wb') as f:
                pickle.dump(data, f)
            print(f"数据成功保存到Pickle: {file_path}")
            return True
        except Exception as e:
            print(f"保存Pickle失败: {e}")
            return False

class DataWarehouse:
    def __init__(self, db_path):
        self.db_path = db_path
        self.engine = create_engine(f'sqlite:///{db_path}')
    
    def create_tables(self):
        """创建数据仓库表结构"""
        with self.engine.connect() as conn:
            # 创建维度表
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS dim_customers (
                    customer_id INTEGER PRIMARY KEY,
                    name TEXT,
                    email TEXT,
                    city TEXT,
                    created_date DATE
                )
            """))
            
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS dim_products (
                    product_id INTEGER PRIMARY KEY,
                    name TEXT,
                    category TEXT,
                    price REAL
                )
            """))
            
            # 创建事实表
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS fact_orders (
                    order_id INTEGER PRIMARY KEY,
                    customer_id INTEGER,
                    product_id INTEGER,
                    quantity INTEGER,
                    total_amount REAL,
                    order_date DATE,
                    FOREIGN KEY (customer_id) REFERENCES dim_customers(customer_id),
                    FOREIGN KEY (product_id) REFERENCES dim_products(product_id)
                )
            """))
            
            conn.commit()
            print("数据仓库表结构创建完成")
    
    def load_dimension_data(self, data, table_name):
        """加载维度数据"""
        try:
            data.to_sql(table_name, self.engine, if_exists='replace', index=False)
            print(f"维度表 {table_name} 数据加载完成")
            return True
        except Exception as e:
            print(f"加载维度表失败: {e}")
            return False
    
    def load_fact_data(self, data, table_name):
        """加载事实数据"""
        try:
            data.to_sql(table_name, self.engine, if_exists='append', index=False)
            print(f"事实表 {table_name} 数据加载完成")
            return True
        except Exception as e:
            print(f"加载事实表失败: {e}")
            return False
    
    def query_data(self, sql_query):
        """查询数据"""
        try:
            with self.engine.connect() as conn:
                result = pd.read_sql(text(sql_query), conn)
            return result
        except Exception as e:
            print(f"查询失败: {e}")
            return None

# 使用示例
print("=== 数据加载与存储示例 ===")

# 创建示例数据
sample_data = pd.DataFrame({
    'id': range(1, 101),
    'name': [f'用户{i}' for i in range(1, 101)],
    'age': np.random.randint(18, 65, 100),
    'city': np.random.choice(['北京', '上海', '广州', '深圳'], 100),
    'salary': np.random.normal(8000, 2000, 100)
})

# 数据加载器示例
loader = DataLoader()

# 保存到不同格式
loader.load_to_csv(sample_data, 'output_data.csv')
loader.load_to_json(sample_data, 'output_data.json')
loader.load_to_parquet(sample_data, 'output_data.parquet')
loader.load_to_pickle(sample_data, 'output_data.pkl')

# 数据仓库示例
warehouse = DataWarehouse('data_warehouse.db')
warehouse.create_tables()

# 加载维度数据
customers_data = sample_data[['id', 'name', 'city']].copy()
customers_data.columns = ['customer_id', 'name', 'city']
warehouse.load_dimension_data(customers_data, 'dim_customers')

# 查询示例
result = warehouse.query_data("SELECT * FROM dim_customers LIMIT 5")
print("\n查询结果:")
print(result)
```

## 3. 数据流处理实践案例

### 3.1 实时数据流处理

```python
import time
import threading
from queue import Queue
from collections import deque
import json

class DataStreamProcessor:
    def __init__(self, window_size=100):
        self.data_queue = Queue()
        self.processed_data = deque(maxlen=window_size)
        self.window_size = window_size
        self.running = False
        self.processors = []
    
    def add_processor(self, processor_func):
        """添加数据处理器"""
        self.processors.append(processor_func)
    
    def start_processing(self):
        """开始处理数据流"""
        self.running = True
        self.processing_thread = threading.Thread(target=self._process_loop)
        self.processing_thread.start()
        print("数据流处理已启动")
    
    def stop_processing(self):
        """停止处理数据流"""
        self.running = False
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join()
        print("数据流处理已停止")
    
    def _process_loop(self):
        """数据处理循环"""
        while self.running:
            try:
                # 非阻塞方式获取数据
                data = self.data_queue.get_nowait()
                processed_data = self._apply_processors(data)
                self.processed_data.append(processed_data)
                
                # 实时统计
                self._calculate_statistics()
                
            except:
                time.sleep(0.1)  # 短暂休眠避免CPU占用过高
    
    def _apply_processors(self, data):
        """应用所有处理器"""
        result = data
        for processor in self.processors:
            result = processor(result)
        return result
    
    def _calculate_statistics(self):
        """计算实时统计信息"""
        if len(self.processed_data) > 0:
            # 计算滑动窗口统计
            recent_data = list(self.processed_data)
            if len(recent_data) >= 10:  # 至少10个数据点
                values = [item.get('value', 0) for item in recent_data[-10:]]
                avg_value = sum(values) / len(values)
                max_value = max(values)
                min_value = min(values)
                
                print(f"实时统计 - 平均值: {avg_value:.2f}, 最大值: {max_value:.2f}, 最小值: {min_value:.2f}")
    
    def add_data(self, data):
        """添加数据到流"""
        self.data_queue.put(data)

class RealTimeAnalytics:
    def __init__(self):
        self.stream_processor = DataStreamProcessor(window_size=1000)
        self._setup_processors()
    
    def _setup_processors(self):
        """设置数据处理器"""
        # 数据清洗处理器
        def clean_data(data):
            if 'value' in data and isinstance(data['value'], (int, float)):
                # 异常值检测和处理
                if data['value'] > 1000 or data['value'] < 0:
                    data['value'] = 0
                data['cleaned'] = True
            return data
        
        # 数据转换处理器
        def transform_data(data):
            if 'value' in data:
                data['value_squared'] = data['value'] ** 2
                data['value_log'] = np.log1p(data['value'])
            return data
        
        # 数据聚合处理器
        def aggregate_data(data):
            data['timestamp'] = time.time()
            return data
        
        self.stream_processor.add_processor(clean_data)
        self.stream_processor.add_processor(transform_data)
        self.stream_processor.add_processor(aggregate_data)
    
    def start_analytics(self):
        """启动实时分析"""
        self.stream_processor.start_processing()
    
    def stop_analytics(self):
        """停止实时分析"""
        self.stream_processor.stop_processing()
    
    def simulate_data_stream(self, duration=60):
        """模拟数据流"""
        print(f"开始模拟数据流，持续{duration}秒...")
        
        start_time = time.time()
        while time.time() - start_time < duration:
            # 生成模拟数据
            data = {
                'id': int(time.time() * 1000),
                'value': np.random.normal(100, 20),
                'category': np.random.choice(['A', 'B', 'C']),
                'source': 'sensor_1'
            }
            
            self.stream_processor.add_data(data)
            time.sleep(0.1)  # 每0.1秒生成一个数据点
        
        print("数据流模拟结束")

# 使用示例
print("=== 实时数据流处理示例 ===")

analytics = RealTimeAnalytics()
analytics.start_analytics()

# 模拟数据流
analytics.simulate_data_stream(duration=10)

analytics.stop_analytics()
```

这些实践案例展示了数据分析与ETL中的核心概念和实际应用，包括探索性数据分析、时间序列分析、ETL流程、数据加载存储和实时数据流处理等。每个案例都提供了完整的代码实现和使用示例，帮助理解这些概念的实际应用。
