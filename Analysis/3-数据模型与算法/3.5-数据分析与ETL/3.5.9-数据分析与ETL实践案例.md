# æ•°æ®åˆ†æä¸ETLå®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [æ•°æ®åˆ†æä¸ETLå®è·µæ¡ˆä¾‹](#æ•°æ®åˆ†æä¸etlå®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ•°æ®åˆ†æå®è·µæ¡ˆä¾‹](#1-æ•°æ®åˆ†æå®è·µæ¡ˆä¾‹)
    - [1.1. æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)](#11-æ¢ç´¢æ€§æ•°æ®åˆ†æeda)
  - [2. æ—¶é—´åºåˆ—åˆ†æ](#2-æ—¶é—´åºåˆ—åˆ†æ)
  - [3. ETLæµç¨‹å®è·µæ¡ˆä¾‹](#3-etlæµç¨‹å®è·µæ¡ˆä¾‹)
    - [3.1. æ•°æ®æå–ä¸è½¬æ¢](#31-æ•°æ®æå–ä¸è½¬æ¢)
  - [4. æ•°æ®åŠ è½½ä¸å­˜å‚¨](#4-æ•°æ®åŠ è½½ä¸å­˜å‚¨)
  - [5. æ•°æ®æµå¤„ç†å®è·µæ¡ˆä¾‹](#5-æ•°æ®æµå¤„ç†å®è·µæ¡ˆä¾‹)
    - [5.1. å®æ—¶æ•°æ®æµå¤„ç†](#51-å®æ—¶æ•°æ®æµå¤„ç†)

---


## 1. æ•°æ®åˆ†æå®è·µæ¡ˆä¾‹

### 1.1. æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

class ExploratoryDataAnalysis:
    def __init__(self):
        self.data = None
        self.numeric_columns = []
        self.categorical_columns = []

    def load_sample_data(self):
        """åŠ è½½ç¤ºä¾‹æ•°æ®é›†"""
        np.random.seed(42)
        n_samples = 1000

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
        data = {
            'age': np.random.normal(35, 10, n_samples),
            'income': np.random.lognormal(10, 0.5, n_samples),
            'education_years': np.random.poisson(12, n_samples),
            'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'], n_samples),
            'gender': np.random.choice(['ç”·', 'å¥³'], n_samples),
            'satisfaction_score': np.random.uniform(1, 10, n_samples),
            'purchase_amount': np.random.exponential(1000, n_samples)
        }

        self.data = pd.DataFrame(data)
        self._identify_column_types()
        return self.data

    def _identify_column_types(self):
        """è¯†åˆ«æ•°å€¼å‹å’Œåˆ†ç±»å‹åˆ—"""
        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_columns = self.data.select_dtypes(include=['object']).columns.tolist()

    def basic_statistics(self):
        """åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        print("=== åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ ===")
        print(f"æ•°æ®é›†å½¢çŠ¶: {self.data.shape}")
        print(f"\næ•°å€¼å‹åˆ—: {self.numeric_columns}")
        print(f"åˆ†ç±»å‹åˆ—: {self.categorical_columns}")

        print("\næ•°å€¼å‹å˜é‡ç»Ÿè®¡:")
        print(self.data[self.numeric_columns].describe())

        print("\nåˆ†ç±»å‹å˜é‡ç»Ÿè®¡:")
        for col in self.categorical_columns:
            print(f"\n{col}:")
            print(self.data[col].value_counts())

    def missing_value_analysis(self):
        """ç¼ºå¤±å€¼åˆ†æ"""
        print("=== ç¼ºå¤±å€¼åˆ†æ ===")
        missing_data = self.data.isnull().sum()
        missing_percentage = (missing_data / len(self.data)) * 100

        missing_df = pd.DataFrame({
            'ç¼ºå¤±æ•°é‡': missing_data,
            'ç¼ºå¤±ç™¾åˆ†æ¯”': missing_percentage
        })

        print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0])

    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        print("=== ç›¸å…³æ€§åˆ†æ ===")
        correlation_matrix = self.data[self.numeric_columns].corr()

# ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('æ•°å€¼å˜é‡ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.tight_layout()
        plt.show()

# æ‰¾å‡ºé«˜ç›¸å…³æ€§å˜é‡å¯¹
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:
                    high_corr_pairs.append({
                        'å˜é‡1': correlation_matrix.columns[i],
                        'å˜é‡2': correlation_matrix.columns[j],
                        'ç›¸å…³ç³»æ•°': corr_value
                    })

        if high_corr_pairs:
            print("\né«˜ç›¸å…³æ€§å˜é‡å¯¹ (|r| > 0.7):")
            for pair in high_corr_pairs:
                print(f"{pair['å˜é‡1']} - {pair['å˜é‡2']}: {pair['ç›¸å…³ç³»æ•°']:.3f}")

    def distribution_analysis(self):
        """åˆ†å¸ƒåˆ†æ"""
        print("=== åˆ†å¸ƒåˆ†æ ===")

# æ•°å€¼å‹å˜é‡åˆ†å¸ƒ
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.ravel()

        for i, col in enumerate(self.numeric_columns[:6]):
# ç›´æ–¹å›¾
            axes[i].hist(self.data[col], bins=30, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'{col} åˆ†å¸ƒ')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('é¢‘æ•°')

        plt.tight_layout()
        plt.show()

# æ­£æ€æ€§æ£€éªŒ
        print("\næ­£æ€æ€§æ£€éªŒ (Shapiro-Wilk):")
        for col in self.numeric_columns:
            stat, p_value = stats.shapiro(self.data[col])
            print(f"{col}: ç»Ÿè®¡é‡={stat:.4f}, på€¼={p_value:.4f}")

    def outlier_analysis(self):
        """å¼‚å¸¸å€¼åˆ†æ"""
        print("=== å¼‚å¸¸å€¼åˆ†æ ===")

        for col in self.numeric_columns:
            Q1 = self.data[col].quantile(0.25)
            Q3 = self.data[col].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = self.data[(self.data[col] < lower_bound) | (self.data[col] > upper_bound)]

            print(f"\n{col}:")
            print(f"  å¼‚å¸¸å€¼æ•°é‡: {len(outliers)}")
            print(f"  å¼‚å¸¸å€¼æ¯”ä¾‹: {len(outliers)/len(self.data)*100:.2f}%")
            print(f"  å¼‚å¸¸å€¼èŒƒå›´: [{outliers[col].min():.2f}, {outliers[col].max():.2f}]")

# ä½¿ç”¨ç¤ºä¾‹
print("=== æ¢ç´¢æ€§æ•°æ®åˆ†æç¤ºä¾‹ ===")
eda = ExploratoryDataAnalysis()
data = eda.load_sample_data()

eda.basic_statistics()
eda.missing_value_analysis()
eda.correlation_analysis()
eda.distribution_analysis()
eda.outlier_analysis()
```

## 2. æ—¶é—´åºåˆ—åˆ†æ

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

class TimeSeriesAnalysis:
    def __init__(self):
        self.data = None
        self.time_column = None

    def generate_sample_data(self, n_days=365):
        """ç”Ÿæˆç¤ºä¾‹æ—¶é—´åºåˆ—æ•°æ®"""
# ç”Ÿæˆæ—¥æœŸåºåˆ—
        start_date = datetime(2023, 1, 1)
        dates = [start_date + timedelta(days=i) for i in range(n_days)]

# ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        np.random.seed(42)
        trend = np.linspace(100, 150, n_days)  # è¶‹åŠ¿
        seasonal = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365)  # å­£èŠ‚æ€§
        noise = np.random.normal(0, 5, n_days)  # å™ªå£°

        values = trend + seasonal + noise

        self.data = pd.DataFrame({
            'date': dates,
            'value': values
        })
        self.time_column = 'date'

        return self.data

    def set_time_index(self):
        """è®¾ç½®æ—¶é—´ç´¢å¼•"""
        self.data = self.data.set_index(self.time_column)
        self.data = self.data.sort_index()

    def plot_time_series(self):
        """ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾"""
        plt.figure(figsize=(12, 6))
        plt.plot(self.data.index, self.data['value'])
        plt.title('æ—¶é—´åºåˆ—æ•°æ®')
        plt.xlabel('æ—¶é—´')
        plt.ylabel('æ•°å€¼')
        plt.grid(True)
        plt.show()

    def seasonal_decomposition(self):
        """å­£èŠ‚æ€§åˆ†è§£"""
        decomposition = seasonal_decompose(self.data['value'], period=365, extrapolate_trend='freq')

        fig, axes = plt.subplots(4, 1, figsize=(12, 10))

# åŸå§‹æ•°æ®
        axes[0].plot(self.data.index, self.data['value'])
        axes[0].set_title('åŸå§‹æ•°æ®')
        axes[0].grid(True)

# è¶‹åŠ¿
        axes[1].plot(self.data.index, decomposition.trend)
        axes[1].set_title('è¶‹åŠ¿')
        axes[1].grid(True)

# å­£èŠ‚æ€§
        axes[2].plot(self.data.index, decomposition.seasonal)
        axes[2].set_title('å­£èŠ‚æ€§')
        axes[2].grid(True)

# æ®‹å·®
        axes[3].plot(self.data.index, decomposition.resid)
        axes[3].set_title('æ®‹å·®')
        axes[3].grid(True)

        plt.tight_layout()
        plt.show()

    def stationarity_test(self):
        """å¹³ç¨³æ€§æ£€éªŒ"""
        print("=== ADFå¹³ç¨³æ€§æ£€éªŒ ===")
        result = adfuller(self.data['value'])

        print(f'ADFç»Ÿè®¡é‡: {result[0]:.4f}')
        print(f'på€¼: {result[1]:.4f}')
        print('ä¸´ç•Œå€¼:')
        for key, value in result[4].items():
            print(f'\t{key}: {value:.4f}')

        if result[1] <= 0.05:
            print("ç»“è®º: æ—¶é—´åºåˆ—æ˜¯å¹³ç¨³çš„ (på€¼ <= 0.05)")
        else:
            print("ç»“è®º: æ—¶é—´åºåˆ—æ˜¯éå¹³ç¨³çš„ (på€¼ > 0.05)")

    def autocorrelation_analysis(self):
        """è‡ªç›¸å…³åˆ†æ"""
        fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# è‡ªç›¸å…³å‡½æ•°
        plot_acf(self.data['value'], ax=axes[0], lags=40)
        axes[0].set_title('è‡ªç›¸å…³å‡½æ•°(ACF)')

# åè‡ªç›¸å…³å‡½æ•°
        plot_pacf(self.data['value'], ax=axes[1], lags=40)
        axes[1].set_title('åè‡ªç›¸å…³å‡½æ•°(PACF)')

        plt.tight_layout()
        plt.show()

    def rolling_statistics(self):
        """æ»šåŠ¨ç»Ÿè®¡"""
# è®¡ç®—æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®
        rolling_mean = self.data['value'].rolling(window=30).mean()
        rolling_std = self.data['value'].rolling(window=30).std()

        plt.figure(figsize=(12, 6))
        plt.plot(self.data.index, self.data['value'], label='åŸå§‹æ•°æ®', alpha=0.7)
        plt.plot(self.data.index, rolling_mean, label='30å¤©æ»šåŠ¨å‡å€¼', linewidth=2)
        plt.fill_between(self.data.index,
                        rolling_mean - 2*rolling_std,
                        rolling_mean + 2*rolling_std,
                        alpha=0.3, label='Â±2ÏƒåŒºé—´')
        plt.title('æ—¶é—´åºåˆ—æ»šåŠ¨ç»Ÿè®¡')
        plt.xlabel('æ—¶é—´')
        plt.ylabel('æ•°å€¼')
        plt.legend()
        plt.grid(True)
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
print("=== æ—¶é—´åºåˆ—åˆ†æç¤ºä¾‹ ===")
tsa = TimeSeriesAnalysis()
data = tsa.generate_sample_data()
tsa.set_time_index()

tsa.plot_time_series()
tsa.seasonal_decomposition()
tsa.stationarity_test()
tsa.autocorrelation_analysis()
tsa.rolling_statistics()
```

## 3. ETLæµç¨‹å®è·µæ¡ˆä¾‹

### 3.1. æ•°æ®æå–ä¸è½¬æ¢

```python
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import json
import xml.etree.ElementTree as ET
from datetime import datetime

class ETLProcessor:
    def __init__(self):
        self.extracted_data = {}
        self.transformed_data = None

    def extract_from_csv(self, file_path):
        """ä»CSVæ–‡ä»¶æå–æ•°æ®"""
        try:
            data = pd.read_csv(file_path)
            self.extracted_data['csv'] = data
            print(f"æˆåŠŸä»CSVæå–æ•°æ®: {data.shape}")
            return data
        except Exception as e:
            print(f"CSVæå–å¤±è´¥: {e}")
            return None

    def extract_from_json(self, file_path):
        """ä»JSONæ–‡ä»¶æå–æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

# å‡è®¾JSONæ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = pd.DataFrame([data])

            self.extracted_data['json'] = df
            print(f"æˆåŠŸä»JSONæå–æ•°æ®: {df.shape}")
            return df
        except Exception as e:
            print(f"JSONæå–å¤±è´¥: {e}")
            return None

    def extract_from_database(self, connection_string, query):
        """ä»æ•°æ®åº“æå–æ•°æ®"""
        try:
            engine = create_engine(connection_string)
            with engine.connect() as conn:
                data = pd.read_sql(text(query), conn)

            self.extracted_data['database'] = data
            print(f"æˆåŠŸä»æ•°æ®åº“æå–æ•°æ®: {data.shape}")
            return data
        except Exception as e:
            print(f"æ•°æ®åº“æå–å¤±è´¥: {e}")
            return None

    def data_cleaning(self, data):
        """æ•°æ®æ¸…æ´—"""
        print("=== æ•°æ®æ¸…æ´— ===")

# å¤„ç†ç¼ºå¤±å€¼
        print(f"æ¸…æ´—å‰ç¼ºå¤±å€¼:\n{data.isnull().sum()}")

# æ•°å€¼å‹åˆ—ç”¨ä¸­ä½æ•°å¡«å……
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if data[col].isnull().sum() > 0:
                median_value = data[col].median()
                data[col].fillna(median_value, inplace=True)
                print(f"åˆ— {col}: ç”¨ä¸­ä½æ•° {median_value:.2f} å¡«å……ç¼ºå¤±å€¼")

# åˆ†ç±»å‹åˆ—ç”¨ä¼—æ•°å¡«å……
        categorical_columns = data.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            if data[col].isnull().sum() > 0:
                mode_value = data[col].mode()[0]
                data[col].fillna(mode_value, inplace=True)
                print(f"åˆ— {col}: ç”¨ä¼—æ•° '{mode_value}' å¡«å……ç¼ºå¤±å€¼")

        print(f"æ¸…æ´—åç¼ºå¤±å€¼:\n{data.isnull().sum()}")
        return data

    def data_transformation(self, data):
        """æ•°æ®è½¬æ¢"""
        print("=== æ•°æ®è½¬æ¢ ===")

# æ•°æ®ç±»å‹è½¬æ¢
        print("æ•°æ®ç±»å‹è½¬æ¢:")
        for col in data.columns:
            if data[col].dtype == 'object':
# å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                try:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
                    if not data[col].isnull().all():
                        print(f"åˆ— {col}: object -> numeric")
                except:
                    pass

# åˆ›å»ºæ–°ç‰¹å¾
        print("\nç‰¹å¾å·¥ç¨‹:")

# å¦‚æœæœ‰æ—¥æœŸåˆ—ï¼Œæå–æ—¶é—´ç‰¹å¾
        date_columns = data.select_dtypes(include=['datetime64']).columns
        for col in date_columns:
            data[f'{col}_year'] = data[col].dt.year
            data[f'{col}_month'] = data[col].dt.month
            data[f'{col}_day'] = data[col].dt.day
            data[f'{col}_weekday'] = data[col].dt.weekday
            print(f"ä» {col} æå–æ—¶é—´ç‰¹å¾")

# æ•°å€¼å‹åˆ—çš„ç»Ÿè®¡ç‰¹å¾
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col not in [f'{dc}_year' for dc in date_columns]:
                data[f'{col}_log'] = np.log1p(data[col])
                data[f'{col}_squared'] = data[col] ** 2
                print(f"ä¸º {col} åˆ›å»ºå¯¹æ•°å’Œå¹³æ–¹ç‰¹å¾")

        return data

    def data_validation(self, data):
        """æ•°æ®éªŒè¯"""
        print("=== æ•°æ®éªŒè¯ ===")

# åŸºæœ¬éªŒè¯
        print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"æ•°æ®ç±»å‹:\n{data.dtypes}")

# æ•°å€¼å‹æ•°æ®éªŒè¯
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            print(f"\n{col}:")
            print(f"  èŒƒå›´: [{data[col].min():.2f}, {data[col].max():.2f}]")
            print(f"  å‡å€¼: {data[col].mean():.2f}")
            print(f"  æ ‡å‡†å·®: {data[col].std():.2f}")

# å¼‚å¸¸å€¼æ£€æµ‹
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = data[(data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)]
            print(f"  å¼‚å¸¸å€¼æ•°é‡: {len(outliers)}")

# åˆ†ç±»å‹æ•°æ®éªŒè¯
        categorical_columns = data.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            print(f"\n{col}:")
            print(f"  å”¯ä¸€å€¼æ•°é‡: {data[col].nunique()}")
            print(f"  å‰5ä¸ªå€¼: {data[col].value_counts().head().to_dict()}")

    def run_etl_pipeline(self, data_sources):
        """è¿è¡Œå®Œæ•´çš„ETLæµç¨‹"""
        print("=== ETLæµç¨‹å¼€å§‹ ===")

# 1. æå–é˜¶æ®µ
        print("\n1. æ•°æ®æå–é˜¶æ®µ")
        for source_type, source_info in data_sources.items():
            if source_type == 'csv':
                self.extract_from_csv(source_info['path'])
            elif source_type == 'json':
                self.extract_from_json(source_info['path'])
            elif source_type == 'database':
                self.extract_from_database(source_info['connection'], source_info['query'])

# 2. è½¬æ¢é˜¶æ®µ
        print("\n2. æ•°æ®è½¬æ¢é˜¶æ®µ")
        if self.extracted_data:
# åˆå¹¶æ‰€æœ‰æ•°æ®æº
            all_data = []
            for source_name, data in self.extracted_data.items():
                data['source'] = source_name
                all_data.append(data)

            self.transformed_data = pd.concat(all_data, ignore_index=True)

# æ•°æ®æ¸…æ´—
            self.transformed_data = self.data_cleaning(self.transformed_data)

# æ•°æ®è½¬æ¢
            self.transformed_data = self.data_transformation(self.transformed_data)

# æ•°æ®éªŒè¯
            self.data_validation(self.transformed_data)

        print("\n=== ETLæµç¨‹å®Œæˆ ===")
        return self.transformed_data

# ä½¿ç”¨ç¤ºä¾‹
print("=== ETLæµç¨‹ç¤ºä¾‹ ===")

# åˆ›å»ºç¤ºä¾‹æ•°æ®æº
data_sources = {
    'csv': {'path': 'sample_data.csv'},
    'json': {'path': 'sample_data.json'},
    'database': {
        'connection': 'sqlite:///sample.db',
        'query': 'SELECT * FROM users'
    }
}

etl = ETLProcessor()

# æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦æä¾›çœŸå®çš„æ•°æ®æ–‡ä»¶
# result = etl.run_etl_pipeline(data_sources)
print("ETLæµç¨‹ç¤ºä¾‹ä»£ç å·²å‡†å¤‡å°±ç»ªï¼Œéœ€è¦çœŸå®æ•°æ®æ–‡ä»¶æ‰èƒ½è¿è¡Œ")
```

## 4. æ•°æ®åŠ è½½ä¸å­˜å‚¨

```python
import pandas as pd
import sqlite3
import json
import pickle
from sqlalchemy import create_engine, text
import os

class DataLoader:
    def __init__(self):
        self.data = None

    def load_to_csv(self, data, file_path):
        """åŠ è½½æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            data.to_csv(file_path, index=False, encoding='utf-8')
            print(f"æ•°æ®æˆåŠŸä¿å­˜åˆ°CSV: {file_path}")
            return True
        except Exception as e:
            print(f"ä¿å­˜CSVå¤±è´¥: {e}")
            return False

    def load_to_json(self, data, file_path):
        """åŠ è½½æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            data.to_json(file_path, orient='records', force_ascii=False, indent=2)
            print(f"æ•°æ®æˆåŠŸä¿å­˜åˆ°JSON: {file_path}")
            return True
        except Exception as e:
            print(f"ä¿å­˜JSONå¤±è´¥: {e}")
            return False

    def load_to_database(self, data, connection_string, table_name):
        """åŠ è½½æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            engine = create_engine(connection_string)
            data.to_sql(table_name, engine, if_exists='replace', index=False)
            print(f"æ•°æ®æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“è¡¨: {table_name}")
            return True
        except Exception as e:
            print(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def load_to_parquet(self, data, file_path):
        """åŠ è½½æ•°æ®åˆ°Parquetæ–‡ä»¶"""
        try:
            data.to_parquet(file_path, index=False)
            print(f"æ•°æ®æˆåŠŸä¿å­˜åˆ°Parquet: {file_path}")
            return True
        except Exception as e:
            print(f"ä¿å­˜Parquetå¤±è´¥: {e}")
            return False

    def load_to_pickle(self, data, file_path):
        """åŠ è½½æ•°æ®åˆ°Pickleæ–‡ä»¶"""
        try:
            with open(file_path, 'wb') as f:
                pickle.dump(data, f)
            print(f"æ•°æ®æˆåŠŸä¿å­˜åˆ°Pickle: {file_path}")
            return True
        except Exception as e:
            print(f"ä¿å­˜Pickleå¤±è´¥: {e}")
            return False

class DataWarehouse:
    def __init__(self, db_path):
        self.db_path = db_path
        self.engine = create_engine(f'sqlite:///{db_path}')

    def create_tables(self):
        """åˆ›å»ºæ•°æ®ä»“åº“è¡¨ç»“æ„"""
        with self.engine.connect() as conn:
# åˆ›å»ºç»´åº¦è¡¨
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS dim_customers (
                    customer_id INTEGER PRIMARY KEY,
                    name TEXT,
                    email TEXT,
                    city TEXT,
                    created_date DATE
                )
            """))

            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS dim_products (
                    product_id INTEGER PRIMARY KEY,
                    name TEXT,
                    category TEXT,
                    price REAL
                )
            """))

# åˆ›å»ºäº‹å®è¡¨
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS fact_orders (
                    order_id INTEGER PRIMARY KEY,
                    customer_id INTEGER,
                    product_id INTEGER,
                    quantity INTEGER,
                    total_amount REAL,
                    order_date DATE,
                    FOREIGN KEY (customer_id) REFERENCES dim_customers(customer_id),
                    FOREIGN KEY (product_id) REFERENCES dim_products(product_id)
                )
            """))

            conn.commit()
            print("æ•°æ®ä»“åº“è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")

    def load_dimension_data(self, data, table_name):
        """åŠ è½½ç»´åº¦æ•°æ®"""
        try:
            data.to_sql(table_name, self.engine, if_exists='replace', index=False)
            print(f"ç»´åº¦è¡¨ {table_name} æ•°æ®åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"åŠ è½½ç»´åº¦è¡¨å¤±è´¥: {e}")
            return False

    def load_fact_data(self, data, table_name):
        """åŠ è½½äº‹å®æ•°æ®"""
        try:
            data.to_sql(table_name, self.engine, if_exists='append', index=False)
            print(f"äº‹å®è¡¨ {table_name} æ•°æ®åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"åŠ è½½äº‹å®è¡¨å¤±è´¥: {e}")
            return False

    def query_data(self, sql_query):
        """æŸ¥è¯¢æ•°æ®"""
        try:
            with self.engine.connect() as conn:
                result = pd.read_sql(text(sql_query), conn)
            return result
        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return None

# ä½¿ç”¨ç¤ºä¾‹
print("=== æ•°æ®åŠ è½½ä¸å­˜å‚¨ç¤ºä¾‹ ===")

# åˆ›å»ºç¤ºä¾‹æ•°æ®
sample_data = pd.DataFrame({
    'id': range(1, 101),
    'name': [f'ç”¨æˆ·{i}' for i in range(1, 101)],
    'age': np.random.randint(18, 65, 100),
    'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'], 100),
    'salary': np.random.normal(8000, 2000, 100)
})

# æ•°æ®åŠ è½½å™¨ç¤ºä¾‹
loader = DataLoader()

# ä¿å­˜åˆ°ä¸åŒæ ¼å¼
loader.load_to_csv(sample_data, 'output_data.csv')
loader.load_to_json(sample_data, 'output_data.json')
loader.load_to_parquet(sample_data, 'output_data.parquet')
loader.load_to_pickle(sample_data, 'output_data.pkl')

# æ•°æ®ä»“åº“ç¤ºä¾‹
warehouse = DataWarehouse('data_warehouse.db')
warehouse.create_tables()

# åŠ è½½ç»´åº¦æ•°æ®
customers_data = sample_data[['id', 'name', 'city']].copy()
customers_data.columns = ['customer_id', 'name', 'city']
warehouse.load_dimension_data(customers_data, 'dim_customers')

# æŸ¥è¯¢ç¤ºä¾‹
result = warehouse.query_data("SELECT * FROM dim_customers LIMIT 5")
print("\næŸ¥è¯¢ç»“æœ:")
print(result)
```

## 5. æ•°æ®æµå¤„ç†å®è·µæ¡ˆä¾‹

### 5.1. å®æ—¶æ•°æ®æµå¤„ç†

```python
import time
import threading
from queue import Queue
from collections import deque
import json

class DataStreamProcessor:
    def __init__(self, window_size=100):
        self.data_queue = Queue()
        self.processed_data = deque(maxlen=window_size)
        self.window_size = window_size
        self.running = False
        self.processors = []

    def add_processor(self, processor_func):
        """æ·»åŠ æ•°æ®å¤„ç†å™¨"""
        self.processors.append(processor_func)

    def start_processing(self):
        """å¼€å§‹å¤„ç†æ•°æ®æµ"""
        self.running = True
        self.processing_thread = threading.Thread(target=self._process_loop)
        self.processing_thread.start()
        print("æ•°æ®æµå¤„ç†å·²å¯åŠ¨")

    def stop_processing(self):
        """åœæ­¢å¤„ç†æ•°æ®æµ"""
        self.running = False
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join()
        print("æ•°æ®æµå¤„ç†å·²åœæ­¢")

    def _process_loop(self):
        """æ•°æ®å¤„ç†å¾ªç¯"""
        while self.running:
            try:
# éé˜»å¡æ–¹å¼è·å–æ•°æ®
                data = self.data_queue.get_nowait()
                processed_data = self._apply_processors(data)
                self.processed_data.append(processed_data)

# å®æ—¶ç»Ÿè®¡
                self._calculate_statistics()

            except:
                time.sleep(0.1)  # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜

    def _apply_processors(self, data):
        """åº”ç”¨æ‰€æœ‰å¤„ç†å™¨"""
        result = data
        for processor in self.processors:
            result = processor(result)
        return result

    def _calculate_statistics(self):
        """è®¡ç®—å®æ—¶ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.processed_data) > 0:
# è®¡ç®—æ»‘åŠ¨çª—å£ç»Ÿè®¡
            recent_data = list(self.processed_data)
            if len(recent_data) >= 10:  # è‡³å°‘10ä¸ªæ•°æ®ç‚¹
                values = [item.get('value', 0) for item in recent_data[-10:]]
                avg_value = sum(values) / len(values)
                max_value = max(values)
                min_value = min(values)

                print(f"å®æ—¶ç»Ÿè®¡ - å¹³å‡å€¼: {avg_value:.2f}, æœ€å¤§å€¼: {max_value:.2f}, æœ€å°å€¼: {min_value:.2f}")

    def add_data(self, data):
        """æ·»åŠ æ•°æ®åˆ°æµ"""
        self.data_queue.put(data)

class RealTimeAnalytics:
    def __init__(self):
        self.stream_processor = DataStreamProcessor(window_size=1000)
        self._setup_processors()

    def _setup_processors(self):
        """è®¾ç½®æ•°æ®å¤„ç†å™¨"""
# æ•°æ®æ¸…æ´—å¤„ç†å™¨
        def clean_data(data):
            if 'value' in data and isinstance(data['value'], (int, float)):
# å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
                if data['value'] > 1000 or data['value'] < 0:
                    data['value'] = 0
                data['cleaned'] = True
            return data

# æ•°æ®è½¬æ¢å¤„ç†å™¨
        def transform_data(data):
            if 'value' in data:
                data['value_squared'] = data['value'] ** 2
                data['value_log'] = np.log1p(data['value'])
            return data

# æ•°æ®èšåˆå¤„ç†å™¨
        def aggregate_data(data):
            data['timestamp'] = time.time()
            return data

        self.stream_processor.add_processor(clean_data)
        self.stream_processor.add_processor(transform_data)
        self.stream_processor.add_processor(aggregate_data)

    def start_analytics(self):
        """å¯åŠ¨å®æ—¶åˆ†æ"""
        self.stream_processor.start_processing()

    def stop_analytics(self):
        """åœæ­¢å®æ—¶åˆ†æ"""
        self.stream_processor.stop_processing()

    def simulate_data_stream(self, duration=60):
        """æ¨¡æ‹Ÿæ•°æ®æµ"""
        print(f"å¼€å§‹æ¨¡æ‹Ÿæ•°æ®æµï¼ŒæŒç»­{duration}ç§’...")

        start_time = time.time()
        while time.time() - start_time < duration:
# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            data = {
                'id': int(time.time() * 1000),
                'value': np.random.normal(100, 20),
                'category': np.random.choice(['A', 'B', 'C']),
                'source': 'sensor_1'
            }

            self.stream_processor.add_data(data)
            time.sleep(0.1)  # æ¯0.1ç§’ç”Ÿæˆä¸€ä¸ªæ•°æ®ç‚¹

        print("æ•°æ®æµæ¨¡æ‹Ÿç»“æŸ")

# ä½¿ç”¨ç¤ºä¾‹
print("=== å®æ—¶æ•°æ®æµå¤„ç†ç¤ºä¾‹ ===")

analytics = RealTimeAnalytics()
analytics.start_analytics()

# æ¨¡æ‹Ÿæ•°æ®æµ
analytics.simulate_data_stream(duration=10)

analytics.stop_analytics()
```

è¿™äº›å®è·µæ¡ˆä¾‹å±•ç¤ºäº†æ•°æ®åˆ†æä¸ETLä¸­çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨ï¼ŒåŒ…æ‹¬æ¢ç´¢æ€§æ•°æ®åˆ†æã€æ—¶é—´åºåˆ—åˆ†æã€ETLæµç¨‹ã€æ•°æ®åŠ è½½å­˜å‚¨å’Œå®æ—¶æ•°æ®æµå¤„ç†ç­‰ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½æä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œä½¿ç”¨ç¤ºä¾‹ï¼Œå¸®åŠ©ç†è§£è¿™äº›æ¦‚å¿µçš„å®é™…åº”ç”¨ã€‚
