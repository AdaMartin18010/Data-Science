# 3.5.1 æ•°æ®åˆ†æåŸºç¡€ç†è®º

## 1. ğŸ“‹ æ¦‚è¿°

æ•°æ®åˆ†ææ˜¯æ•°æ®ç§‘å­¦çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œæ”¶é›†ã€æ¸…æ´—ã€æ¢ç´¢ã€å»ºæ¨¡å’Œè§£é‡Šï¼Œä»¥å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼ã€è¶‹åŠ¿å’Œæ´å¯Ÿã€‚

## 2. ï¸ ç†è®ºæ¡†æ¶

### 2.1. æ•°æ®åˆ†æçš„å®šä¹‰ä¸æµç¨‹

æ•°æ®åˆ†ææ˜¯æŒ‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ¸…æ´—ã€è½¬æ¢ã€å»ºæ¨¡ã€è§£é‡Šçš„è¿‡ç¨‹ï¼Œæ—¨åœ¨å‘ç°æœ‰ç”¨ä¿¡æ¯ã€å¾—å‡ºç»“è®ºå¹¶æ”¯æŒå†³ç­–ã€‚

#### 2.1.1. æ ¸å¿ƒæµç¨‹

```mermaid
flowchart TD
    A[æ•°æ®æ”¶é›†] --> B[æ•°æ®æ¸…æ´—]
    B --> C[æ•°æ®æ¢ç´¢æ€§åˆ†æ]
    C --> D[ç‰¹å¾å·¥ç¨‹]
    D --> E[å»ºæ¨¡ä¸è¯„ä¼°]
    E --> F[ç»“æœè§£é‡Šä¸å¯è§†åŒ–]
    F --> G[å†³ç­–æ”¯æŒ]
```

#### 2.1.2. è¯¦ç»†æ­¥éª¤è¯´æ˜

1. **æ•°æ®æ”¶é›† (Data Collection)**
   - ç¡®å®šæ•°æ®éœ€æ±‚
   - é€‰æ‹©æ•°æ®æº
   - è®¾è®¡æ•°æ®æ”¶é›†æ–¹æ¡ˆ

2. **æ•°æ®æ¸…æ´— (Data Cleaning)**
   - å¤„ç†ç¼ºå¤±å€¼
   - è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸å€¼
   - æ•°æ®æ ¼å¼æ ‡å‡†åŒ–

3. **æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)**
   - æ•°æ®åˆ†å¸ƒåˆ†æ
   - ç›¸å…³æ€§åˆ†æ
   - å¼‚å¸¸æ£€æµ‹

4. **ç‰¹å¾å·¥ç¨‹ (Feature Engineering)**
   - ç‰¹å¾é€‰æ‹©
   - ç‰¹å¾å˜æ¢
   - ç‰¹å¾ç»„åˆ

5. **å»ºæ¨¡ä¸è¯„ä¼° (Modeling & Evaluation)**
   - æ¨¡å‹é€‰æ‹©
   - å‚æ•°è°ƒä¼˜
   - äº¤å‰éªŒè¯

6. **ç»“æœè§£é‡Šä¸å¯è§†åŒ– (Interpretation & Visualization)**
   - æ¨¡å‹è§£é‡Š
   - ç»“æœå¯è§†åŒ–
   - æ´å¯Ÿæ€»ç»“

### 2.2. ä¸»è¦åˆ†ææ–¹æ³•

#### 2.2.1. æè¿°æ€§ç»Ÿè®¡ (Descriptive Statistics)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class DescriptiveAnalysis:
    def __init__(self, data):
        self.data = data

    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡æè¿°"""
        return {
            'count': self.data.count(),
            'mean': self.data.mean(),
            'std': self.data.std(),
            'min': self.data.min(),
            '25%': self.data.quantile(0.25),
            '50%': self.data.quantile(0.50),
            '75%': self.data.quantile(0.75),
            'max': self.data.max()
        }

    def distribution_analysis(self, column):
        """åˆ†å¸ƒåˆ†æ"""
        plt.figure(figsize=(12, 4))

# ç›´æ–¹å›¾
        plt.subplot(1, 3, 1)
        plt.hist(self.data[column], bins=30, alpha=0.7)
        plt.title(f'{column} åˆ†å¸ƒç›´æ–¹å›¾')

# ç®±çº¿å›¾
        plt.subplot(1, 3, 2)
        plt.boxplot(self.data[column])
        plt.title(f'{column} ç®±çº¿å›¾')

# Q-Qå›¾
        plt.subplot(1, 3, 3)
        from scipy import stats
        stats.probplot(self.data[column], dist="norm", plot=plt)
        plt.title(f'{column} Q-Qå›¾')

        plt.tight_layout()
        plt.show()

    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        correlation_matrix = self.data.corr()

        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.show()

        return correlation_matrix

# ä½¿ç”¨ç¤ºä¾‹
df = pd.read_csv('data.csv')
analyzer = DescriptiveAnalysis(df)
stats = analyzer.basic_statistics()
analyzer.distribution_analysis('target_column')
corr_matrix = analyzer.correlation_analysis()
```

## 3. æ¨æ–­æ€§ç»Ÿè®¡ (Inferential Statistics)

```python
from scipy import stats
from scipy.stats import ttest_ind, chi2_contingency, pearsonr

class InferentialAnalysis:
    def __init__(self, data):
        self.data = data

    def hypothesis_testing(self, group1, group2, test_type='t_test'):
        """å‡è®¾æ£€éªŒ"""
        if test_type == 't_test':
# ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ
            statistic, p_value = ttest_ind(group1, group2)
            return {
                'test_type': 'Independent t-test',
                'statistic': statistic,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        elif test_type == 'chi_square':
# å¡æ–¹æ£€éªŒ
            contingency_table = pd.crosstab(group1, group2)
            statistic, p_value, dof, expected = chi2_contingency(contingency_table)
            return {
                'test_type': 'Chi-square test',
                'statistic': statistic,
                'p_value': p_value,
                'significant': p_value < 0.05
            }

    def confidence_interval(self, data, confidence=0.95):
        """ç½®ä¿¡åŒºé—´è®¡ç®—"""
        mean = np.mean(data)
        std_err = stats.sem(data)
        ci = stats.t.interval(confidence, len(data)-1, loc=mean, scale=std_err)
        return {
            'mean': mean,
            'confidence_interval': ci,
            'confidence_level': confidence
        }

# ä½¿ç”¨ç¤ºä¾‹
infer_analyzer = InferentialAnalysis(df)
t_test_result = infer_analyzer.hypothesis_testing(df['group1'], df['group2'])
ci_result = infer_analyzer.confidence_interval(df['target_column'])
```

## 4. æ•°æ®åˆ†ææ–¹æ³•è®º

### 4.1. CRISP-DM æ–¹æ³•è®º

```python
class CRISPDM:
    """CRISP-DM æ•°æ®åˆ†ææ–¹æ³•è®ºå®ç°"""

    def __init__(self):
        self.current_phase = None
        self.results = {}

    def business_understanding(self, business_objectives, success_criteria):
        """ä¸šåŠ¡ç†è§£é˜¶æ®µ"""
        self.current_phase = "Business Understanding"
        self.results['business_objectives'] = business_objectives
        self.results['success_criteria'] = success_criteria
        print(f"é˜¶æ®µ: {self.current_phase}")
        print(f"ä¸šåŠ¡ç›®æ ‡: {business_objectives}")
        print(f"æˆåŠŸæ ‡å‡†: {success_criteria}")

    def data_understanding(self, data_sources, data_quality_report):
        """æ•°æ®ç†è§£é˜¶æ®µ"""
        self.current_phase = "Data Understanding"
        self.results['data_sources'] = data_sources
        self.results['data_quality'] = data_quality_report
        print(f"é˜¶æ®µ: {self.current_phase}")
        print(f"æ•°æ®æº: {data_sources}")
        print(f"æ•°æ®è´¨é‡æŠ¥å‘Š: {data_quality_report}")

    def data_preparation(self, data_cleaning_steps, feature_engineering):
        """æ•°æ®å‡†å¤‡é˜¶æ®µ"""
        self.current_phase = "Data Preparation"
        self.results['cleaning_steps'] = data_cleaning_steps
        self.results['feature_engineering'] = feature_engineering
        print(f"é˜¶æ®µ: {self.current_phase}")
        print(f"æ•°æ®æ¸…æ´—æ­¥éª¤: {data_cleaning_steps}")
        print(f"ç‰¹å¾å·¥ç¨‹: {feature_engineering}")

    def modeling(self, model_selection, model_training):
        """å»ºæ¨¡é˜¶æ®µ"""
        self.current_phase = "Modeling"
        self.results['model_selection'] = model_selection
        self.results['model_training'] = model_training
        print(f"é˜¶æ®µ: {self.current_phase}")
        print(f"æ¨¡å‹é€‰æ‹©: {model_selection}")
        print(f"æ¨¡å‹è®­ç»ƒ: {model_training}")

    def evaluation(self, model_performance, business_impact):
        """è¯„ä¼°é˜¶æ®µ"""
        self.current_phase = "Evaluation"
        self.results['model_performance'] = model_performance
        self.results['business_impact'] = business_impact
        print(f"é˜¶æ®µ: {self.current_phase}")
        print(f"æ¨¡å‹æ€§èƒ½: {model_performance}")
        print(f"ä¸šåŠ¡å½±å“: {business_impact}")

    def deployment(self, deployment_plan, monitoring_strategy):
        """éƒ¨ç½²é˜¶æ®µ"""
        self.current_phase = "Deployment"
        self.results['deployment_plan'] = deployment_plan
        self.results['monitoring_strategy'] = monitoring_strategy
        print(f"é˜¶æ®µ: {self.current_phase}")
        print(f"éƒ¨ç½²è®¡åˆ’: {deployment_plan}")
        print(f"ç›‘æ§ç­–ç•¥: {monitoring_strategy}")

# ä½¿ç”¨ç¤ºä¾‹
crisp_dm = CRISPDM()
crisp_dm.business_understanding(
    business_objectives="æé«˜å®¢æˆ·æ»¡æ„åº¦",
    success_criteria="æ»¡æ„åº¦æå‡10%"
)
crisp_dm.data_understanding(
    data_sources=["CRMç³»ç»Ÿ", "å®¢æœç³»ç»Ÿ"],
    data_quality_report="æ•°æ®å®Œæ•´æ€§95%"
)
```

## 5. æ•°æ®åˆ†ææŠ€æœ¯æ ˆ

### 5.1. æ•°æ®å¤„ç†æŠ€æœ¯æ ˆ

```python
class DataAnalysisTechStack:
    """æ•°æ®åˆ†ææŠ€æœ¯æ ˆ"""

    def __init__(self):
        self.tech_stack = {
            'data_manipulation': {
                'pandas': 'æ•°æ®æ“ä½œå’Œåˆ†æ',
                'numpy': 'æ•°å€¼è®¡ç®—',
                'polars': 'é«˜æ€§èƒ½æ•°æ®å¤„ç†'
            },
            'visualization': {
                'matplotlib': 'åŸºç¡€å¯è§†åŒ–',
                'seaborn': 'ç»Ÿè®¡å¯è§†åŒ–',
                'plotly': 'äº¤äº’å¼å¯è§†åŒ–',
                'bokeh': 'Webå¯è§†åŒ–'
            },
            'statistical_analysis': {
                'scipy': 'ç§‘å­¦è®¡ç®—',
                'statsmodels': 'ç»Ÿè®¡å»ºæ¨¡',
                'pingouin': 'ç»Ÿè®¡æµ‹è¯•'
            },
            'machine_learning': {
                'scikit-learn': 'æœºå™¨å­¦ä¹ ',
                'xgboost': 'æ¢¯åº¦æå‡',
                'lightgbm': 'è½»é‡çº§æ¢¯åº¦æå‡'
            },
            'deep_learning': {
                'tensorflow': 'æ·±åº¦å­¦ä¹ æ¡†æ¶',
                'pytorch': 'æ·±åº¦å­¦ä¹ æ¡†æ¶',
                'keras': 'é«˜çº§ç¥ç»ç½‘ç»œAPI'
            }
        }

    def get_recommended_stack(self, analysis_type):
        """æ ¹æ®åˆ†æç±»å‹æ¨èæŠ€æœ¯æ ˆ"""
        recommendations = {
            'exploratory': ['pandas', 'numpy', 'matplotlib', 'seaborn'],
            'statistical': ['pandas', 'scipy', 'statsmodels', 'matplotlib'],
            'machine_learning': ['pandas', 'scikit-learn', 'matplotlib', 'seaborn'],
            'deep_learning': ['pandas', 'tensorflow', 'matplotlib', 'plotly'],
            'big_data': ['polars', 'dask', 'vaex', 'plotly']
        }
        return recommendations.get(analysis_type, [])

# ä½¿ç”¨ç¤ºä¾‹
tech_stack = DataAnalysisTechStack()
ml_tools = tech_stack.get_recommended_stack('machine_learning')
```

## 6. æ•°æ®è´¨é‡è¯„ä¼°æ¡†æ¶

```python
class DataQualityFramework:
    """æ•°æ®è´¨é‡è¯„ä¼°æ¡†æ¶"""

    def __init__(self, data):
        self.data = data
        self.quality_metrics = {}

    def completeness_check(self):
        """å®Œæ•´æ€§æ£€æŸ¥"""
        missing_data = self.data.isnull().sum()
        completeness_rate = 1 - (missing_data / len(self.data))

        self.quality_metrics['completeness'] = {
            'missing_counts': missing_data.to_dict(),
            'completeness_rate': completeness_rate.to_dict(),
            'overall_completeness': completeness_rate.mean()
        }
        return self.quality_metrics['completeness']

    def accuracy_check(self, validation_rules):
        """å‡†ç¡®æ€§æ£€æŸ¥"""
        accuracy_results = {}
        for column, rules in validation_rules.items():
            if column in self.data.columns:
                violations = 0
                for rule in rules:
                    violations += (~rule(self.data[column])).sum()
                accuracy_results[column] = 1 - (violations / len(self.data))

        self.quality_metrics['accuracy'] = accuracy_results
        return accuracy_results

    def generate_quality_report(self):
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        report = {
            'summary': {
                'total_records': len(self.data),
                'total_columns': len(self.data.columns),
                'overall_quality_score': self._calculate_overall_score()
            },
            'detailed_metrics': self.quality_metrics
        }
        return report

    def _calculate_overall_score(self):
        """è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°"""
        scores = []
        if 'completeness' in self.quality_metrics:
            scores.append(self.quality_metrics['completeness']['overall_completeness'])
        if 'accuracy' in self.quality_metrics:
            scores.append(np.mean(list(self.quality_metrics['accuracy'].values())))

        return np.mean(scores) if scores else 0

# ä½¿ç”¨ç¤ºä¾‹
quality_framework = DataQualityFramework(df)
completeness = quality_framework.completeness_check()

# å®šä¹‰éªŒè¯è§„åˆ™
validation_rules = {
    'age': [lambda x: x >= 0, lambda x: x <= 120],
    'salary': [lambda x: x >= 0]
}
accuracy = quality_framework.accuracy_check(validation_rules)
quality_report = quality_framework.generate_quality_report()
```

## 7. å®é™…åº”ç”¨æ¡ˆä¾‹

### 7.1. è¡Œä¸šæ¡ˆä¾‹ä¸å¤šè¡¨å¾

#### 7.1.1. é‡‘èè¡Œä¸šæ¡ˆä¾‹

```python
class FinancialDataAnalysis:
    """é‡‘èæ•°æ®åˆ†ææ¡ˆä¾‹"""

    def __init__(self, transaction_data, customer_data):
        self.transaction_data = transaction_data
        self.customer_data = customer_data

    def credit_scoring_analysis(self):
        """ä¿¡ç”¨è¯„åˆ†åˆ†æ"""
# ç‰¹å¾å·¥ç¨‹
        features = self._engineer_credit_features()

# ç»Ÿè®¡åˆ†æ
        credit_stats = self._analyze_credit_distribution()

# é£é™©å»ºæ¨¡
        risk_model = self._build_risk_model(features)

        return {
            'features': features,
            'statistics': credit_stats,
            'risk_model': risk_model
        }

    def _engineer_credit_features(self):
        """ä¿¡ç”¨ç‰¹å¾å·¥ç¨‹"""
        features = {}

# äº¤æ˜“é¢‘ç‡
        features['transaction_frequency'] = self.transaction_data.groupby('customer_id').size()

# å¹³å‡äº¤æ˜“é‡‘é¢
        features['avg_transaction_amount'] = self.transaction_data.groupby('customer_id')['amount'].mean()

# äº¤æ˜“é‡‘é¢æ ‡å‡†å·®
        features['transaction_amount_std'] = self.transaction_data.groupby('customer_id')['amount'].std()

        return pd.DataFrame(features)

    def _analyze_credit_distribution(self):
        """ä¿¡ç”¨åˆ†å¸ƒåˆ†æ"""
        return {
            'mean_credit_score': self.customer_data['credit_score'].mean(),
            'credit_score_distribution': self.customer_data['credit_score'].value_counts(),
            'credit_score_percentiles': self.customer_data['credit_score'].quantile([0.25, 0.5, 0.75])
        }

    def _build_risk_model(self, features):
        """æ„å»ºé£é™©æ¨¡å‹"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split

# å‡†å¤‡è®­ç»ƒæ•°æ®
        X = features.fillna(0)
        y = self.customer_data['default_risk']

# åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)

        return {
            'model': model,
            'feature_importance': model.feature_importances_,
            'test_score': model.score(X_test, y_test)
        }
```

## 8. åŒ»ç–—è¡Œä¸šæ¡ˆä¾‹

```python
class MedicalDataAnalysis:
    """åŒ»ç–—æ•°æ®åˆ†ææ¡ˆä¾‹"""

    def __init__(self, patient_data, treatment_data):
        self.patient_data = patient_data
        self.treatment_data = treatment_data

    def patient_outcome_analysis(self):
        """æ‚£è€…é¢„ååˆ†æ"""
# ç”Ÿå­˜åˆ†æ
        survival_analysis = self._perform_survival_analysis()

# æ²»ç–—æ•ˆæœåˆ†æ
        treatment_effectiveness = self._analyze_treatment_effectiveness()

        return {
            'survival_analysis': survival_analysis,
            'treatment_effectiveness': treatment_effectiveness
        }

    def _perform_survival_analysis(self):
        """æ‰§è¡Œç”Ÿå­˜åˆ†æ"""
        from lifelines import KaplanMeierFitter

# Kaplan-Meierç”Ÿå­˜åˆ†æ
        kmf = KaplanMeierFitter()
        kmf.fit(self.patient_data['survival_time'], self.patient_data['event'])

        return {
            'kaplan_meier': kmf,
            'median_survival': kmf.median_survival_time_
        }

    def _analyze_treatment_effectiveness(self):
        """åˆ†ææ²»ç–—æ•ˆæœ"""
# æ²»ç–—å‰åå¯¹æ¯”
        before_after = self.patient_data.groupby('treatment_group').agg({
            'symptom_score': ['mean', 'std'],
            'quality_of_life': ['mean', 'std']
        })

# ç»Ÿè®¡æ£€éªŒ
        from scipy.stats import ttest_ind
        treatment_effect = ttest_ind(
            self.patient_data[self.patient_data['treatment_group'] == 'treatment']['symptom_score'],
            self.patient_data[self.patient_data['treatment_group'] == 'control']['symptom_score']
        )

        return {
            'before_after_comparison': before_after,
            'treatment_effect_test': treatment_effect
        }
```

## 9. é›¶å”®è¡Œä¸šæ¡ˆä¾‹

```python
class RetailDataAnalysis:
    """é›¶å”®æ•°æ®åˆ†ææ¡ˆä¾‹"""

    def __init__(self, sales_data, customer_data, product_data):
        self.sales_data = sales_data
        self.customer_data = customer_data
        self.product_data = product_data

    def sales_analysis(self):
        """é”€å”®åˆ†æ"""
# é”€å”®è¶‹åŠ¿åˆ†æ
        sales_trends = self._analyze_sales_trends()

# äº§å“è¡¨ç°åˆ†æ
        product_performance = self._analyze_product_performance()

        return {
            'sales_trends': sales_trends,
            'product_performance': product_performance
        }

    def _analyze_sales_trends(self):
        """åˆ†æé”€å”®è¶‹åŠ¿"""
# æ—¶é—´åºåˆ—åˆ†æ
        sales_by_date = self.sales_data.groupby('date')['amount'].sum()

# å­£èŠ‚æ€§åˆ†æ
        from statsmodels.tsa.seasonal import seasonal_decompose
        decomposition = seasonal_decompose(sales_by_date, period=30)

        return {
            'sales_timeseries': sales_by_date,
            'seasonal_decomposition': decomposition
        }

    def _analyze_product_performance(self):
        """åˆ†æäº§å“è¡¨ç°"""
        product_metrics = self.sales_data.groupby('product_id').agg({
            'amount': ['sum', 'mean', 'count'],
            'quantity': ['sum', 'mean']
        }).round(2)

# äº§å“æ’å
        product_ranking = product_metrics['amount']['sum'].sort_values(ascending=False)

# ABCåˆ†æ
        cumulative_percentage = product_ranking.cumsum() / product_ranking.sum() * 100
        abc_classification = pd.cut(cumulative_percentage,
                                  bins=[0, 80, 95, 100],
                                  labels=['A', 'B', 'C'])

        return {
            'product_metrics': product_metrics,
            'product_ranking': product_ranking,
            'abc_classification': abc_classification
        }
```

## 10. å¤šè¡¨å¾ç¤ºä¾‹

### 10.1. å¯è§†åŒ–è¡¨å¾

```python
class DataVisualization:
    """æ•°æ®å¯è§†åŒ–å·¥å…·ç±»"""

    def __init__(self):
        self.colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

    def create_comprehensive_dashboard(self, data, analysis_results):
        """åˆ›å»ºç»¼åˆåˆ†æä»ªè¡¨æ¿"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. æ•°æ®åˆ†å¸ƒå›¾
        self._plot_distribution(axes[0, 0], data)

# 2. ç›¸å…³æ€§çƒ­åŠ›å›¾
        self._plot_correlation_heatmap(axes[0, 1], data)

# 3. æ—¶é—´åºåˆ—å›¾
        self._plot_time_series(axes[0, 2], data)

# 4. ç®±çº¿å›¾
        self._plot_boxplot(axes[1, 0], data)

# 5. æ•£ç‚¹å›¾
        self._plot_scatter(axes[1, 1], data)

# 6. ç»Ÿè®¡æ‘˜è¦
        self._plot_statistics_summary(axes[1, 2], analysis_results)

        plt.tight_layout()
        plt.show()

    def _plot_distribution(self, ax, data):
        """ç»˜åˆ¶åˆ†å¸ƒå›¾"""
        for column in data.select_dtypes(include=[np.number]).columns[:3]:
            ax.hist(data[column].dropna(), alpha=0.7, label=column, bins=20)
        ax.set_title('æ•°æ®åˆ†å¸ƒ')
        ax.legend()
        ax.grid(True, alpha=0.3)

    def _plot_correlation_heatmap(self, ax, data):
        """ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        corr_matrix = data.corr()
        im = ax.imshow(corr_matrix, cmap='coolwarm', aspect='auto')
        ax.set_xticks(range(len(corr_matrix.columns)))
        ax.set_yticks(range(len(corr_matrix.columns)))
        ax.set_xticklabels(corr_matrix.columns, rotation=45)
        ax.set_yticklabels(corr_matrix.columns)
        ax.set_title('ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.colorbar(im, ax=ax)

    def _plot_time_series(self, ax, data):
        """ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾"""
        if 'date' in data.columns:
            time_data = data.groupby('date').sum()
            ax.plot(time_data.index, time_data.iloc[:, 0])
            ax.set_title('æ—¶é—´åºåˆ—è¶‹åŠ¿')
            ax.grid(True, alpha=0.3)

    def _plot_boxplot(self, ax, data):
        """ç»˜åˆ¶ç®±çº¿å›¾"""
        numeric_data = data.select_dtypes(include=[np.number])
        if len(numeric_data.columns) > 0:
            ax.boxplot([numeric_data[col].dropna() for col in numeric_data.columns[:5]])
            ax.set_xticklabels(numeric_data.columns[:5], rotation=45)
            ax.set_title('ç®±çº¿å›¾')
            ax.grid(True, alpha=0.3)

    def _plot_scatter(self, ax, data):
        """ç»˜åˆ¶æ•£ç‚¹å›¾"""
        numeric_data = data.select_dtypes(include=[np.number])
        if len(numeric_data.columns) >= 2:
            ax.scatter(numeric_data.iloc[:, 0], numeric_data.iloc[:, 1], alpha=0.6)
            ax.set_xlabel(numeric_data.columns[0])
            ax.set_ylabel(numeric_data.columns[1])
            ax.set_title('æ•£ç‚¹å›¾')
            ax.grid(True, alpha=0.3)

    def _plot_statistics_summary(self, ax, results):
        """ç»˜åˆ¶ç»Ÿè®¡æ‘˜è¦"""
        ax.axis('off')
        summary_text = "ç»Ÿè®¡æ‘˜è¦:\n"
        for key, value in results.items():
            summary_text += f"{key}: {value}\n"
        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,
                fontsize=10, verticalalignment='top')

# ä½¿ç”¨ç¤ºä¾‹
visualizer = DataVisualization()
# visualizer.create_comprehensive_dashboard(df, analysis_results)
```

## 11. ğŸ”— ç›¸å…³å†…å®¹è·³è½¬

- è¯¦è§ [3.5.2-ETLç†è®ºä¸å®è·µ.md](./3.5.2-ETLç†è®ºä¸å®è·µ.md)
- è¯¦è§ [3.5.3-æ•°æ®å¯è§†åŒ–æ–¹æ³•ä¸å·¥å…·.md](./3.5.3-æ•°æ®å¯è§†åŒ–æ–¹æ³•ä¸å·¥å…·.md)
- è¯¦è§ [3.5.4-ä¸“ä¸šæ•°æ®åˆ†æç®—æ³•.md](./3.5.4-ä¸“ä¸šæ•°æ®åˆ†æç®—æ³•.md)
- è¯¦è§ [3.5.9-æ•°æ®åˆ†æä¸ETLå®è·µæ¡ˆä¾‹.md](./3.5.9-æ•°æ®åˆ†æä¸ETLå®è·µæ¡ˆä¾‹.md)

## 12. æ€»ç»“

æ•°æ®åˆ†æåŸºç¡€ç†è®ºä¸ºæ•´ä¸ªæ•°æ®ç§‘å­¦æµç¨‹æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„åˆ†ææµç¨‹ã€ç§‘å­¦çš„æ–¹æ³•è®ºå’Œä¸°å¯Œçš„æŠ€æœ¯æ ˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»åŸå§‹æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä¸ºä¸šåŠ¡å†³ç­–æä¾›ç§‘å­¦ä¾æ®ã€‚

### 12.1. å…³é”®è¦ç‚¹

1. **ç³»ç»Ÿæ€§æµç¨‹**ï¼šä»æ•°æ®æ”¶é›†åˆ°ç»“æœè§£é‡Šçš„å®Œæ•´åˆ†ææµç¨‹
2. **ç§‘å­¦æ–¹æ³•è®º**ï¼šCRISP-DMã€KDDç­‰æˆç†Ÿçš„åˆ†ææ–¹æ³•è®º
3. **æŠ€æœ¯æ ˆæ”¯æŒ**ï¼šä¸°å¯Œçš„æ•°æ®åˆ†æå·¥å…·å’ŒæŠ€æœ¯
4. **è´¨é‡ä¿è¯**ï¼šå®Œå–„çš„æ•°æ®è´¨é‡è¯„ä¼°æ¡†æ¶
5. **å®é™…åº”ç”¨**ï¼šå„è¡Œä¸šçš„å®é™…åº”ç”¨æ¡ˆä¾‹

### 12.2. å­¦ä¹ å»ºè®®

1. **ç†è®ºåŸºç¡€**ï¼šæ·±å…¥ç†è§£æ•°æ®åˆ†æçš„åŸºæœ¬æ¦‚å¿µå’Œæ–¹æ³•
2. **å·¥å…·æŒæ¡**ï¼šç†Ÿç»ƒä½¿ç”¨å„ç§æ•°æ®åˆ†æå·¥å…·å’Œåº“
3. **å®è·µåº”ç”¨**ï¼šé€šè¿‡å®é™…é¡¹ç›®ç§¯ç´¯ç»éªŒ
4. **æŒç»­å­¦ä¹ **ï¼šå…³æ³¨æ–°æŠ€æœ¯å’Œæ–¹æ³•çš„å‘å±•
