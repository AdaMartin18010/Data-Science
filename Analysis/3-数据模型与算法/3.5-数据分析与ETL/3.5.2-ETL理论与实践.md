# 3.5.2 ETLç†è®ºä¸å®è·µ

## ğŸ“‘ ç›®å½•

- [3.5.2 ETLç†è®ºä¸å®è·µ](#352-etlç†è®ºä¸å®è·µ)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1. ETLå®šä¹‰](#11-etlå®šä¹‰)
    - [1.2. ETLä½œç”¨](#12-etlä½œç”¨)
    - [1.3. ETLå†å²](#13-etlå†å²)
  - [2. ETLæ ¸å¿ƒæµç¨‹](#2-etlæ ¸å¿ƒæµç¨‹)
    - [2.1. æ•°æ®æŠ½å–ï¼ˆExtractï¼‰](#21-æ•°æ®æŠ½å–extract)
      - [2.1.1. æŠ½å–ç­–ç•¥](#211-æŠ½å–ç­–ç•¥)
      - [2.1.2. æ•°æ®æºç±»å‹](#212-æ•°æ®æºç±»å‹)
    - [2.2. æ•°æ®è½¬æ¢ï¼ˆTransformï¼‰](#22-æ•°æ®è½¬æ¢transform)
      - [2.2.1. æ•°æ®æ¸…æ´—](#221-æ•°æ®æ¸…æ´—)
      - [2.2.2. æ•°æ®è½¬æ¢](#222-æ•°æ®è½¬æ¢)
    - [2.3. æ•°æ®åŠ è½½ï¼ˆLoadï¼‰](#23-æ•°æ®åŠ è½½load)
      - [2.3.1. åŠ è½½ç­–ç•¥](#231-åŠ è½½ç­–ç•¥)
      - [2.3.2. æ‰¹é‡åŠ è½½](#232-æ‰¹é‡åŠ è½½)
  - [3. ETLæ¨¡å¼ä¸æ–¹æ³•](#3-etlæ¨¡å¼ä¸æ–¹æ³•)
    - [3.1. æ‰¹å¤„ç†ETL](#31-æ‰¹å¤„ç†etl)
      - [3.1.1. æ‰¹å¤„ç†ç‰¹ç‚¹](#311-æ‰¹å¤„ç†ç‰¹ç‚¹)
      - [3.1.2. æ‰¹å¤„ç†å®ç°](#312-æ‰¹å¤„ç†å®ç°)
    - [3.2. æµå¤„ç†ETL](#32-æµå¤„ç†etl)
      - [3.2.1. æµå¤„ç†ç‰¹ç‚¹](#321-æµå¤„ç†ç‰¹ç‚¹)
      - [3.2.2. æµå¤„ç†å®ç°](#322-æµå¤„ç†å®ç°)
    - [3.3. å®æ—¶ETL](#33-å®æ—¶etl)
      - [3.3.1. å®æ—¶ETLç‰¹ç‚¹](#331-å®æ—¶etlç‰¹ç‚¹)
  - [4. ETLå·¥å…·ä¸æ¡†æ¶](#4-etlå·¥å…·ä¸æ¡†æ¶)
    - [4.1. å¼€æºETLå·¥å…·](#41-å¼€æºetlå·¥å…·)
      - [4.1.1. Apache Airflow](#411-apache-airflow)
      - [4.1.2. Apache Spark](#412-apache-spark)
    - [4.2. å•†ä¸šETLå·¥å…·](#42-å•†ä¸šetlå·¥å…·)
      - [4.2.1. Informatica](#421-informatica)
      - [4.2.2. Talend](#422-talend)
    - [4.3. å¤§æ•°æ®ETLæ¡†æ¶](#43-å¤§æ•°æ®etlæ¡†æ¶)
      - [4.3.1. Apache Flink](#431-apache-flink)
  - [5. ETLè®¾è®¡æ¨¡å¼](#5-etlè®¾è®¡æ¨¡å¼)
    - [5.1. æ•°æ®æŠ½å–æ¨¡å¼](#51-æ•°æ®æŠ½å–æ¨¡å¼)
      - [5.1.1. æŠ½å–æ¨¡å¼ç±»å‹](#511-æŠ½å–æ¨¡å¼ç±»å‹)
    - [5.2. æ•°æ®è½¬æ¢æ¨¡å¼](#52-æ•°æ®è½¬æ¢æ¨¡å¼)
      - [5.2.1. è½¬æ¢æ¨¡å¼ç±»å‹](#521-è½¬æ¢æ¨¡å¼ç±»å‹)
    - [5.3. æ•°æ®åŠ è½½æ¨¡å¼](#53-æ•°æ®åŠ è½½æ¨¡å¼)
      - [5.3.1. åŠ è½½æ¨¡å¼ç±»å‹](#531-åŠ è½½æ¨¡å¼ç±»å‹)
  - [6. ETLæ€§èƒ½ä¼˜åŒ–](#6-etlæ€§èƒ½ä¼˜åŒ–)
    - [6.1. æŠ½å–ä¼˜åŒ–](#61-æŠ½å–ä¼˜åŒ–)
      - [6.1.1. æŠ½å–ä¼˜åŒ–ç­–ç•¥](#611-æŠ½å–ä¼˜åŒ–ç­–ç•¥)
    - [6.2. è½¬æ¢ä¼˜åŒ–](#62-è½¬æ¢ä¼˜åŒ–)
      - [6.2.1. è½¬æ¢ä¼˜åŒ–ç­–ç•¥](#621-è½¬æ¢ä¼˜åŒ–ç­–ç•¥)
    - [6.3. åŠ è½½ä¼˜åŒ–](#63-åŠ è½½ä¼˜åŒ–)
      - [6.3.1. åŠ è½½ä¼˜åŒ–ç­–ç•¥](#631-åŠ è½½ä¼˜åŒ–ç­–ç•¥)
  - [7. ETLè´¨é‡ä¿è¯](#7-etlè´¨é‡ä¿è¯)
    - [7.1. æ•°æ®è´¨é‡æ£€æŸ¥](#71-æ•°æ®è´¨é‡æ£€æŸ¥)
      - [7.1.1. è´¨é‡æ£€æŸ¥æŒ‡æ ‡](#711-è´¨é‡æ£€æŸ¥æŒ‡æ ‡)
    - [7.2. é”™è¯¯å¤„ç†](#72-é”™è¯¯å¤„ç†)
      - [7.2.1. é”™è¯¯å¤„ç†ç­–ç•¥](#721-é”™è¯¯å¤„ç†ç­–ç•¥)
    - [7.3. æ•°æ®éªŒè¯](#73-æ•°æ®éªŒè¯)
      - [7.3.1. éªŒè¯è§„åˆ™](#731-éªŒè¯è§„åˆ™)
  - [8. å®é™…åº”ç”¨æ¡ˆä¾‹](#8-å®é™…åº”ç”¨æ¡ˆä¾‹)
    - [8.1. é›¶å”®è¡Œä¸šæ¡ˆä¾‹](#81-é›¶å”®è¡Œä¸šæ¡ˆä¾‹)
    - [8.2. äº’è”ç½‘è¡Œä¸šæ¡ˆä¾‹](#82-äº’è”ç½‘è¡Œä¸šæ¡ˆä¾‹)
  - [9. å½¢å¼åŒ–å®šä¹‰](#9-å½¢å¼åŒ–å®šä¹‰)
    - [9.1. ETLå½¢å¼åŒ–](#91-etlå½¢å¼åŒ–)
    - [9.2. æ•°æ®è´¨é‡å½¢å¼åŒ–](#92-æ•°æ®è´¨é‡å½¢å¼åŒ–)
  - [10. å¤šè¡¨å¾](#10-å¤šè¡¨å¾)
  - [11. æ€»ç»“ä¸å±•æœ›](#11-æ€»ç»“ä¸å±•æœ›)
    - [11.1. æ€»ç»“](#111-æ€»ç»“)
    - [11.2. å‘å±•è¶‹åŠ¿](#112-å‘å±•è¶‹åŠ¿)

## 1. æ¦‚è¿°

### 1.1. ETLå®šä¹‰

**ETLï¼ˆExtract-Transform-Loadï¼‰**æ˜¯æ•°æ®é›†æˆå’Œæ•°æ®ä»“åº“å»ºè®¾çš„æ ¸å¿ƒè¿‡ç¨‹ï¼š

- **Extractï¼ˆæŠ½å–ï¼‰**ï¼šä»å„ç§æ•°æ®æºæå–æ•°æ®
- **Transformï¼ˆè½¬æ¢ï¼‰**ï¼šæ¸…æ´—ã€è½¬æ¢ã€éªŒè¯æ•°æ®
- **Loadï¼ˆåŠ è½½ï¼‰**ï¼šå°†æ•°æ®åŠ è½½åˆ°ç›®æ ‡ç³»ç»Ÿ

**ETLå½¢å¼åŒ–å®šä¹‰**ï¼š

è®¾ETLè¿‡ç¨‹ä¸ºä¸‰å…ƒç»„ $ETL = (E, T, L)$ï¼Œå…¶ä¸­ï¼š

- $E: S \to D_{raw}$ï¼šæŠ½å–å‡½æ•°ï¼Œä»æ•°æ®æºé›†åˆ $S$ æŠ½å–åŸå§‹æ•°æ® $D_{raw}$
- $T: D_{raw} \to D_{clean}$ï¼šè½¬æ¢å‡½æ•°ï¼Œå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¸…æ´—åçš„æ•°æ® $D_{clean}$
- $L: D_{clean} \to T$ï¼šåŠ è½½å‡½æ•°ï¼Œå°†æ¸…æ´—åçš„æ•°æ®åŠ è½½åˆ°ç›®æ ‡ç³»ç»Ÿ $T$

### 1.2. ETLä½œç”¨

**ETLçš„ä¸»è¦ä½œç”¨**ï¼š

1. **æ•°æ®é›†æˆ**ï¼šæ•´åˆå¤šä¸ªå¼‚æ„æ•°æ®æº
2. **æ•°æ®è´¨é‡**ï¼šä¿è¯æ•°æ®è´¨é‡å’Œä¸€è‡´æ€§
3. **æ•°æ®è½¬æ¢**ï¼šå°†æ•°æ®è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šé«˜æ•ˆå¤„ç†å¤§æ•°æ®é‡

### 1.3. ETLå†å²

**ETLå‘å±•å†ç¨‹**ï¼š

1. **1970s-1980s**ï¼šæ•°æ®ä»“åº“æ¦‚å¿µæå‡ºï¼ŒETLå¼€å§‹å‘å±•
2. **1990s**ï¼šå•†ä¸šETLå·¥å…·å‡ºç°ï¼ˆInformaticaã€DataStageï¼‰
3. **2000s**ï¼šå¼€æºETLå·¥å…·å…´èµ·ï¼ˆPentahoã€Talendï¼‰
4. **2010s**ï¼šå¤§æ•°æ®ETLæ¡†æ¶ï¼ˆHadoopã€Sparkï¼‰
5. **2020s**ï¼šå®æ—¶ETLå’Œæµå¤„ç†ï¼ˆFlinkã€Kafkaï¼‰

---

## 2. ETLæ ¸å¿ƒæµç¨‹

### 2.1. æ•°æ®æŠ½å–ï¼ˆExtractï¼‰

#### 2.1.1. æŠ½å–ç­–ç•¥

**æŠ½å–ç­–ç•¥ç±»å‹**ï¼š

1. **å…¨é‡æŠ½å–**ï¼šä¸€æ¬¡æ€§æŠ½å–æ‰€æœ‰æ•°æ®
2. **å¢é‡æŠ½å–**ï¼šåªæŠ½å–å˜æ›´çš„æ•°æ®
3. **CDCï¼ˆå˜æ›´æ•°æ®æ•è·ï¼‰**ï¼šå®æ—¶æ•è·æ•°æ®å˜æ›´

**å…¨é‡æŠ½å–**ï¼š

```python
class FullExtract:
    def extract(self, source):
        """å…¨é‡æŠ½å–"""
        # ä»æ•°æ®æºæŠ½å–æ‰€æœ‰æ•°æ®
        data = source.read_all()
        return data
```

**å¢é‡æŠ½å–**ï¼š

```python
class IncrementalExtract:
    def __init__(self, last_extract_time):
        self.last_extract_time = last_extract_time

    def extract(self, source):
        """å¢é‡æŠ½å–"""
        # åªæŠ½å–ä¸Šæ¬¡æŠ½å–åçš„å˜æ›´æ•°æ®
        data = source.read_since(self.last_extract_time)
        return data
```

#### 2.1.2. æ•°æ®æºç±»å‹

**æ•°æ®æºç±»å‹**ï¼š

1. **å…³ç³»æ•°æ®åº“**ï¼šMySQLã€PostgreSQLã€Oracle
2. **NoSQLæ•°æ®åº“**ï¼šMongoDBã€Cassandraã€Redis
3. **æ–‡ä»¶ç³»ç»Ÿ**ï¼šCSVã€JSONã€Parquet
4. **APIæ¥å£**ï¼šREST APIã€GraphQL
5. **æ¶ˆæ¯é˜Ÿåˆ—**ï¼šKafkaã€RabbitMQ

**å¤šæ•°æ®æºæŠ½å–**ï¼š

```python
class MultiSourceExtract:
    def __init__(self, sources):
        self.sources = sources

    def extract_all(self):
        """ä»å¤šä¸ªæ•°æ®æºæŠ½å–"""
        results = {}
        for name, source in self.sources.items():
            results[name] = source.extract()
        return results
```

### 2.2. æ•°æ®è½¬æ¢ï¼ˆTransformï¼‰

#### 2.2.1. æ•°æ®æ¸…æ´—

**æ•°æ®æ¸…æ´—æ“ä½œ**ï¼š

1. **å»é‡**ï¼šåˆ é™¤é‡å¤æ•°æ®
2. **ç©ºå€¼å¤„ç†**ï¼šå¤„ç†ç¼ºå¤±å€¼
3. **æ ¼å¼è½¬æ¢**ï¼šç»Ÿä¸€æ•°æ®æ ¼å¼
4. **å¼‚å¸¸å€¼å¤„ç†**ï¼šå¤„ç†å¼‚å¸¸æ•°æ®

**æ•°æ®æ¸…æ´—å®ç°**ï¼š

```python
class DataCleaning:
    def clean(self, data):
        """æ•°æ®æ¸…æ´—"""
        # å»é‡
        data = data.drop_duplicates()

        # ç©ºå€¼å¤„ç†
        data = data.fillna(method='ffill')

        # æ ¼å¼è½¬æ¢
        data['date'] = pd.to_datetime(data['date'])

        # å¼‚å¸¸å€¼å¤„ç†
        data = data[(data['value'] >= 0) & (data['value'] <= 100)]

        return data
```

#### 2.2.2. æ•°æ®è½¬æ¢

**æ•°æ®è½¬æ¢æ“ä½œ**ï¼š

1. **æ•°æ®ç±»å‹è½¬æ¢**ï¼šè½¬æ¢æ•°æ®ç±»å‹
2. **è®¡ç®—å­—æ®µ**ï¼šè®¡ç®—è¡ç”Ÿå­—æ®µ
3. **æ•°æ®èšåˆ**ï¼šæ•°æ®èšåˆæ“ä½œ
4. **æ•°æ®æ ‡å‡†åŒ–**ï¼šæ•°æ®æ ‡å‡†åŒ–

**æ•°æ®è½¬æ¢å®ç°**ï¼š

```python
class DataTransform:
    def transform(self, data):
        """æ•°æ®è½¬æ¢"""
        # æ•°æ®ç±»å‹è½¬æ¢
        data['amount'] = data['amount'].astype(float)

        # è®¡ç®—å­—æ®µ
        data['total'] = data['quantity'] * data['price']

        # æ•°æ®èšåˆ
        aggregated = data.groupby('category').agg({
            'amount': 'sum',
            'quantity': 'sum'
        })

        # æ•°æ®æ ‡å‡†åŒ–
        data['normalized'] = (data['value'] - data['value'].mean()) / data['value'].std()

        return data
```

### 2.3. æ•°æ®åŠ è½½ï¼ˆLoadï¼‰

#### 2.3.1. åŠ è½½ç­–ç•¥

**åŠ è½½ç­–ç•¥ç±»å‹**ï¼š

1. **å…¨é‡åŠ è½½**ï¼šæ¸…ç©ºååŠ è½½æ‰€æœ‰æ•°æ®
2. **å¢é‡åŠ è½½**ï¼šè¿½åŠ æ–°æ•°æ®
3. **æ›´æ–°åŠ è½½**ï¼šæ›´æ–°å·²æœ‰æ•°æ®
4. **UPSERT**ï¼šæ’å…¥æˆ–æ›´æ–°

**å…¨é‡åŠ è½½**ï¼š

```python
class FullLoad:
    def load(self, target, data):
        """å…¨é‡åŠ è½½"""
        # æ¸…ç©ºç›®æ ‡è¡¨
        target.truncate()

        # åŠ è½½æ•°æ®
        target.insert(data)
```

**å¢é‡åŠ è½½**ï¼š

```python
class IncrementalLoad:
    def load(self, target, data):
        """å¢é‡åŠ è½½"""
        # åªåŠ è½½æ–°æ•°æ®
        existing_ids = target.get_existing_ids()
        new_data = data[~data['id'].isin(existing_ids)]
        target.insert(new_data)
```

#### 2.3.2. æ‰¹é‡åŠ è½½

**æ‰¹é‡åŠ è½½ä¼˜åŒ–**ï¼š

```python
class BatchLoad:
    def __init__(self, batch_size=1000):
        self.batch_size = batch_size

    def load(self, target, data):
        """æ‰¹é‡åŠ è½½"""
        for i in range(0, len(data), self.batch_size):
            batch = data[i:i + self.batch_size]
            target.insert_batch(batch)
```

---

## 3. ETLæ¨¡å¼ä¸æ–¹æ³•

### 3.1. æ‰¹å¤„ç†ETL

#### 3.1.1. æ‰¹å¤„ç†ç‰¹ç‚¹

**æ‰¹å¤„ç†ETLç‰¹ç‚¹**ï¼š

- **å®šæ—¶æ‰§è¡Œ**ï¼šæŒ‰è®¡åˆ’å®šæ—¶æ‰§è¡Œ
- **å¤§æ‰¹é‡**ï¼šå¤„ç†å¤§æ‰¹é‡æ•°æ®
- **é«˜åå**ï¼šé«˜ååé‡å¤„ç†
- **å»¶è¿Ÿè¾ƒé«˜**ï¼šæ•°æ®å»¶è¿Ÿè¾ƒé«˜

**æ‰¹å¤„ç†ETLæµç¨‹**ï¼š

```mermaid
flowchart LR
    S[æ•°æ®æº] -->|å®šæ—¶æŠ½å–| E[æŠ½å–]
    E --> T[è½¬æ¢]
    T --> L[åŠ è½½]
    L --> W[æ•°æ®ä»“åº“]
```

#### 3.1.2. æ‰¹å¤„ç†å®ç°

**æ‰¹å¤„ç†ETLå®ç°**ï¼š

```python
class BatchETL:
    def __init__(self, extractor, transformer, loader):
        self.extractor = extractor
        self.transformer = transformer
        self.loader = loader

    def run(self):
        """æ‰§è¡Œæ‰¹å¤„ç†ETL"""
        # æŠ½å–
        raw_data = self.extractor.extract()

        # è½¬æ¢
        clean_data = self.transformer.transform(raw_data)

        # åŠ è½½
        self.loader.load(clean_data)
```

### 3.2. æµå¤„ç†ETL

#### 3.2.1. æµå¤„ç†ç‰¹ç‚¹

**æµå¤„ç†ETLç‰¹ç‚¹**ï¼š

- **å®æ—¶å¤„ç†**ï¼šå®æ—¶å¤„ç†æ•°æ®æµ
- **ä½å»¶è¿Ÿ**ï¼šä½å»¶è¿Ÿå¤„ç†
- **è¿ç»­å¤„ç†**ï¼šè¿ç»­å¤„ç†æ•°æ®
- **çŠ¶æ€ç®¡ç†**ï¼šéœ€è¦çŠ¶æ€ç®¡ç†

**æµå¤„ç†ETLæµç¨‹**ï¼š

```mermaid
flowchart LR
    S[æ•°æ®æµ] -->|å®æ—¶| E[æŠ½å–]
    E --> T[è½¬æ¢]
    T --> L[åŠ è½½]
    L --> W[ç›®æ ‡ç³»ç»Ÿ]
```

#### 3.2.2. æµå¤„ç†å®ç°

**æµå¤„ç†ETLå®ç°ï¼ˆApache Flinkï¼‰**ï¼š

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
table_env = StreamTableEnvironment.create(env)

# å®šä¹‰æºè¡¨
table_env.execute_sql("""
    CREATE TABLE source (
        id INT,
        name STRING,
        value DOUBLE
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'input-topic'
    )
""")

# å®šä¹‰ç›®æ ‡è¡¨
table_env.execute_sql("""
    CREATE TABLE sink (
        id INT,
        name STRING,
        value DOUBLE,
        processed_time TIMESTAMP
    ) WITH (
        'connector' = 'jdbc',
        'url' = 'jdbc:postgresql://localhost:5432/mydb',
        'table-name' = 'target_table'
    )
""")

# æ‰§è¡ŒETL
table_env.execute_sql("""
    INSERT INTO sink
    SELECT id, name, value, CURRENT_TIMESTAMP
    FROM source
""")
```

### 3.3. å®æ—¶ETL

#### 3.3.1. å®æ—¶ETLç‰¹ç‚¹

**å®æ—¶ETLç‰¹ç‚¹**ï¼š

- **æ¯«ç§’çº§å»¶è¿Ÿ**ï¼šæ¯«ç§’çº§å¤„ç†å»¶è¿Ÿ
- **äº‹ä»¶é©±åŠ¨**ï¼šäº‹ä»¶é©±åŠ¨å¤„ç†
- **é«˜å¯ç”¨**ï¼šé«˜å¯ç”¨æ€§è¦æ±‚
- **å¤æ‚å¤„ç†**ï¼šæ”¯æŒå¤æ‚å¤„ç†é€»è¾‘

---

## 4. ETLå·¥å…·ä¸æ¡†æ¶

### 4.1. å¼€æºETLå·¥å…·

#### 4.1.1. Apache Airflow

**Airflowç‰¹ç‚¹**ï¼š

- **å·¥ä½œæµç¼–æ’**ï¼šDAGå·¥ä½œæµç¼–æ’
- **è°ƒåº¦ç®¡ç†**ï¼šå¼ºå¤§çš„è°ƒåº¦åŠŸèƒ½
- **å¯æ‰©å±•**ï¼šä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€

**Airflowç¤ºä¾‹**ï¼š

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def extract_data():
    # æŠ½å–æ•°æ®
    pass

def transform_data():
    # è½¬æ¢æ•°æ®
    pass

def load_data():
    # åŠ è½½æ•°æ®
    pass

dag = DAG('etl_pipeline', start_date=datetime(2024, 1, 1))

extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load_data,
    dag=dag
)

extract_task >> transform_task >> load_task
```

#### 4.1.2. Apache Spark

**Spark ETLç‰¹ç‚¹**ï¼š

- **åˆ†å¸ƒå¼å¤„ç†**ï¼šåˆ†å¸ƒå¼æ•°æ®å¤„ç†
- **å†…å­˜è®¡ç®—**ï¼šå†…å­˜è®¡ç®—åŠ é€Ÿ
- **å¤šç§æ•°æ®æº**ï¼šæ”¯æŒå¤šç§æ•°æ®æº

**Spark ETLç¤ºä¾‹**ï¼š

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ETL").getOrCreate()

# æŠ½å–
df = spark.read.format("jdbc").options(
    url="jdbc:postgresql://localhost:5432/mydb",
    dbtable="source_table"
).load()

# è½¬æ¢
df_transformed = df.filter(df["status"] == "active") \
    .groupBy("category") \
    .agg({"amount": "sum"})

# åŠ è½½
df_transformed.write.format("jdbc").options(
    url="jdbc:postgresql://localhost:5432/mydb",
    dbtable="target_table"
).mode("overwrite").save()
```

### 4.2. å•†ä¸šETLå·¥å…·

#### 4.2.1. Informatica

**Informaticaç‰¹ç‚¹**ï¼š

- **ä¼ä¸šçº§**ï¼šä¼ä¸šçº§ETLå·¥å…·
- **å¯è§†åŒ–**ï¼šå¯è§†åŒ–è®¾è®¡ç•Œé¢
- **é«˜æ€§èƒ½**ï¼šé«˜æ€§èƒ½å¤„ç†å¼•æ“

#### 4.2.2. Talend

**Talendç‰¹ç‚¹**ï¼š

- **å¼€æºå’Œå•†ä¸šç‰ˆæœ¬**ï¼šå¼€æºå’Œå•†ä¸šç‰ˆæœ¬
- **ä»£ç ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç”Ÿæˆä»£ç 
- **å¤§æ•°æ®æ”¯æŒ**ï¼šæ”¯æŒå¤§æ•°æ®å¤„ç†

### 4.3. å¤§æ•°æ®ETLæ¡†æ¶

#### 4.3.1. Apache Flink

**Flink ETLç‰¹ç‚¹**ï¼š

- **æµæ‰¹ä¸€ä½“**ï¼šç»Ÿä¸€çš„æµæ‰¹å¤„ç†
- **ä½å»¶è¿Ÿ**ï¼šä½å»¶è¿Ÿå¤„ç†
- **çŠ¶æ€ç®¡ç†**ï¼šå¼ºå¤§çš„çŠ¶æ€ç®¡ç†

---

## 5. ETLè®¾è®¡æ¨¡å¼

### 5.1. æ•°æ®æŠ½å–æ¨¡å¼

#### 5.1.1. æŠ½å–æ¨¡å¼ç±»å‹

**æŠ½å–æ¨¡å¼**ï¼š

1. **å…¨é‡æŠ½å–æ¨¡å¼**ï¼šé€‚åˆå°æ•°æ®é‡
2. **å¢é‡æŠ½å–æ¨¡å¼**ï¼šé€‚åˆå¤§æ•°æ®é‡
3. **CDCæ¨¡å¼**ï¼šé€‚åˆå®æ—¶éœ€æ±‚

### 5.2. æ•°æ®è½¬æ¢æ¨¡å¼

#### 5.2.1. è½¬æ¢æ¨¡å¼ç±»å‹

**è½¬æ¢æ¨¡å¼**ï¼š

1. **ç®€å•è½¬æ¢**ï¼šç›´æ¥æ˜ å°„
2. **å¤æ‚è½¬æ¢**ï¼šå¤šæ­¥éª¤è½¬æ¢
3. **èšåˆè½¬æ¢**ï¼šæ•°æ®èšåˆ

### 5.3. æ•°æ®åŠ è½½æ¨¡å¼

#### 5.3.1. åŠ è½½æ¨¡å¼ç±»å‹

**åŠ è½½æ¨¡å¼**ï¼š

1. **å…¨é‡æ›¿æ¢**ï¼šæ¸…ç©ºååŠ è½½
2. **å¢é‡è¿½åŠ **ï¼šè¿½åŠ æ–°æ•°æ®
3. **æ›´æ–°åˆå¹¶**ï¼šæ›´æ–°å·²æœ‰æ•°æ®

---

## 6. ETLæ€§èƒ½ä¼˜åŒ–

ETLæ€§èƒ½ä¼˜åŒ–æ˜¯å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„å…³é”®ï¼Œéœ€è¦ä»æŠ½å–ã€è½¬æ¢ã€åŠ è½½ä¸‰ä¸ªç¯èŠ‚è¿›è¡Œä¼˜åŒ–ã€‚

### 6.1. æŠ½å–ä¼˜åŒ–

#### 6.1.1. æŠ½å–ä¼˜åŒ–ç­–ç•¥

**å¢é‡æŠ½å–**ï¼š

åªæŠ½å–è‡ªä¸Šæ¬¡æŠ½å–åå‘ç”Ÿå˜åŒ–çš„æ•°æ®ï¼Œå‡å°‘æ•°æ®ä¼ è¾“é‡ã€‚

**å®ç°ç¤ºä¾‹**ï¼š

```python
import json
from datetime import datetime
import pandas as pd

class IncrementalExtract:
    """å¢é‡æŠ½å–"""

    def __init__(self, source_db, checkpoint_file='checkpoint.json'):
        self.source_db = source_db
        self.checkpoint_file = checkpoint_file

    def get_last_timestamp(self):
        """è·å–ä¸Šæ¬¡æŠ½å–çš„æ—¶é—´æˆ³"""
        try:
            with open(self.checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                return checkpoint.get('last_timestamp')
        except FileNotFoundError:
            return None

    def extract_incremental(self, table_name, timestamp_column='updated_at'):
        """å¢é‡æŠ½å–æ•°æ®"""
        last_timestamp = self.get_last_timestamp()

        if last_timestamp:
            query = f"""
                SELECT * FROM {table_name}
                WHERE {timestamp_column} > '{last_timestamp}'
                ORDER BY {timestamp_column}
            """
        else:
            query = f"SELECT * FROM {table_name}"

        data = pd.read_sql(query, self.source_db)

        # æ›´æ–°æ£€æŸ¥ç‚¹
        if not data.empty:
            latest_timestamp = data[timestamp_column].max()
            self.save_checkpoint(latest_timestamp)

        return data

    def save_checkpoint(self, timestamp):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {'last_timestamp': str(timestamp)}
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint, f)
```

**å¹¶è¡ŒæŠ½å–**ï¼š

ä½¿ç”¨å¤šè¿›ç¨‹/å¤šçº¿ç¨‹å¹¶è¡ŒæŠ½å–å¤šä¸ªæ•°æ®æºã€‚

**å®ç°ç¤ºä¾‹**ï¼š

```python
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp

def parallel_extract(sources: list, max_workers=None):
    """
    å¹¶è¡ŒæŠ½å–å¤šä¸ªæ•°æ®æº

    å‚æ•°:
        sources: æ•°æ®æºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(æ•°æ®æº, æŸ¥è¯¢)å…ƒç»„
        max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    """
    if max_workers is None:
        max_workers = min(len(sources), mp.cpu_count())

    def extract_from_source(source_query):
        source, query = source_query
        return pd.read_sql(query, source)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(extract_from_source, sources))

    return results
```

**æŠ½å–ä¼˜åŒ–**ï¼š

1. **å¹¶è¡ŒæŠ½å–**ï¼šå¤šçº¿ç¨‹å¹¶è¡ŒæŠ½å–
2. **å¢é‡æŠ½å–**ï¼šåªæŠ½å–å˜æ›´æ•°æ®
3. **åˆ†åŒºæŠ½å–**ï¼šæŒ‰åˆ†åŒºæŠ½å–

**å¹¶è¡ŒæŠ½å–**ï¼š

```python
from concurrent.futures import ThreadPoolExecutor

class ParallelExtract:
    def __init__(self, sources, max_workers=4):
        self.sources = sources
        self.max_workers = max_workers

    def extract_all(self):
        """å¹¶è¡ŒæŠ½å–"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = executor.map(lambda s: s.extract(), self.sources)
        return list(results)
```

### 6.2. è½¬æ¢ä¼˜åŒ–

#### 6.2.1. è½¬æ¢ä¼˜åŒ–ç­–ç•¥

**è½¬æ¢ä¼˜åŒ–**ï¼š

1. **å‘é‡åŒ–æ“ä½œ**ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œ
2. **ç¼“å­˜ä¸­é—´ç»“æœ**ï¼šç¼“å­˜ä¸­é—´ç»“æœ
3. **å¹¶è¡Œè½¬æ¢**ï¼šå¹¶è¡Œè½¬æ¢å¤„ç†

### 6.3. åŠ è½½ä¼˜åŒ–

#### 6.3.1. åŠ è½½ä¼˜åŒ–ç­–ç•¥

**åŠ è½½ä¼˜åŒ–**ï¼š

1. **æ‰¹é‡åŠ è½½**ï¼šæ‰¹é‡æ’å…¥
2. **å¹¶è¡ŒåŠ è½½**ï¼šå¹¶è¡ŒåŠ è½½
3. **ç´¢å¼•ä¼˜åŒ–**ï¼šåŠ è½½ååˆ›å»ºç´¢å¼•

---

## 7. ETLè´¨é‡ä¿è¯

### 7.1. æ•°æ®è´¨é‡æ£€æŸ¥

#### 7.1.1. è´¨é‡æ£€æŸ¥æŒ‡æ ‡

**è´¨é‡æ£€æŸ¥æŒ‡æ ‡**ï¼š

1. **å®Œæ•´æ€§**ï¼šæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
2. **å‡†ç¡®æ€§**ï¼šæ•°æ®å‡†ç¡®æ€§æ£€æŸ¥
3. **ä¸€è‡´æ€§**ï¼šæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
4. **åŠæ—¶æ€§**ï¼šæ•°æ®åŠæ—¶æ€§æ£€æŸ¥

**è´¨é‡æ£€æŸ¥å®ç°**ï¼š

```python
class DataQualityCheck:
    def check_completeness(self, data):
        """å®Œæ•´æ€§æ£€æŸ¥"""
        missing_rate = data.isnull().sum() / len(data)
        return missing_rate < 0.05  # ç¼ºå¤±ç‡å°äº5%

    def check_accuracy(self, data, rules):
        """å‡†ç¡®æ€§æ£€æŸ¥"""
        for rule in rules:
            if not rule.validate(data):
                return False
        return True
```

### 7.2. é”™è¯¯å¤„ç†

#### 7.2.1. é”™è¯¯å¤„ç†ç­–ç•¥

**é”™è¯¯å¤„ç†ç­–ç•¥**ï¼š

1. **è·³è¿‡é”™è¯¯**ï¼šè·³è¿‡é”™è¯¯è®°å½•
2. **è®°å½•é”™è¯¯**ï¼šè®°å½•é”™è¯¯åˆ°æ—¥å¿—
3. **é‡è¯•æœºåˆ¶**ï¼šå¤±è´¥é‡è¯•

**é”™è¯¯å¤„ç†å®ç°**ï¼š

```python
class ErrorHandler:
    def handle_error(self, error, record):
        """é”™è¯¯å¤„ç†"""
        # è®°å½•é”™è¯¯
        self.log_error(error, record)

        # æ ¹æ®é”™è¯¯ç±»å‹å¤„ç†
        if error.type == 'data_quality':
            # æ•°æ®è´¨é‡é”™è¯¯ï¼Œè·³è¿‡
            return 'skip'
        elif error.type == 'network':
            # ç½‘ç»œé”™è¯¯ï¼Œé‡è¯•
            return 'retry'
```

### 7.3. æ•°æ®éªŒè¯

#### 7.3.1. éªŒè¯è§„åˆ™

**éªŒè¯è§„åˆ™**ï¼š

1. **æ ¼å¼éªŒè¯**ï¼šæ•°æ®æ ¼å¼éªŒè¯
2. **èŒƒå›´éªŒè¯**ï¼šæ•°æ®èŒƒå›´éªŒè¯
3. **ä¸šåŠ¡è§„åˆ™éªŒè¯**ï¼šä¸šåŠ¡è§„åˆ™éªŒè¯

---

## 8. å®é™…åº”ç”¨æ¡ˆä¾‹

### 8.1. é›¶å”®è¡Œä¸šæ¡ˆä¾‹

**åœºæ™¯**ï¼š

- å¤šæ¸ é“é”€å”®æ•°æ®ETL
- ç»Ÿä¸€åˆ†æåº“å­˜ä¸é”€å”®è¶‹åŠ¿

**å®ç°**ï¼š

```python
# ä»POSã€ç½‘åº—ã€ERPç­‰å¤šæºæå–æ•°æ®
sources = {
    'pos': POSExtractor(),
    'online': OnlineExtractor(),
    'erp': ERPExtractor()
}

# è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
transformer = UnifiedTransformer()

# åŠ è½½è‡³æ•°æ®ä»“åº“
loader = DataWarehouseLoader()

# æ‰§è¡ŒETL
etl = ETLPipeline(sources, transformer, loader)
etl.run()
```

### 8.2. äº’è”ç½‘è¡Œä¸šæ¡ˆä¾‹

**åœºæ™¯**ï¼š

- ç”¨æˆ·è¡Œä¸ºæ—¥å¿—ETL
- æ”¯æ’‘å®æ—¶æ¨èä¸åˆ†æ

**å®ç°**ï¼š

```python
# ä½¿ç”¨Flinkè¿›è¡Œå®æ—¶ETL
from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()

# ä»Kafkaè¯»å–æ—¥å¿—
log_stream = env.add_source(KafkaSource())

# å®æ—¶è½¬æ¢
transformed_stream = log_stream \
    .filter(lambda x: x['event_type'] == 'click') \
    .map(lambda x: transform_log(x))

# å†™å…¥æ•°æ®ä»“åº“
transformed_stream.add_sink(DataWarehouseSink())
```

---

## 9. å½¢å¼åŒ–å®šä¹‰

### 9.1. ETLå½¢å¼åŒ–

**ETLè¿‡ç¨‹å½¢å¼åŒ–**ï¼š

$$ETL(S, T) = L(T(E(S)))$$

å…¶ä¸­ï¼š

- $S$ï¼šæ•°æ®æºé›†åˆ
- $E$ï¼šæŠ½å–å‡½æ•°
- $T$ï¼šè½¬æ¢å‡½æ•°
- $L$ï¼šåŠ è½½å‡½æ•°

### 9.2. æ•°æ®è´¨é‡å½¢å¼åŒ–

**æ•°æ®è´¨é‡å®šä¹‰**ï¼š

$$Quality(D) = \alpha \cdot Completeness(D) + \beta \cdot Accuracy(D) + \gamma \cdot Consistency(D)$$

å…¶ä¸­ $\alpha + \beta + \gamma = 1$ã€‚

---

## 10. å¤šè¡¨å¾

æœ¬ä¸»é¢˜æ”¯æŒå¤šç§è¡¨å¾æ–¹å¼ï¼š

1. **ç¬¦å·è¡¨å¾**ï¼šå½¢å¼åŒ–å®šä¹‰ã€æ•°å­¦å…¬å¼
2. **å›¾ç»“æ„**ï¼šETLæµç¨‹å›¾ã€æ•°æ®æµå›¾
3. **ä»£ç å®ç°**ï¼šETLä»£ç ç¤ºä¾‹
4. **è‡ªç„¶è¯­è¨€**ï¼šæ¦‚å¿µå®šä¹‰ã€æœ€ä½³å®è·µ
5. **å¯è§†åŒ–**ï¼šETLå¯è§†åŒ–ã€ç›‘æ§é¢æ¿

---

## 11. æ€»ç»“ä¸å±•æœ›

### 11.1. æ€»ç»“

ETLç†è®ºä¸å®è·µçš„æ ¸å¿ƒè¦ç‚¹ï¼š

1. **æ ¸å¿ƒæµç¨‹**ï¼šæŠ½å–ã€è½¬æ¢ã€åŠ è½½
2. **å¤„ç†æ¨¡å¼**ï¼šæ‰¹å¤„ç†ã€æµå¤„ç†ã€å®æ—¶å¤„ç†
3. **å·¥å…·æ¡†æ¶**ï¼šAirflowã€Sparkã€Flinkç­‰
4. **è´¨é‡ä¿è¯**ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ã€é”™è¯¯å¤„ç†

### 11.2. å‘å±•è¶‹åŠ¿

**æœªæ¥å‘å±•æ–¹å‘**ï¼š

1. **å®æ—¶åŒ–**ï¼šå®æ—¶ETLæˆä¸ºä¸»æµ
2. **è‡ªåŠ¨åŒ–**ï¼šè‡ªåŠ¨åŒ–ETLæµç¨‹
3. **æ™ºèƒ½åŒ–**ï¼šAIé©±åŠ¨çš„ETLä¼˜åŒ–

---

**å‚è€ƒæ–‡çŒ®**ï¼š

1. Kimball, R., & Ross, M. (2013). "The Data Warehouse Toolkit"
2. Inmon, W. H. (2005). "Building the Data Warehouse"

---

[è¿”å›æ•°æ®åˆ†æä¸ETLå¯¼èˆª](../README.md)
