# 3.1.26 æ•°æ®ç§‘å­¦å®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [3.1.26 æ•°æ®ç§‘å­¦å®è·µæ¡ˆä¾‹](#3126-æ•°æ®ç§‘å­¦å®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. æ•°æ®åˆ†æå®è·µæ¡ˆä¾‹](#2-æ•°æ®åˆ†æå®è·µæ¡ˆä¾‹)
    - [2.1. ç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æ](#21-ç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æ)
  - [3. é‡‘èé£é™©åˆ†æ](#3-é‡‘èé£é™©åˆ†æ)
  - [4. æœºå™¨å­¦ä¹ å®è·µæ¡ˆä¾‹](#4-æœºå™¨å­¦ä¹ å®è·µæ¡ˆä¾‹)
    - [4.1. æ¨èç³»ç»Ÿå®ç°](#41-æ¨èç³»ç»Ÿå®ç°)
  - [5. æ·±åº¦å­¦ä¹ å®è·µæ¡ˆä¾‹](#5-æ·±åº¦å­¦ä¹ å®è·µæ¡ˆä¾‹)
    - [5.1. å›¾åƒåˆ†ç±»æ¨¡å‹](#51-å›¾åƒåˆ†ç±»æ¨¡å‹)
  - [6. è‡ªç„¶è¯­è¨€å¤„ç†å®è·µæ¡ˆä¾‹](#6-è‡ªç„¶è¯­è¨€å¤„ç†å®è·µæ¡ˆä¾‹)
    - [6.1. æ–‡æœ¬æƒ…æ„Ÿåˆ†æ](#61-æ–‡æœ¬æƒ…æ„Ÿåˆ†æ)
  - [7. æ€»ç»“](#7-æ€»ç»“)

---


## 1. æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ•°æ®ç§‘å­¦åœ¨å®é™…åº”ç”¨ä¸­çš„å…·ä½“æ¡ˆä¾‹ï¼Œæ¶µç›–æ•°æ®åˆ†æã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰å„ä¸ªæ–¹é¢çš„å®è·µåº”ç”¨ã€‚

## 2. æ•°æ®åˆ†æå®è·µæ¡ˆä¾‹

### 2.1. ç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æ

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

class EcommerceUserAnalysis:
    def __init__(self):
        self.data = None

    def load_data(self, file_path):
        """åŠ è½½ç”µå•†ç”¨æˆ·æ•°æ®"""
        self.data = pd.read_csv(file_path)
        return self.data

    def user_segmentation(self, n_clusters=4):
        """ç”¨æˆ·åˆ†ç¾¤åˆ†æ"""
        features = ['age', 'total_spent', 'purchase_frequency', 'activity_score']
        X = self.data[features].fillna(self.data[features].mean())

# æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

# K-meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(X_scaled)

        self.data['cluster'] = clusters

# åˆ†æå„ç¾¤ç‰¹å¾
        cluster_analysis = self.data.groupby('cluster')[features].mean()
        print("ç”¨æˆ·åˆ†ç¾¤ç‰¹å¾åˆ†æ:")
        print(cluster_analysis)

        return cluster_analysis

    def generate_recommendations(self):
        """ç”Ÿæˆæ¨èç­–ç•¥"""
        cluster_analysis = self.data.groupby('cluster').agg({
            'total_spent': 'mean',
            'purchase_frequency': 'mean'
        })

        recommendations = {}
        for cluster in cluster_analysis.index:
            profile = cluster_analysis.loc[cluster]

            if profile['total_spent'] > cluster_analysis['total_spent'].mean():
                if profile['purchase_frequency'] > cluster_analysis['purchase_frequency'].mean():
                    recommendations[cluster] = "é«˜ä»·å€¼æ´»è·ƒç”¨æˆ· - æ¨èVIPæœåŠ¡"
                else:
                    recommendations[cluster] = "é«˜ä»·å€¼ä½é¢‘ç”¨æˆ· - æ¨èä¿ƒé”€æ´»åŠ¨"
            else:
                recommendations[cluster] = "ä½ä»·å€¼ç”¨æˆ· - æ¨èåŸºç¡€äº§å“"

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
def ecommerce_analysis_example():
    """ç”µå•†ç”¨æˆ·åˆ†æç¤ºä¾‹"""
# åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_users = 1000

    data = pd.DataFrame({
        'user_id': range(1, n_users + 1),
        'age': np.random.normal(35, 10, n_users).clip(18, 70),
        'total_spent': np.random.exponential(1000, n_users),
        'purchase_frequency': np.random.poisson(5, n_users),
        'activity_score': np.random.beta(2, 5, n_users) * 100
    })

# è¿›è¡Œåˆ†æ
    analyzer = EcommerceUserAnalysis()
    analyzer.data = data
    cluster_analysis = analyzer.user_segmentation()
    recommendations = analyzer.generate_recommendations()

    return analyzer, cluster_analysis, recommendations
```

## 3. é‡‘èé£é™©åˆ†æ

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np

class FinancialRiskAnalysis:
    def __init__(self):
        self.model = None

    def prepare_features(self, data):
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        feature_columns = [
            'age', 'income', 'credit_score', 'debt_ratio',
            'payment_history', 'employment_length', 'loan_amount'
        ]

        X = data[feature_columns].fillna(data[feature_columns].mean())
        y = data['default_risk']

        return X, y

    def train_risk_model(self, X, y):
        """è®­ç»ƒé£é™©é¢„æµ‹æ¨¡å‹"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X_train, y_train)

        y_pred = self.model.predict(X_test)
        print("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))

        return X_test, y_test, y_pred

    def risk_assessment(self, customer_data):
        """é£é™©è¯„ä¼°"""
        risk_prob = self.model.predict_proba(customer_data)[0, 1]

        if risk_prob < 0.3:
            risk_level = "ä½é£é™©"
        elif risk_prob < 0.7:
            risk_level = "ä¸­ç­‰é£é™©"
        else:
            risk_level = "é«˜é£é™©"

        return {
            'risk_probability': risk_prob,
            'risk_level': risk_level
        }

# ä½¿ç”¨ç¤ºä¾‹
def financial_risk_example():
    """é‡‘èé£é™©åˆ†æç¤ºä¾‹"""
# åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_customers = 2000

    data = pd.DataFrame({
        'age': np.random.normal(40, 12, n_customers).clip(18, 80),
        'income': np.random.exponential(50000, n_customers),
        'credit_score': np.random.normal(650, 100, n_customers).clip(300, 850),
        'debt_ratio': np.random.beta(2, 5, n_customers),
        'payment_history': np.random.normal(0.8, 0.2, n_customers).clip(0, 1),
        'employment_length': np.random.exponential(5, n_customers),
        'loan_amount': np.random.exponential(100000, n_customers)
    })

# ç”Ÿæˆé£é™©æ ‡ç­¾
    risk_score = (
        (data['age'] < 30) * 0.3 +
        (data['income'] < 30000) * 0.4 +
        (data['credit_score'] < 600) * 0.5 +
        np.random.normal(0, 0.1, n_customers)
    )
    data['default_risk'] = (risk_score > 0.5).astype(int)

# è¿›è¡Œåˆ†æ
    analyzer = FinancialRiskAnalysis()
    X, y = analyzer.prepare_features(data)
    X_test, y_test, y_pred = analyzer.train_risk_model(X, y)

    return analyzer, data
```

## 4. æœºå™¨å­¦ä¹ å®è·µæ¡ˆä¾‹

### 4.1. æ¨èç³»ç»Ÿå®ç°

```python
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

class RecommendationSystem:
    def __init__(self):
        self.user_item_matrix = None
        self.item_similarity_matrix = None

    def create_user_item_matrix(self, ratings_data):
        """åˆ›å»ºç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ"""
        self.user_item_matrix = ratings_data.pivot_table(
            index='user_id',
            columns='item_id',
            values='rating'
        ).fillna(0)

        return self.user_item_matrix

    def collaborative_filtering(self):
        """ååŒè¿‡æ»¤æ¨è"""
        self.item_similarity_matrix = cosine_similarity(self.user_item_matrix.T)
        return self.item_similarity_matrix

    def get_recommendations(self, user_id, n_recommendations=5):
        """è·å–æ¨è"""
        user_ratings = self.user_item_matrix.loc[user_id]
        rated_items = user_ratings[user_ratings > 0].index

        item_scores = {}
        for item_id in self.user_item_matrix.columns:
            if item_id not in rated_items:
                score = 0
                total_similarity = 0

                for rated_item in rated_items:
                    similarity = self.item_similarity_matrix[
                        self.user_item_matrix.columns.get_loc(item_id),
                        self.user_item_matrix.columns.get_loc(rated_item)
                    ]
                    score += similarity * user_ratings[rated_item]
                    total_similarity += abs(similarity)

                if total_similarity > 0:
                    item_scores[item_id] = score / total_similarity

        recommendations = sorted(item_scores.items(),
                               key=lambda x: x[1], reverse=True)
        return recommendations[:n_recommendations]

# ä½¿ç”¨ç¤ºä¾‹
def recommendation_system_example():
    """æ¨èç³»ç»Ÿç¤ºä¾‹"""
# åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_users = 100
    n_items = 50

    ratings_data = []
    for user_id in range(1, n_users + 1):
        for item_id in range(1, n_items + 1):
            if np.random.random() < 0.3:
                rating = np.random.randint(1, 6)
                ratings_data.append({
                    'user_id': user_id,
                    'item_id': item_id,
                    'rating': rating
                })

    ratings_df = pd.DataFrame(ratings_data)

# æ„å»ºæ¨èç³»ç»Ÿ
    recommender = RecommendationSystem()
    recommender.create_user_item_matrix(ratings_df)
    recommender.collaborative_filtering()

# è·å–æ¨è
    recommendations = recommender.get_recommendations(user_id=1, n_recommendations=5)
    print("æ¨èç‰©å“:")
    for item_id, score in recommendations:
        print(f"ç‰©å“ {item_id}: æ¨èåˆ†æ•° {score:.3f}")

    return recommender, recommendations
```

## 5. æ·±åº¦å­¦ä¹ å®è·µæ¡ˆä¾‹

### 5.1. å›¾åƒåˆ†ç±»æ¨¡å‹

```python
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

class ImageClassificationModel:
    def __init__(self, input_shape=(224, 224, 3), num_classes=10):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None

    def build_model(self):
        """æ„å»ºCNNæ¨¡å‹"""
        self.model = models.Sequential([
# å·ç§¯å±‚
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

# å…¨è¿æ¥å±‚
            layers.Flatten(),
            layers.Dropout(0.5),
            layers.Dense(512, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(self.num_classes, activation='softmax')
        ])

# ç¼–è¯‘æ¨¡å‹
        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        return self.model

    def train_model(self, train_data, validation_data, epochs=50):
        """è®­ç»ƒæ¨¡å‹"""
# æ—©åœæœºåˆ¶
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )

# è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            train_data,
            epochs=epochs,
            validation_data=validation_data,
            callbacks=[early_stopping]
        )

        return history

    def predict_image(self, image_path):
        """é¢„æµ‹å•å¼ å›¾ç‰‡"""
        from tensorflow.keras.preprocessing import image

        img = image.load_img(image_path, target_size=self.input_shape[:2])
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array /= 255.0

        predictions = self.model.predict(img_array)
        predicted_class = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class]

        return predicted_class, confidence

# ä½¿ç”¨ç¤ºä¾‹
def image_classification_example():
    """å›¾åƒåˆ†ç±»ç¤ºä¾‹"""
    model = ImageClassificationModel(input_shape=(224, 224, 3), num_classes=10)
    model.build_model()

    print("å›¾åƒåˆ†ç±»æ¨¡å‹å·²åˆ›å»º")
    print("è¯·æä¾›å®é™…çš„å›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒ")

    return model
```

## 6. è‡ªç„¶è¯­è¨€å¤„ç†å®è·µæ¡ˆä¾‹

### 6.1. æ–‡æœ¬æƒ…æ„Ÿåˆ†æ

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np
import re
import jieba

class SentimentAnalysis:
    def __init__(self):
        self.vectorizer = None
        self.model = None
        self.sentiment_labels = {0: 'è´Ÿé¢', 1: 'æ­£é¢'}

    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def tokenize_chinese(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        return ' '.join(jieba.cut(text))

    def prepare_data(self, data, text_column, label_column):
        """å‡†å¤‡æ•°æ®"""
        data['processed_text'] = data[text_column].apply(self.preprocess_text)
        data['tokenized_text'] = data['processed_text'].apply(self.tokenize_chinese)

        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        X = self.vectorizer.fit_transform(data['tokenized_text'])
        y = data[label_column]

        return X, y

    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X_train, y_train)

        y_pred = self.model.predict(X_test)
        print("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['è´Ÿé¢', 'æ­£é¢']))

        return X_test, y_test, y_pred

    def predict_sentiment(self, text):
        """é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿ"""
        processed_text = self.preprocess_text(text)
        tokenized_text = self.tokenize_chinese(processed_text)
        features = self.vectorizer.transform([tokenized_text])

        prediction = self.model.predict(features)[0]
        probability = self.model.predict_proba(features)[0]

        return {
            'sentiment': self.sentiment_labels[prediction],
            'confidence': max(probability)
        }

# ä½¿ç”¨ç¤ºä¾‹
def sentiment_analysis_example():
    """æƒ…æ„Ÿåˆ†æç¤ºä¾‹"""
# åˆ›å»ºç¤ºä¾‹æ•°æ®
    positive_texts = [
        "è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼Œæˆ‘å¾ˆæ»¡æ„ï¼",
        "æœåŠ¡æ€åº¦å¾ˆæ£’ï¼Œæ¨èè´­ä¹°",
        "è´¨é‡ä¸é”™ï¼Œå€¼å¾—æ¨è"
    ]

    negative_texts = [
        "è´¨é‡å¤ªå·®äº†ï¼Œä¸æ¨èè´­ä¹°",
        "æœåŠ¡æ€åº¦å¾ˆç³Ÿç³•ï¼Œå¾ˆå¤±æœ›",
        "ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†å¥½ä¹…"
    ]

    data = []
    for text in positive_texts:
        data.append({'text': text, 'sentiment': 1})
    for text in negative_texts:
        data.append({'text': text, 'sentiment': 0})

    df = pd.DataFrame(data)

# è¿›è¡Œæƒ…æ„Ÿåˆ†æ
    analyzer = SentimentAnalysis()
    X, y = analyzer.prepare_data(df, 'text', 'sentiment')
    X_test, y_test, y_pred = analyzer.train_model(X, y)

# æµ‹è¯•é¢„æµ‹
    test_text = "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼"
    result = analyzer.predict_sentiment(test_text)
    print(f"æ–‡æœ¬: {test_text}")
    print(f"æƒ…æ„Ÿ: {result['sentiment']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")

    return analyzer, df
```

## 7. æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†æ•°æ®ç§‘å­¦åœ¨å®é™…åº”ç”¨ä¸­çš„å…·ä½“æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬ï¼š

1. **æ•°æ®åˆ†æå®è·µ**ï¼šç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œé‡‘èé£é™©åˆ†æ
2. **æœºå™¨å­¦ä¹ å®è·µ**ï¼šæ¨èç³»ç»Ÿå®ç°
3. **æ·±åº¦å­¦ä¹ å®è·µ**ï¼šå›¾åƒåˆ†ç±»æ¨¡å‹
4. **è‡ªç„¶è¯­è¨€å¤„ç†å®è·µ**ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ

è¿™äº›æ¡ˆä¾‹å±•ç¤ºäº†æ•°æ®ç§‘å­¦åœ¨å„ä¸ªé¢†åŸŸçš„å®é™…åº”ç”¨ï¼Œä¸ºæ•°æ®ç§‘å­¦é¡¹ç›®æä¾›äº†å®ç”¨çš„å‚è€ƒå’ŒæŒ‡å¯¼ã€‚
