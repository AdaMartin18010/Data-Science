# 3.1.26 数据科学实践案例

## 1. 概述

本文档提供数据科学在实际应用中的具体案例，涵盖数据分析、机器学习、深度学习等各个方面的实践应用。

## 2. 数据分析实践案例

### 2.1 电商用户行为分析

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

class EcommerceUserAnalysis:
    def __init__(self):
        self.data = None
        
    def load_data(self, file_path):
        """加载电商用户数据"""
        self.data = pd.read_csv(file_path)
        return self.data
    
    def user_segmentation(self, n_clusters=4):
        """用户分群分析"""
        features = ['age', 'total_spent', 'purchase_frequency', 'activity_score']
        X = self.data[features].fillna(self.data[features].mean())
        
        # 标准化数据
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # K-means聚类
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(X_scaled)
        
        self.data['cluster'] = clusters
        
        # 分析各群特征
        cluster_analysis = self.data.groupby('cluster')[features].mean()
        print("用户分群特征分析:")
        print(cluster_analysis)
        
        return cluster_analysis
    
    def generate_recommendations(self):
        """生成推荐策略"""
        cluster_analysis = self.data.groupby('cluster').agg({
            'total_spent': 'mean',
            'purchase_frequency': 'mean'
        })
        
        recommendations = {}
        for cluster in cluster_analysis.index:
            profile = cluster_analysis.loc[cluster]
            
            if profile['total_spent'] > cluster_analysis['total_spent'].mean():
                if profile['purchase_frequency'] > cluster_analysis['purchase_frequency'].mean():
                    recommendations[cluster] = "高价值活跃用户 - 推荐VIP服务"
                else:
                    recommendations[cluster] = "高价值低频用户 - 推荐促销活动"
            else:
                recommendations[cluster] = "低价值用户 - 推荐基础产品"
        
        return recommendations

# 使用示例
def ecommerce_analysis_example():
    """电商用户分析示例"""
    # 创建示例数据
    np.random.seed(42)
    n_users = 1000
    
    data = pd.DataFrame({
        'user_id': range(1, n_users + 1),
        'age': np.random.normal(35, 10, n_users).clip(18, 70),
        'total_spent': np.random.exponential(1000, n_users),
        'purchase_frequency': np.random.poisson(5, n_users),
        'activity_score': np.random.beta(2, 5, n_users) * 100
    })
    
    # 进行分析
    analyzer = EcommerceUserAnalysis()
    analyzer.data = data
    cluster_analysis = analyzer.user_segmentation()
    recommendations = analyzer.generate_recommendations()
    
    return analyzer, cluster_analysis, recommendations
```

### 2.2 金融风险分析

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np

class FinancialRiskAnalysis:
    def __init__(self):
        self.model = None
        
    def prepare_features(self, data):
        """准备特征数据"""
        feature_columns = [
            'age', 'income', 'credit_score', 'debt_ratio',
            'payment_history', 'employment_length', 'loan_amount'
        ]
        
        X = data[feature_columns].fillna(data[feature_columns].mean())
        y = data['default_risk']
        
        return X, y
    
    def train_risk_model(self, X, y):
        """训练风险预测模型"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X_train, y_train)
        
        y_pred = self.model.predict(X_test)
        print("模型评估报告:")
        print(classification_report(y_test, y_pred))
        
        return X_test, y_test, y_pred
    
    def risk_assessment(self, customer_data):
        """风险评估"""
        risk_prob = self.model.predict_proba(customer_data)[0, 1]
        
        if risk_prob < 0.3:
            risk_level = "低风险"
        elif risk_prob < 0.7:
            risk_level = "中等风险"
        else:
            risk_level = "高风险"
        
        return {
            'risk_probability': risk_prob,
            'risk_level': risk_level
        }

# 使用示例
def financial_risk_example():
    """金融风险分析示例"""
    # 创建示例数据
    np.random.seed(42)
    n_customers = 2000
    
    data = pd.DataFrame({
        'age': np.random.normal(40, 12, n_customers).clip(18, 80),
        'income': np.random.exponential(50000, n_customers),
        'credit_score': np.random.normal(650, 100, n_customers).clip(300, 850),
        'debt_ratio': np.random.beta(2, 5, n_customers),
        'payment_history': np.random.normal(0.8, 0.2, n_customers).clip(0, 1),
        'employment_length': np.random.exponential(5, n_customers),
        'loan_amount': np.random.exponential(100000, n_customers)
    })
    
    # 生成风险标签
    risk_score = (
        (data['age'] < 30) * 0.3 +
        (data['income'] < 30000) * 0.4 +
        (data['credit_score'] < 600) * 0.5 +
        np.random.normal(0, 0.1, n_customers)
    )
    data['default_risk'] = (risk_score > 0.5).astype(int)
    
    # 进行分析
    analyzer = FinancialRiskAnalysis()
    X, y = analyzer.prepare_features(data)
    X_test, y_test, y_pred = analyzer.train_risk_model(X, y)
    
    return analyzer, data
```

## 3. 机器学习实践案例

### 3.1 推荐系统实现

```python
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

class RecommendationSystem:
    def __init__(self):
        self.user_item_matrix = None
        self.item_similarity_matrix = None
        
    def create_user_item_matrix(self, ratings_data):
        """创建用户-物品评分矩阵"""
        self.user_item_matrix = ratings_data.pivot_table(
            index='user_id', 
            columns='item_id', 
            values='rating'
        ).fillna(0)
        
        return self.user_item_matrix
    
    def collaborative_filtering(self):
        """协同过滤推荐"""
        self.item_similarity_matrix = cosine_similarity(self.user_item_matrix.T)
        return self.item_similarity_matrix
    
    def get_recommendations(self, user_id, n_recommendations=5):
        """获取推荐"""
        user_ratings = self.user_item_matrix.loc[user_id]
        rated_items = user_ratings[user_ratings > 0].index
        
        item_scores = {}
        for item_id in self.user_item_matrix.columns:
            if item_id not in rated_items:
                score = 0
                total_similarity = 0
                
                for rated_item in rated_items:
                    similarity = self.item_similarity_matrix[
                        self.user_item_matrix.columns.get_loc(item_id),
                        self.user_item_matrix.columns.get_loc(rated_item)
                    ]
                    score += similarity * user_ratings[rated_item]
                    total_similarity += abs(similarity)
                
                if total_similarity > 0:
                    item_scores[item_id] = score / total_similarity
        
        recommendations = sorted(item_scores.items(), 
                               key=lambda x: x[1], reverse=True)
        return recommendations[:n_recommendations]

# 使用示例
def recommendation_system_example():
    """推荐系统示例"""
    # 创建示例数据
    np.random.seed(42)
    n_users = 100
    n_items = 50
    
    ratings_data = []
    for user_id in range(1, n_users + 1):
        for item_id in range(1, n_items + 1):
            if np.random.random() < 0.3:
                rating = np.random.randint(1, 6)
                ratings_data.append({
                    'user_id': user_id,
                    'item_id': item_id,
                    'rating': rating
                })
    
    ratings_df = pd.DataFrame(ratings_data)
    
    # 构建推荐系统
    recommender = RecommendationSystem()
    recommender.create_user_item_matrix(ratings_df)
    recommender.collaborative_filtering()
    
    # 获取推荐
    recommendations = recommender.get_recommendations(user_id=1, n_recommendations=5)
    print("推荐物品:")
    for item_id, score in recommendations:
        print(f"物品 {item_id}: 推荐分数 {score:.3f}")
    
    return recommender, recommendations
```

## 4. 深度学习实践案例

### 4.1 图像分类模型

```python
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

class ImageClassificationModel:
    def __init__(self, input_shape=(224, 224, 3), num_classes=10):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
        
    def build_model(self):
        """构建CNN模型"""
        self.model = models.Sequential([
            # 卷积层
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),
            
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),
            
            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),
            
            # 全连接层
            layers.Flatten(),
            layers.Dropout(0.5),
            layers.Dense(512, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(self.num_classes, activation='softmax')
        ])
        
        # 编译模型
        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return self.model
    
    def train_model(self, train_data, validation_data, epochs=50):
        """训练模型"""
        # 早停机制
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # 训练模型
        history = self.model.fit(
            train_data,
            epochs=epochs,
            validation_data=validation_data,
            callbacks=[early_stopping]
        )
        
        return history
    
    def predict_image(self, image_path):
        """预测单张图片"""
        from tensorflow.keras.preprocessing import image
        
        img = image.load_img(image_path, target_size=self.input_shape[:2])
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array /= 255.0
        
        predictions = self.model.predict(img_array)
        predicted_class = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class]
        
        return predicted_class, confidence

# 使用示例
def image_classification_example():
    """图像分类示例"""
    model = ImageClassificationModel(input_shape=(224, 224, 3), num_classes=10)
    model.build_model()
    
    print("图像分类模型已创建")
    print("请提供实际的图像数据进行训练")
    
    return model
```

## 5. 自然语言处理实践案例

### 5.1 文本情感分析

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np
import re
import jieba

class SentimentAnalysis:
    def __init__(self):
        self.vectorizer = None
        self.model = None
        self.sentiment_labels = {0: '负面', 1: '正面'}
        
    def preprocess_text(self, text):
        """文本预处理"""
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def tokenize_chinese(self, text):
        """中文分词"""
        return ' '.join(jieba.cut(text))
    
    def prepare_data(self, data, text_column, label_column):
        """准备数据"""
        data['processed_text'] = data[text_column].apply(self.preprocess_text)
        data['tokenized_text'] = data['processed_text'].apply(self.tokenize_chinese)
        
        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        X = self.vectorizer.fit_transform(data['tokenized_text'])
        y = data[label_column]
        
        return X, y
    
    def train_model(self, X, y):
        """训练模型"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X_train, y_train)
        
        y_pred = self.model.predict(X_test)
        print("模型评估报告:")
        print(classification_report(y_test, y_pred, target_names=['负面', '正面']))
        
        return X_test, y_test, y_pred
    
    def predict_sentiment(self, text):
        """预测文本情感"""
        processed_text = self.preprocess_text(text)
        tokenized_text = self.tokenize_chinese(processed_text)
        features = self.vectorizer.transform([tokenized_text])
        
        prediction = self.model.predict(features)[0]
        probability = self.model.predict_proba(features)[0]
        
        return {
            'sentiment': self.sentiment_labels[prediction],
            'confidence': max(probability)
        }

# 使用示例
def sentiment_analysis_example():
    """情感分析示例"""
    # 创建示例数据
    positive_texts = [
        "这个产品非常好用，我很满意！",
        "服务态度很棒，推荐购买",
        "质量不错，值得推荐"
    ]
    
    negative_texts = [
        "质量太差了，不推荐购买",
        "服务态度很糟糕，很失望",
        "物流太慢了，等了好久"
    ]
    
    data = []
    for text in positive_texts:
        data.append({'text': text, 'sentiment': 1})
    for text in negative_texts:
        data.append({'text': text, 'sentiment': 0})
    
    df = pd.DataFrame(data)
    
    # 进行情感分析
    analyzer = SentimentAnalysis()
    X, y = analyzer.prepare_data(df, 'text', 'sentiment')
    X_test, y_test, y_pred = analyzer.train_model(X, y)
    
    # 测试预测
    test_text = "这个产品真的很棒，我很喜欢！"
    result = analyzer.predict_sentiment(test_text)
    print(f"文本: {test_text}")
    print(f"情感: {result['sentiment']}")
    print(f"置信度: {result['confidence']:.3f}")
    
    return analyzer, df
```

## 6. 总结

本文档提供了数据科学在实际应用中的具体案例，包括：

1. **数据分析实践**：电商用户行为分析和金融风险分析
2. **机器学习实践**：推荐系统实现
3. **深度学习实践**：图像分类模型
4. **自然语言处理实践**：文本情感分析

这些案例展示了数据科学在各个领域的实际应用，为数据科学项目提供了实用的参考和指导。
