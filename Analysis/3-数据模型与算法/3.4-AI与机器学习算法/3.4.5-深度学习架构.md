# 3.4.5 深度学习架构

## 概述

深度学习是基于多层人工神经网络的机器学习方法，通过多层非线性变换自动学习数据的层次化表示。
本文档从数学原理出发，系统阐述深度学习的理论基础、经典架构、现代前沿和工程实现。

## 目录

- [3.4.5 深度学习架构](#345-深度学习架构)
  - [概述](#概述)
  - [目录](#目录)
  - [神经网络基础理论](#神经网络基础理论)
    - [1.1 神经元模型](#11-神经元模型)
    - [1.2 万能逼近定理](#12-万能逼近定理)
    - [1.3 深度的重要性](#13-深度的重要性)
  - [前馈神经网络](#前馈神经网络)
    - [2.1 多层感知机（MLP）](#21-多层感知机mlp)
    - [2.2 反向传播算法](#22-反向传播算法)
  - [卷积神经网络](#卷积神经网络)
    - [3.1 卷积运算](#31-卷积运算)
    - [3.2 经典CNN架构](#32-经典cnn架构)
    - [3.3 注意力机制](#33-注意力机制)
  - [循环神经网络](#循环神经网络)
    - [4.1 标准RNN](#41-标准rnn)
    - [4.2 LSTM架构](#42-lstm架构)
    - [4.3 GRU简化设计](#43-gru简化设计)
  - [Transformer架构](#transformer架构)
    - [5.1 自注意力机制](#51-自注意力机制)
    - [5.2 多头注意力](#52-多头注意力)
    - [5.3 位置编码](#53-位置编码)
    - [5.4 Transformer块结构](#54-transformer块结构)
  - [生成模型](#生成模型)
    - [6.1 变分自编码器（VAE）](#61-变分自编码器vae)
    - [6.2 生成对抗网络（GAN）](#62-生成对抗网络gan)
    - [6.3 扩散模型](#63-扩散模型)
  - [优化理论与技术](#优化理论与技术)
    - [7.1 梯度下降变体](#71-梯度下降变体)
    - [7.2 学习率调度](#72-学习率调度)
    - [7.3 批归一化](#73-批归一化)
  - [正则化与泛化理论](#正则化与泛化理论)
    - [8.1 Dropout](#81-dropout)
    - [8.2 权重衰减](#82-权重衰减)
    - [8.3 泛化界](#83-泛化界)
  - [表示学习理论](#表示学习理论)
    - [9.1 特征学习](#91-特征学习)
    - [9.2 流形假设](#92-流形假设)
    - [9.3 信息瓶颈原理](#93-信息瓶颈原理)
  - [Rust实现示例](#rust实现示例)
    - [10.1 基础数据结构](#101-基础数据结构)
    - [10.2 神经网络层](#102-神经网络层)
    - [10.3 多层感知机](#103-多层感知机)
    - [10.4 卷积层实现](#104-卷积层实现)
    - [10.5 训练循环](#105-训练循环)
  - [本地跳转链接](#本地跳转链接)
  - [参考文献](#参考文献)
  - [多表征](#多表征)
  - [形式化语义](#形式化语义)
  - [形式化语法与证明](#形式化语法与证明)

## 神经网络基础理论

### 1.1 神经元模型

**人工神经元的数学描述**：

对于神经元 $i$，其输出为：
$$y_i = f\left(\sum_{j} w_{ij} x_j + b_i\right) = f(\mathbf{w}_i^T \mathbf{x} + b_i)$$

其中：

- $x_j$ 是输入信号
- $w_{ij}$ 是连接权重
- $b_i$ 是偏置
- $f(\cdot)$ 是激活函数

**常用激活函数**：

1. **Sigmoid**: $\sigma(z) = \frac{1}{1 + e^{-z}}$
   - 导数：$\sigma'(z) = \sigma(z)(1 - \sigma(z))$
   - 梯度消失问题

2. **ReLU**: $f(z) = \max(0, z)$
   - 导数：$f'(z) = \mathbb{I}(z > 0)$
   - 计算高效，缓解梯度消失

3. **Tanh**: $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
   - 导数：$\tanh'(z) = 1 - \tanh^2(z)$
   - 零中心化

### 1.2 万能逼近定理

**理论基础**：

对于紧集 $K \subset \mathbb{R}^n$ 上的连续函数 $f$，存在具有一个隐藏层的神经网络 $N_{\epsilon}$，使得：
$$\sup_{x \in K} |f(x) - N_{\epsilon}(x)| < \epsilon$$

这表明神经网络具有理论上的强大表达能力。

### 1.3 深度的重要性

**表示效率**：深层网络可以用更少的参数表示某些函数类：

- 层数为 $d$ 的网络可以用 $O(n)$ 个节点表示的函数
- 层数为 $d-1$ 的网络可能需要 $O(2^n)$ 个节点

## 前馈神经网络

### 2.1 多层感知机（MLP）

**前向传播**：

对于 $L$ 层网络：

$$
\begin{align}
\mathbf{z}^{(1)} &= \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)} \\
\mathbf{a}^{(1)} &= f(\mathbf{z}^{(1)}) \\
&\vdots \\
\mathbf{z}^{(L)} &= \mathbf{W}^{(L)} \mathbf{a}^{(L-1)} + \mathbf{b}^{(L)} \\
\mathbf{y} &= g(\mathbf{z}^{(L)})
\end{align}
$$

### 2.2 反向传播算法

**核心思想**：链式法则计算梯度

损失函数关于参数的梯度：
$$\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial z_i^{(l)}} \frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} = \delta_i^{(l)} a_j^{(l-1)}$$

其中误差项：
$$\delta_i^{(l)} = \frac{\partial \mathcal{L}}{\partial z_i^{(l)}} = \sum_k w_{ki}^{(l+1)} \delta_k^{(l+1)} f'(z_i^{(l)})$$

## 卷积神经网络

### 3.1 卷积运算

**离散卷积**：

对于输入 $I$ 和核 $K$：
$$(I * K)(i,j) = \sum_m \sum_n I(i-m, j-n) K(m,n)$$

**参数共享**：同一个核在整个输入上共享参数，大大减少参数数量。

### 3.2 经典CNN架构

**LeNet-5**：

- 卷积层 → 池化层 → 卷积层 → 池化层 → 全连接层

**AlexNet创新**：

- ReLU激活函数
- Dropout正则化
- GPU加速
- 数据增强

**VGG网络**：

- 小核大深度（3×3卷积核）
- 层数更深（VGG-16, VGG-19）

**ResNet残差连接**：

解决深度网络训练困难：
$$\mathbf{y} = F(\mathbf{x}, \{W_i\}) + \mathbf{x}$$

残差学习比直接学习原函数更容易。

### 3.3 注意力机制

**注意力权重计算**：

$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^T \exp(e_{ik})}$$

其中 $e_{ij} = a(s_{i-1}, h_j)$ 是注意力函数。

## 循环神经网络

### 4.1 标准RNN

**递归关系**：

$$
\begin{align}
\mathbf{h}_t &= f(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b}) \\
\mathbf{y}_t &= \mathbf{W}_y \mathbf{h}_t + \mathbf{b}_y
\end{align}
$$

**梯度消失问题**：

长序列中的梯度会指数衰减：
$$\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} = \prod_{i=k+1}^t \frac{\partial \mathbf{h}_i}{\partial \mathbf{h}_{i-1}}$$

### 4.2 LSTM架构

**门控机制**：

- 遗忘门：$\mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)$
- 输入门：$\mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)$
- 输出门：$\mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)$

**细胞状态更新**：

$$
\begin{align}
\tilde{\mathbf{C}}_t &= \tanh(\mathbf{W}_C [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_C) \\
\mathbf{C}_t &= \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t \\
\mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{C}_t)
\end{align}
$$

### 4.3 GRU简化设计

**更新门和重置门**：

$$
\begin{align}
\mathbf{z}_t &= \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\mathbf{r}_t &= \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\tilde{\mathbf{h}}_t &= \tanh(\mathbf{W} [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\mathbf{h}_t &= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
\end{align}
$$

## Transformer架构

### 5.1 自注意力机制

**缩放点积注意力**：

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$

其中：

- $\mathbf{Q}$ = 查询矩阵
- $\mathbf{K}$ = 键矩阵  
- $\mathbf{V}$ = 值矩阵
- $d_k$ = 键向量维度

### 5.2 多头注意力

**并行注意力头**：

$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, ..., \text{head}_h)\mathbf{W}^O$$

其中：
$$\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)$$

### 5.3 位置编码

**正弦位置编码**：

$$
\begin{align}
PE_{(pos, 2i)} &= \sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} &= \cos(pos/10000^{2i/d_{model}})
\end{align}
$$

### 5.4 Transformer块结构

$$
\begin{align}
\mathbf{Z} &= \text{LayerNorm}(\mathbf{X} + \text{MultiHead}(\mathbf{X}, \mathbf{X}, \mathbf{X})) \\
\text{Output} &= \text{LayerNorm}(\mathbf{Z} + \text{FFN}(\mathbf{Z}))
\end{align}
$$

## 生成模型

### 6.1 变分自编码器（VAE）

**编码器和解码器**：

- 编码器：$q_\phi(\mathbf{z}|\mathbf{x})$
- 解码器：$p_\theta(\mathbf{x}|\mathbf{z})$

**变分下界**：

$$\log p(\mathbf{x}) \geq \mathcal{L}(\theta, \phi; \mathbf{x}) = -\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) + \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]$$

### 6.2 生成对抗网络（GAN）

**对抗训练**：

- 生成器：$G: \mathcal{Z} \rightarrow \mathcal{X}$
- 判别器：$D: \mathcal{X} \rightarrow [0,1]$

**目标函数**：

$$\min_G \max_D V(D,G) = \mathbb{E}_{\mathbf{x} \sim p_{data}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - D(G(\mathbf{z})))]$$

### 6.3 扩散模型

**前向扩散过程**：

$$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$$

**逆向生成过程**：

$$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))$$

## 优化理论与技术

### 7.1 梯度下降变体

**SGD动量**：

$$
\begin{align}
\mathbf{v}_t &= \gamma \mathbf{v}_{t-1} + \eta \nabla_\theta J(\theta) \\
\theta &= \theta - \mathbf{v}_t
\end{align}
$$

**Adam优化器**：

$$
\begin{align}
\mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1)\mathbf{g}_t \\
\mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2)\mathbf{g}_t^2 \\
\hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1-\beta_1^t} \\
\hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1-\beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}\hat{\mathbf{m}}_t
\end{align}
$$

### 7.2 学习率调度

**余弦退火**：

$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{T_{cur}}{T_{max}}\pi))$$

### 7.3 批归一化

**批归一化变换**：

$$\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B^{(k)}}{\sqrt{\sigma_B^{(k)2} + \epsilon}}$$

$$y^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{(k)}$$

## 正则化与泛化理论

### 8.1 Dropout

**训练时随机失活**：

$$\mathbf{r} \sim \text{Bernoulli}(p)$$
$$\tilde{\mathbf{h}} = \mathbf{r} \odot \mathbf{h}$$

**推理时缩放**：

$$\mathbf{h}_{test} = p \cdot \mathbf{h}$$

### 8.2 权重衰减

**L2正则化**：

$$\mathcal{L}_{total} = \mathcal{L}_{data} + \lambda \sum_i w_i^2$$

### 8.3 泛化界

**Rademacher复杂度**：

$$\mathcal{R}_S(\mathcal{F}) = \mathbb{E}_{\boldsymbol{\sigma}} \left[ \sup_{f \in \mathcal{F}} \frac{1}{m} \sum_{i=1}^m \sigma_i f(\mathbf{x}_i) \right]$$

**泛化误差界**：

$$\mathbb{E}[R(f)] - R_S(f) \leq 2\mathcal{R}_S(\mathcal{F}) + \sqrt{\frac{\log(2/\delta)}{2m}}$$

## 表示学习理论

### 9.1 特征学习

**分层表示**：

深度网络学习分层特征：

- 低层：边缘、纹理
- 中层：部件、模式
- 高层：语义概念

### 9.2 流形假设

**数据流形**：

高维数据分布在低维流形上：
$$\mathcal{M} = \{f(\mathbf{z}) : \mathbf{z} \in \mathcal{Z}\}$$

其中 $f: \mathcal{Z} \rightarrow \mathcal{X}$ 是非线性映射。

### 9.3 信息瓶颈原理

**信息瓶颈目标**：

$$\min_{p(t|x)} I(X; T) - \beta I(T; Y)$$

平衡压缩和预测能力。

## Rust实现示例

### 10.1 基础数据结构

```rust
use ndarray::{Array2, Array1, Axis};
use rand::Rng;
use std::collections::HashMap;

# [derive(Debug, Clone)]
pub struct Tensor {
    data: Array2<f64>,
    requires_grad: bool,
    grad: Option<Array2<f64>>,
}

impl Tensor {
    pub fn new(data: Array2<f64>) -> Self {
        Self {
            data,
            requires_grad: false,
            grad: None,
        }
    }

    pub fn zeros(shape: (usize, usize)) -> Self {
        Self::new(Array2::zeros(shape))
    }

    pub fn random(shape: (usize, usize)) -> Self {
        let mut rng = rand::thread_rng();
        let data = Array2::from_shape_fn(shape, |_| rng.gen_range(-1.0..1.0));
        Self::new(data)
    }

    pub fn shape(&self) -> (usize, usize) {
        self.data.dim()
    }

    pub fn matmul(&self, other: &Tensor) -> Tensor {
        let result = self.data.dot(&other.data);
        Tensor::new(result)
    }
}

# [derive(Debug, Clone)]
pub enum ActivationFunction {
    ReLU,
    Sigmoid,
    Tanh,
    Softmax,
}

impl ActivationFunction {
    pub fn forward(&self, x: &Array2<f64>) -> Array2<f64> {
        match self {
            ActivationFunction::ReLU => x.map(|&v| v.max(0.0)),
            ActivationFunction::Sigmoid => x.map(|&v| 1.0 / (1.0 + (-v).exp())),
            ActivationFunction::Tanh => x.map(|&v| v.tanh()),
            ActivationFunction::Softmax => {
                let mut result = x.clone();
                for mut row in result.axis_iter_mut(Axis(0)) {
                    let max_val = row.fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                    row.map_inplace(|v| *v = (*v - max_val).exp());
                    let sum: f64 = row.sum();
                    row.map_inplace(|v| *v /= sum);
                }
                result
            }
        }
    }

    pub fn backward(&self, x: &Array2<f64>, grad_output: &Array2<f64>) -> Array2<f64> {
        match self {
            ActivationFunction::ReLU => {
                x.map(|&v| if v > 0.0 { 1.0 } else { 0.0 }) * grad_output
            },
            ActivationFunction::Sigmoid => {
                let sigmoid = self.forward(x);
                &sigmoid * &(1.0 - &sigmoid) * grad_output
            },
            ActivationFunction::Tanh => {
                let tanh = self.forward(x);
                (1.0 - &tanh * &tanh) * grad_output
            },
            ActivationFunction::Softmax => {
                // Simplified softmax gradient
                grad_output.clone()
            }
        }
    }
}
```

### 10.2 神经网络层

```rust
# [derive(Debug)]
pub struct DenseLayer {
    weights: Tensor,
    bias: Tensor,
    activation: ActivationFunction,
    input_cache: Option<Array2<f64>>,
    pre_activation_cache: Option<Array2<f64>>,
}

impl DenseLayer {
    pub fn new(input_size: usize, output_size: usize, activation: ActivationFunction) -> Self {
        let scale = (2.0 / input_size as f64).sqrt();
        let weights = Tensor::random((input_size, output_size));
        let weights_data = weights.data.map(|&x| x * scale);

        Self {
            weights: Tensor::new(weights_data),
            bias: Tensor::zeros((1, output_size)),
            activation,
            input_cache: None,
            pre_activation_cache: None,
        }
    }

    pub fn forward(&mut self, input: &Array2<f64>) -> Array2<f64> {
        // 缓存输入用于反向传播
        self.input_cache = Some(input.clone());

        // 线性变换：output = input * weights + bias
        let linear_output = input.dot(&self.weights.data) + &self.bias.data;
        self.pre_activation_cache = Some(linear_output.clone());

        // 激活函数
        self.activation.forward(&linear_output)
    }

    pub fn backward(&self, grad_output: &Array2<f64>) -> (Array2<f64>, Array2<f64>, Array2<f64>) {
        let input = self.input_cache.as_ref().unwrap();
        let pre_activation = self.pre_activation_cache.as_ref().unwrap();

        // 激活函数的梯度
        let grad_pre_activation = self.activation.backward(pre_activation, grad_output);

        // 权重梯度：input^T * grad_pre_activation
        let grad_weights = input.t().dot(&grad_pre_activation);

        // 偏置梯度：沿批次维度求和
        let grad_bias = grad_pre_activation.sum_axis(Axis(0)).insert_axis(Axis(0));

        // 输入梯度：grad_pre_activation * weights^T
        let grad_input = grad_pre_activation.dot(&self.weights.data.t());

        (grad_weights, grad_bias, grad_input)
    }

    pub fn update_weights(&mut self, grad_weights: &Array2<f64>, grad_bias: &Array2<f64>, learning_rate: f64) {
        self.weights.data = &self.weights.data - &(grad_weights * learning_rate);
        self.bias.data = &self.bias.data - &(grad_bias * learning_rate);
    }
}
```

### 10.3 多层感知机

```rust
pub struct MLP {
    layers: Vec<DenseLayer>,
    learning_rate: f64,
}

impl MLP {
    pub fn new(layer_sizes: &[usize], activations: Vec<ActivationFunction>, learning_rate: f64) -> Self {
        let mut layers = Vec::new();

        for i in 0..layer_sizes.len() - 1 {
            let layer = DenseLayer::new(
                layer_sizes[i],
                layer_sizes[i + 1],
                activations[i].clone()
            );
            layers.push(layer);
        }

        Self {
            layers,
            learning_rate,
        }
    }

    pub fn forward(&mut self, input: &Array2<f64>) -> Array2<f64> {
        let mut output = input.clone();

        for layer in &mut self.layers {
            output = layer.forward(&output);
        }

        output
    }

    pub fn backward(&mut self, grad_output: &Array2<f64>) {
        let mut current_grad = grad_output.clone();

        // 反向遍历所有层
        for layer in self.layers.iter_mut().rev() {
            let (grad_weights, grad_bias, grad_input) = layer.backward(&current_grad);

            // 更新权重
            layer.update_weights(&grad_weights, &grad_bias, self.learning_rate);

            // 传递梯度到前一层
            current_grad = grad_input;
        }
    }

    pub fn train_step(&mut self, input: &Array2<f64>, target: &Array2<f64>) -> f64 {
        // 前向传播
        let output = self.forward(input);

        // 计算损失（MSE）
        let diff = &output - target;
        let loss = (&diff * &diff).mean().unwrap();

        // 反向传播
        let grad_output = 2.0 * &diff / (diff.len() as f64);
        self.backward(&grad_output);

        loss
    }
}
```

### 10.4 卷积层实现

```rust
# [derive(Debug)]
pub struct Conv2D {
    filters: Array2<f64>,
    bias: Array1<f64>,
    stride: usize,
    padding: usize,
    input_cache: Option<Array2<f64>>,
}

impl Conv2D {
    pub fn new(
        in_channels: usize,
        out_channels: usize,
        kernel_size: usize,
        stride: usize,
        padding: usize
    ) -> Self {
        let filter_shape = (out_channels, in_channels * kernel_size * kernel_size);
        let scale = (2.0 / (in_channels * kernel_size * kernel_size) as f64).sqrt();

        let mut rng = rand::thread_rng();
        let filters = Array2::from_shape_fn(filter_shape, |_| {
            rng.gen_range(-scale..scale)
        });

        Self {
            filters,
            bias: Array1::zeros(out_channels),
            stride,
            padding,
            input_cache: None,
        }
    }

    pub fn forward(&mut self, input: &Array2<f64>) -> Array2<f64> {
        self.input_cache = Some(input.clone());

        // 简化的2D卷积实现
        // 实际实现需要处理多维张量和im2col转换
        let output_size = (input.nrows() - self.filters.ncols() + 2 * self.padding) / self.stride + 1;
        let mut output = Array2::zeros((output_size, self.filters.nrows()));

        // 这里是简化版本，实际需要完整的卷积实现
        output
    }
}
```

### 10.5 训练循环

```rust
pub struct Trainer {
    model: MLP,
    batch_size: usize,
    epochs: usize,
}

impl Trainer {
    pub fn new(model: MLP, batch_size: usize, epochs: usize) -> Self {
        Self {
            model,
            batch_size,
            epochs,
        }
    }

    pub fn train(&mut self, train_data: &[(Array2<f64>, Array2<f64>)]) {
        for epoch in 0..self.epochs {
            let mut total_loss = 0.0;
            let mut batch_count = 0;

            // 小批次训练
            for batch in train_data.chunks(self.batch_size) {
                let mut batch_input = Vec::new();
                let mut batch_target = Vec::new();

                for (input, target) in batch {
                    batch_input.push(input.clone());
                    batch_target.push(target.clone());
                }

                // 合并批次数据
                if let (Some(first_input), Some(first_target)) =
                   (batch_input.first(), batch_target.first()) {
                    let batch_input_tensor = Array2::from_shape_vec(
                        (batch_input.len(), first_input.len()),
                        batch_input.into_iter().flatten().collect()
                    ).unwrap();

                    let batch_target_tensor = Array2::from_shape_vec(
                        (batch_target.len(), first_target.len()),
                        batch_target.into_iter().flatten().collect()
                    ).unwrap();

                    let loss = self.model.train_step(&batch_input_tensor, &batch_target_tensor);
                    total_loss += loss;
                    batch_count += 1;
                }
            }

            let avg_loss = total_loss / batch_count as f64;
            println!("Epoch {}: Average Loss = {:.6}", epoch + 1, avg_loss);
        }
    }

    pub fn evaluate(&mut self, test_data: &[(Array2<f64>, Array2<f64>)]) -> f64 {
        let mut total_loss = 0.0;
        let mut count = 0;

        for (input, target) in test_data {
            let output = self.model.forward(input);
            let diff = &output - target;
            let loss = (&diff * &diff).mean().unwrap();
            total_loss += loss;
            count += 1;
        }

        total_loss / count as f64
    }
}

// 使用示例
pub fn example_usage() {
    // 创建简单的MLP模型
    let layer_sizes = vec![784, 128, 64, 10]; // MNIST分类
    let activations = vec![
        ActivationFunction::ReLU,
        ActivationFunction::ReLU,
        ActivationFunction::Softmax,
    ];

    let model = MLP::new(&layer_sizes, activations, 0.001);
    let mut trainer = Trainer::new(model, 32, 100);

    // 准备训练数据（这里是示例数据）
    let mut train_data = Vec::new();
    for _ in 0..1000 {
        let input = Array2::random((1, 784));
        let target = Array2::random((1, 10));
        train_data.push((input, target));
    }

    // 训练模型
    trainer.train(&train_data);

    // 评估模型
    let test_loss = trainer.evaluate(&train_data[0..100]);
    println!("Test Loss: {:.6}", test_loss);
}
```

## 本地跳转链接

- [类型理论基础](../../2-形式科学理论/2.1-类型理论/2.1.1-类型理论基础.md)
- [机器学习基础理论](./3.4.1-机器学习基础理论.md)
- [监督学习算法](./3.4.2-监督学习算法.md)
- [强化学习理论](./3.4.4-强化学习理论.md)
- [优化理论与算法](./3.4.6-优化理论与算法.md)

## 参考文献

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.
3. Vaswani, A., et al. (2017). Attention is all you need. NIPS.
4. He, K., et al. (2016). Deep residual learning for image recognition. CVPR.
5. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation.

## 多表征

深度学习架构支持多种表征方式，包括：

- 符号表征（网络结构、激活函数、损失函数、算法伪代码等）
- 图结构（神经网络结构图、层次结构图、数据流图等）
- 向量/张量（权重矩阵、特征向量、嵌入）
- 自然语言（定义、注释、描述）
- 图像/可视化（网络结构图、训练过程可视化、特征可视化等）
这些表征可互映，提升深度学习架构的表达力。

## 形式化语义

- 语义域：$D$，如输入空间、输出空间、参数空间、网络空间、损失空间
- 解释函数：$I: S \to D$，将符号/结构映射到具体语义对象
- 语义一致性：每个网络结构/层/激活函数/损失函数/公式在$D$中有明确定义

## 形式化语法与证明

- 语法规则：如网络结构定义、层次规则、激活函数、损失函数、训练规则、推理规则、约束条件
- **定理**：深度学习架构的语法系统具一致性与可扩展性。
- **证明**：由网络结构、层次规则、激活与损失函数、推理规则递归定义，保证系统一致与可扩展。
