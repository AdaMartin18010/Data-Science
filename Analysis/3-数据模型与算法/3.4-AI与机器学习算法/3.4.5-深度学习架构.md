# 3.4.5 æ·±åº¦å­¦ä¹ æ¶æ„

## ğŸ“‘ ç›®å½•

- [3.4.5 æ·±åº¦å­¦ä¹ æ¶æ„](#345-æ·±åº¦å­¦ä¹ æ¶æ„)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [3. ç¥ç»ç½‘ç»œåŸºç¡€ç†è®º](#3-ç¥ç»ç½‘ç»œåŸºç¡€ç†è®º)
    - [3.1. ç¥ç»å…ƒæ¨¡å‹](#31-ç¥ç»å…ƒæ¨¡å‹)
    - [3.2. ä¸‡èƒ½é€¼è¿‘å®šç†](#32-ä¸‡èƒ½é€¼è¿‘å®šç†)
    - [3.3. æ·±åº¦çš„é‡è¦æ€§](#33-æ·±åº¦çš„é‡è¦æ€§)
  - [4. å‰é¦ˆç¥ç»ç½‘ç»œ](#4-å‰é¦ˆç¥ç»ç½‘ç»œ)
    - [4.1. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰](#41-å¤šå±‚æ„ŸçŸ¥æœºmlp)
    - [4.2. åå‘ä¼ æ’­ç®—æ³•](#42-åå‘ä¼ æ’­ç®—æ³•)
  - [5. å·ç§¯ç¥ç»ç½‘ç»œ](#5-å·ç§¯ç¥ç»ç½‘ç»œ)
    - [5.1. å·ç§¯è¿ç®—](#51-å·ç§¯è¿ç®—)
    - [5.2. ç»å…¸CNNæ¶æ„](#52-ç»å…¸cnnæ¶æ„)
    - [5.3. æ³¨æ„åŠ›æœºåˆ¶](#53-æ³¨æ„åŠ›æœºåˆ¶)
  - [6. å¾ªç¯ç¥ç»ç½‘ç»œ](#6-å¾ªç¯ç¥ç»ç½‘ç»œ)
    - [6.1. æ ‡å‡†RNN](#61-æ ‡å‡†rnn)
    - [6.2. LSTMæ¶æ„](#62-lstmæ¶æ„)
    - [6.3. GRUç®€åŒ–è®¾è®¡](#63-gruç®€åŒ–è®¾è®¡)
  - [7. Transformeræ¶æ„](#7-transformeræ¶æ„)
    - [7.1. è‡ªæ³¨æ„åŠ›æœºåˆ¶](#71-è‡ªæ³¨æ„åŠ›æœºåˆ¶)
    - [7.2. å¤šå¤´æ³¨æ„åŠ›](#72-å¤šå¤´æ³¨æ„åŠ›)
    - [7.3. ä½ç½®ç¼–ç ](#73-ä½ç½®ç¼–ç )
    - [7.4. Transformerå—ç»“æ„](#74-transformerå—ç»“æ„)
  - [8. ç”Ÿæˆæ¨¡å‹](#8-ç”Ÿæˆæ¨¡å‹)
    - [8.1. å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰](#81-å˜åˆ†è‡ªç¼–ç å™¨vae)
    - [8.2. ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰](#82-ç”Ÿæˆå¯¹æŠ—ç½‘ç»œgan)
    - [8.3. æ‰©æ•£æ¨¡å‹](#83-æ‰©æ•£æ¨¡å‹)
  - [9. ä¼˜åŒ–ç†è®ºä¸æŠ€æœ¯](#9-ä¼˜åŒ–ç†è®ºä¸æŠ€æœ¯)
    - [9.1. æ¢¯åº¦ä¸‹é™å˜ä½“](#91-æ¢¯åº¦ä¸‹é™å˜ä½“)
    - [9.2. å­¦ä¹ ç‡è°ƒåº¦](#92-å­¦ä¹ ç‡è°ƒåº¦)
    - [9.3. æ‰¹å½’ä¸€åŒ–](#93-æ‰¹å½’ä¸€åŒ–)
  - [10. æ­£åˆ™åŒ–ä¸æ³›åŒ–ç†è®º](#10-æ­£åˆ™åŒ–ä¸æ³›åŒ–ç†è®º)
    - [10.1. Dropout](#101-dropout)
    - [10.2. æƒé‡è¡°å‡](#102-æƒé‡è¡°å‡)
    - [10.3. æ³›åŒ–ç•Œ](#103-æ³›åŒ–ç•Œ)
  - [11. è¡¨ç¤ºå­¦ä¹ ç†è®º](#11-è¡¨ç¤ºå­¦ä¹ ç†è®º)
    - [11.1. ç‰¹å¾å­¦ä¹ ](#111-ç‰¹å¾å­¦ä¹ )
    - [11.2. æµå½¢å‡è®¾](#112-æµå½¢å‡è®¾)
    - [11.3. ä¿¡æ¯ç“¶é¢ˆåŸç†](#113-ä¿¡æ¯ç“¶é¢ˆåŸç†)
  - [12. Rustå®ç°ç¤ºä¾‹](#12-rustå®ç°ç¤ºä¾‹)
    - [12.1. åŸºç¡€æ•°æ®ç»“æ„](#121-åŸºç¡€æ•°æ®ç»“æ„)
  - [13. ç¥ç»ç½‘ç»œå±‚](#13-ç¥ç»ç½‘ç»œå±‚)
  - [14. å¤šå±‚æ„ŸçŸ¥æœº](#14-å¤šå±‚æ„ŸçŸ¥æœº)
    - [14.1. å·ç§¯å±‚å®ç°](#141-å·ç§¯å±‚å®ç°)
  - [15. è®­ç»ƒå¾ªç¯](#15-è®­ç»ƒå¾ªç¯)
  - [16. æœ¬åœ°è·³è½¬é“¾æ¥](#16-æœ¬åœ°è·³è½¬é“¾æ¥)
  - [17. å‚è€ƒæ–‡çŒ®](#17-å‚è€ƒæ–‡çŒ®)
  - [18. å¤šè¡¨å¾](#18-å¤šè¡¨å¾)
  - [19. å½¢å¼åŒ–è¯­ä¹‰](#19-å½¢å¼åŒ–è¯­ä¹‰)
  - [20. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜](#20-å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜)

---


## 1. æ¦‚è¿°

æ·±åº¦å­¦ä¹ æ˜¯åŸºäºå¤šå±‚äººå·¥ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢è‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚
æœ¬æ–‡æ¡£ä»æ•°å­¦åŸç†å‡ºå‘ï¼Œç³»ç»Ÿé˜è¿°æ·±åº¦å­¦ä¹ çš„ç†è®ºåŸºç¡€ã€ç»å…¸æ¶æ„ã€ç°ä»£å‰æ²¿å’Œå·¥ç¨‹å®ç°ã€‚

## 3. ç¥ç»ç½‘ç»œåŸºç¡€ç†è®º

### 3.1. ç¥ç»å…ƒæ¨¡å‹

**äººå·¥ç¥ç»å…ƒçš„æ•°å­¦æè¿°**ï¼š

å¯¹äºç¥ç»å…ƒ $i$ï¼Œå…¶è¾“å‡ºä¸ºï¼š
$$y_i = f\left(\sum_{j} w_{ij} x_j + b_i\right) = f(\mathbf{w}_i^T \mathbf{x} + b_i)$$

å…¶ä¸­ï¼š

- $x_j$ æ˜¯è¾“å…¥ä¿¡å·
- $w_{ij}$ æ˜¯è¿æ¥æƒé‡
- $b_i$ æ˜¯åç½®
- $f(\cdot)$ æ˜¯æ¿€æ´»å‡½æ•°

**å¸¸ç”¨æ¿€æ´»å‡½æ•°**ï¼š

1. **Sigmoid**: $\sigma(z) = \frac{1}{1 + e^{-z}}$
   - å¯¼æ•°ï¼š$\sigma'(z) = \sigma(z)(1 - \sigma(z))$
   - æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

2. **ReLU**: $f(z) = \max(0, z)$
   - å¯¼æ•°ï¼š$f'(z) = \mathbb{I}(z > 0)$
   - è®¡ç®—é«˜æ•ˆï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±

3. **Tanh**: $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
   - å¯¼æ•°ï¼š$\tanh'(z) = 1 - \tanh^2(z)$
   - é›¶ä¸­å¿ƒåŒ–

### 3.2. ä¸‡èƒ½é€¼è¿‘å®šç†

**ç†è®ºåŸºç¡€**ï¼š

å¯¹äºç´§é›† $K \subset \mathbb{R}^n$ ä¸Šçš„è¿ç»­å‡½æ•° $f$ï¼Œå­˜åœ¨å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ $N_{\epsilon}$ï¼Œä½¿å¾—ï¼š
$$\sup_{x \in K} |f(x) - N_{\epsilon}(x)| < \epsilon$$

è¿™è¡¨æ˜ç¥ç»ç½‘ç»œå…·æœ‰ç†è®ºä¸Šçš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ã€‚

### 3.3. æ·±åº¦çš„é‡è¦æ€§

**è¡¨ç¤ºæ•ˆç‡**ï¼šæ·±å±‚ç½‘ç»œå¯ä»¥ç”¨æ›´å°‘çš„å‚æ•°è¡¨ç¤ºæŸäº›å‡½æ•°ç±»ï¼š

- å±‚æ•°ä¸º $d$ çš„ç½‘ç»œå¯ä»¥ç”¨ $O(n)$ ä¸ªèŠ‚ç‚¹è¡¨ç¤ºçš„å‡½æ•°
- å±‚æ•°ä¸º $d-1$ çš„ç½‘ç»œå¯èƒ½éœ€è¦ $O(2^n)$ ä¸ªèŠ‚ç‚¹

## 4. å‰é¦ˆç¥ç»ç½‘ç»œ

### 4.1. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

**å‰å‘ä¼ æ’­**ï¼š

å¯¹äº $L$ å±‚ç½‘ç»œï¼š

$$
\begin{align}
\mathbf{z}^{(1)} &= \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)} \\
\mathbf{a}^{(1)} &= f(\mathbf{z}^{(1)}) \\
&\vdots \\
\mathbf{z}^{(L)} &= \mathbf{W}^{(L)} \mathbf{a}^{(L-1)} + \mathbf{b}^{(L)} \\
\mathbf{y} &= g(\mathbf{z}^{(L)})
\end{align}
$$

### 4.2. åå‘ä¼ æ’­ç®—æ³•

**æ ¸å¿ƒæ€æƒ³**ï¼šé“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦

æŸå¤±å‡½æ•°å…³äºå‚æ•°çš„æ¢¯åº¦ï¼š
$$\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \frac{\partial \mathcal{L}}{\partial z_i^{(l)}} \frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}} = \delta_i^{(l)} a_j^{(l-1)}$$

å…¶ä¸­è¯¯å·®é¡¹ï¼š
$$\delta_i^{(l)} = \frac{\partial \mathcal{L}}{\partial z_i^{(l)}} = \sum_k w_{ki}^{(l+1)} \delta_k^{(l+1)} f'(z_i^{(l)})$$

## 5. å·ç§¯ç¥ç»ç½‘ç»œ

### 5.1. å·ç§¯è¿ç®—

**ç¦»æ•£å·ç§¯**ï¼š

å¯¹äºè¾“å…¥ $I$ å’Œæ ¸ $K$ï¼š
$$(I * K)(i,j) = \sum_m \sum_n I(i-m, j-n) K(m,n)$$

**å‚æ•°å…±äº«**ï¼šåŒä¸€ä¸ªæ ¸åœ¨æ•´ä¸ªè¾“å…¥ä¸Šå…±äº«å‚æ•°ï¼Œå¤§å¤§å‡å°‘å‚æ•°æ•°é‡ã€‚

### 5.2. ç»å…¸CNNæ¶æ„

**LeNet-5**ï¼š

- å·ç§¯å±‚ â†’ æ± åŒ–å±‚ â†’ å·ç§¯å±‚ â†’ æ± åŒ–å±‚ â†’ å…¨è¿æ¥å±‚

**AlexNetåˆ›æ–°**ï¼š

- ReLUæ¿€æ´»å‡½æ•°
- Dropoutæ­£åˆ™åŒ–
- GPUåŠ é€Ÿ
- æ•°æ®å¢å¼º

**VGGç½‘ç»œ**ï¼š

- å°æ ¸å¤§æ·±åº¦ï¼ˆ3Ã—3å·ç§¯æ ¸ï¼‰
- å±‚æ•°æ›´æ·±ï¼ˆVGG-16, VGG-19ï¼‰

**ResNetæ®‹å·®è¿æ¥**ï¼š

è§£å†³æ·±åº¦ç½‘ç»œè®­ç»ƒå›°éš¾ï¼š
$$\mathbf{y} = F(\mathbf{x}, \{W_i\}) + \mathbf{x}$$

æ®‹å·®å­¦ä¹ æ¯”ç›´æ¥å­¦ä¹ åŸå‡½æ•°æ›´å®¹æ˜“ã€‚

### 5.3. æ³¨æ„åŠ›æœºåˆ¶

**æ³¨æ„åŠ›æƒé‡è®¡ç®—**ï¼š

$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^T \exp(e_{ik})}$$

å…¶ä¸­ $e_{ij} = a(s_{i-1}, h_j)$ æ˜¯æ³¨æ„åŠ›å‡½æ•°ã€‚

## 6. å¾ªç¯ç¥ç»ç½‘ç»œ

### 6.1. æ ‡å‡†RNN

**é€’å½’å…³ç³»**ï¼š

$$
\begin{align}
\mathbf{h}_t &= f(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b}) \\
\mathbf{y}_t &= \mathbf{W}_y \mathbf{h}_t + \mathbf{b}_y
\end{align}
$$

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼š

é•¿åºåˆ—ä¸­çš„æ¢¯åº¦ä¼šæŒ‡æ•°è¡°å‡ï¼š
$$\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} = \prod_{i=k+1}^t \frac{\partial \mathbf{h}_i}{\partial \mathbf{h}_{i-1}}$$

### 6.2. LSTMæ¶æ„

**é—¨æ§æœºåˆ¶**ï¼š

- é—å¿˜é—¨ï¼š$\mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)$
- è¾“å…¥é—¨ï¼š$\mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)$
- è¾“å‡ºé—¨ï¼š$\mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)$

**ç»†èƒçŠ¶æ€æ›´æ–°**ï¼š

$$
\begin{align}
\tilde{\mathbf{C}}_t &= \tanh(\mathbf{W}_C [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_C) \\
\mathbf{C}_t &= \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t \\
\mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{C}_t)
\end{align}
$$

### 6.3. GRUç®€åŒ–è®¾è®¡

**æ›´æ–°é—¨å’Œé‡ç½®é—¨**ï¼š

$$
\begin{align}
\mathbf{z}_t &= \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\mathbf{r}_t &= \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\tilde{\mathbf{h}}_t &= \tanh(\mathbf{W} [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t]) \\
\mathbf{h}_t &= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
\end{align}
$$

## 7. Transformeræ¶æ„

### 7.1. è‡ªæ³¨æ„åŠ›æœºåˆ¶

**ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**ï¼š

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$

å…¶ä¸­ï¼š

- $\mathbf{Q}$ = æŸ¥è¯¢çŸ©é˜µ
- $\mathbf{K}$ = é”®çŸ©é˜µ
- $\mathbf{V}$ = å€¼çŸ©é˜µ
- $d_k$ = é”®å‘é‡ç»´åº¦

### 7.2. å¤šå¤´æ³¨æ„åŠ›

**å¹¶è¡Œæ³¨æ„åŠ›å¤´**ï¼š

$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, ..., \text{head}_h)\mathbf{W}^O$$

å…¶ä¸­ï¼š
$$\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)$$

### 7.3. ä½ç½®ç¼–ç 

**æ­£å¼¦ä½ç½®ç¼–ç **ï¼š

$$
\begin{align}
PE_{(pos, 2i)} &= \sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} &= \cos(pos/10000^{2i/d_{model}})
\end{align}
$$

### 7.4. Transformerå—ç»“æ„

$$
\begin{align}
\mathbf{Z} &= \text{LayerNorm}(\mathbf{X} + \text{MultiHead}(\mathbf{X}, \mathbf{X}, \mathbf{X})) \\
\text{Output} &= \text{LayerNorm}(\mathbf{Z} + \text{FFN}(\mathbf{Z}))
\end{align}
$$

## 8. ç”Ÿæˆæ¨¡å‹

### 8.1. å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰

**ç¼–ç å™¨å’Œè§£ç å™¨**ï¼š

- ç¼–ç å™¨ï¼š$q_\phi(\mathbf{z}|\mathbf{x})$
- è§£ç å™¨ï¼š$p_\theta(\mathbf{x}|\mathbf{z})$

**å˜åˆ†ä¸‹ç•Œ**ï¼š

$$\log p(\mathbf{x}) \geq \mathcal{L}(\theta, \phi; \mathbf{x}) = -\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) + \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]$$

### 8.2. ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰

**å¯¹æŠ—è®­ç»ƒ**ï¼š

- ç”Ÿæˆå™¨ï¼š$G: \mathcal{Z} \rightarrow \mathcal{X}$
- åˆ¤åˆ«å™¨ï¼š$D: \mathcal{X} \rightarrow [0,1]$

**ç›®æ ‡å‡½æ•°**ï¼š

$$\min_G \max_D V(D,G) = \mathbb{E}_{\mathbf{x} \sim p_{data}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - D(G(\mathbf{z})))]$$

### 8.3. æ‰©æ•£æ¨¡å‹

**å‰å‘æ‰©æ•£è¿‡ç¨‹**ï¼š

$$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$$

**é€†å‘ç”Ÿæˆè¿‡ç¨‹**ï¼š

$$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))$$

## 9. ä¼˜åŒ–ç†è®ºä¸æŠ€æœ¯

### 9.1. æ¢¯åº¦ä¸‹é™å˜ä½“

**SGDåŠ¨é‡**ï¼š

$$
\begin{align}
\mathbf{v}_t &= \gamma \mathbf{v}_{t-1} + \eta \nabla_\theta J(\theta) \\
\theta &= \theta - \mathbf{v}_t
\end{align}
$$

**Adamä¼˜åŒ–å™¨**ï¼š

$$
\begin{align}
\mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1)\mathbf{g}_t \\
\mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2)\mathbf{g}_t^2 \\
\hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1-\beta_1^t} \\
\hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1-\beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}\hat{\mathbf{m}}_t
\end{align}
$$

### 9.2. å­¦ä¹ ç‡è°ƒåº¦

**ä½™å¼¦é€€ç«**ï¼š

$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{T_{cur}}{T_{max}}\pi))$$

### 9.3. æ‰¹å½’ä¸€åŒ–

**æ‰¹å½’ä¸€åŒ–å˜æ¢**ï¼š

$$\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B^{(k)}}{\sqrt{\sigma_B^{(k)2} + \epsilon}}$$

$$y^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{(k)}$$

## 10. æ­£åˆ™åŒ–ä¸æ³›åŒ–ç†è®º

### 10.1. Dropout

**è®­ç»ƒæ—¶éšæœºå¤±æ´»**ï¼š

$$\mathbf{r} \sim \text{Bernoulli}(p)$$
$$\tilde{\mathbf{h}} = \mathbf{r} \odot \mathbf{h}$$

**æ¨ç†æ—¶ç¼©æ”¾**ï¼š

$$\mathbf{h}_{test} = p \cdot \mathbf{h}$$

### 10.2. æƒé‡è¡°å‡

**L2æ­£åˆ™åŒ–**ï¼š

$$\mathcal{L}_{total} = \mathcal{L}_{data} + \lambda \sum_i w_i^2$$

### 10.3. æ³›åŒ–ç•Œ

**Rademacherå¤æ‚åº¦**ï¼š

$$\mathcal{R}_S(\mathcal{F}) = \mathbb{E}_{\boldsymbol{\sigma}} \left[ \sup_{f \in \mathcal{F}} \frac{1}{m} \sum_{i=1}^m \sigma_i f(\mathbf{x}_i) \right]$$

**æ³›åŒ–è¯¯å·®ç•Œ**ï¼š

$$\mathbb{E}[R(f)] - R_S(f) \leq 2\mathcal{R}_S(\mathcal{F}) + \sqrt{\frac{\log(2/\delta)}{2m}}$$

## 11. è¡¨ç¤ºå­¦ä¹ ç†è®º

### 11.1. ç‰¹å¾å­¦ä¹ 

**åˆ†å±‚è¡¨ç¤º**ï¼š

æ·±åº¦ç½‘ç»œå­¦ä¹ åˆ†å±‚ç‰¹å¾ï¼š

- ä½å±‚ï¼šè¾¹ç¼˜ã€çº¹ç†
- ä¸­å±‚ï¼šéƒ¨ä»¶ã€æ¨¡å¼
- é«˜å±‚ï¼šè¯­ä¹‰æ¦‚å¿µ

### 11.2. æµå½¢å‡è®¾

**æ•°æ®æµå½¢**ï¼š

é«˜ç»´æ•°æ®åˆ†å¸ƒåœ¨ä½ç»´æµå½¢ä¸Šï¼š
$$\mathcal{M} = \{f(\mathbf{z}) : \mathbf{z} \in \mathcal{Z}\}$$

å…¶ä¸­ $f: \mathcal{Z} \rightarrow \mathcal{X}$ æ˜¯éçº¿æ€§æ˜ å°„ã€‚

### 11.3. ä¿¡æ¯ç“¶é¢ˆåŸç†

**ä¿¡æ¯ç“¶é¢ˆç›®æ ‡**ï¼š

$$\min_{p(t|x)} I(X; T) - \beta I(T; Y)$$

å¹³è¡¡å‹ç¼©å’Œé¢„æµ‹èƒ½åŠ›ã€‚

## 12. Rustå®ç°ç¤ºä¾‹

### 12.1. åŸºç¡€æ•°æ®ç»“æ„

```rust
use ndarray::{Array2, Array1, Axis};
use rand::Rng;
use std::collections::HashMap;

# [derive(Debug, Clone)]
pub struct Tensor {
    data: Array2<f64>,
    requires_grad: bool,
    grad: Option<Array2<f64>>,
}

impl Tensor {
    pub fn new(data: Array2<f64>) -> Self {
        Self {
            data,
            requires_grad: false,
            grad: None,
        }
    }

    pub fn zeros(shape: (usize, usize)) -> Self {
        Self::new(Array2::zeros(shape))
    }

    pub fn random(shape: (usize, usize)) -> Self {
        let mut rng = rand::thread_rng();
        let data = Array2::from_shape_fn(shape, |_| rng.gen_range(-1.0..1.0));
        Self::new(data)
    }

    pub fn shape(&self) -> (usize, usize) {
        self.data.dim()
    }

    pub fn matmul(&self, other: &Tensor) -> Tensor {
        let result = self.data.dot(&other.data);
        Tensor::new(result)
    }
}

# [derive(Debug, Clone)]
pub enum ActivationFunction {
    ReLU,
    Sigmoid,
    Tanh,
    Softmax,
}

impl ActivationFunction {
    pub fn forward(&self, x: &Array2<f64>) -> Array2<f64> {
        match self {
            ActivationFunction::ReLU => x.map(|&v| v.max(0.0)),
            ActivationFunction::Sigmoid => x.map(|&v| 1.0 / (1.0 + (-v).exp())),
            ActivationFunction::Tanh => x.map(|&v| v.tanh()),
            ActivationFunction::Softmax => {
                let mut result = x.clone();
                for mut row in result.axis_iter_mut(Axis(0)) {
                    let max_val = row.fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                    row.map_inplace(|v| *v = (*v - max_val).exp());
                    let sum: f64 = row.sum();
                    row.map_inplace(|v| *v /= sum);
                }
                result
            }
        }
    }

    pub fn backward(&self, x: &Array2<f64>, grad_output: &Array2<f64>) -> Array2<f64> {
        match self {
            ActivationFunction::ReLU => {
                x.map(|&v| if v > 0.0 { 1.0 } else { 0.0 }) * grad_output
            },
            ActivationFunction::Sigmoid => {
                let sigmoid = self.forward(x);
                &sigmoid * &(1.0 - &sigmoid) * grad_output
            },
            ActivationFunction::Tanh => {
                let tanh = self.forward(x);
                (1.0 - &tanh * &tanh) * grad_output
            },
            ActivationFunction::Softmax => {
                // Simplified softmax gradient
                grad_output.clone()
            }
        }
    }
}
```

## 13. ç¥ç»ç½‘ç»œå±‚

```rust
# [derive(Debug)]
pub struct DenseLayer {
    weights: Tensor,
    bias: Tensor,
    activation: ActivationFunction,
    input_cache: Option<Array2<f64>>,
    pre_activation_cache: Option<Array2<f64>>,
}

impl DenseLayer {
    pub fn new(input_size: usize, output_size: usize, activation: ActivationFunction) -> Self {
        let scale = (2.0 / input_size as f64).sqrt();
        let weights = Tensor::random((input_size, output_size));
        let weights_data = weights.data.map(|&x| x * scale);

        Self {
            weights: Tensor::new(weights_data),
            bias: Tensor::zeros((1, output_size)),
            activation,
            input_cache: None,
            pre_activation_cache: None,
        }
    }

    pub fn forward(&mut self, input: &Array2<f64>) -> Array2<f64> {
        // ç¼“å­˜è¾“å…¥ç”¨äºåå‘ä¼ æ’­
        self.input_cache = Some(input.clone());

        // çº¿æ€§å˜æ¢ï¼šoutput = input * weights + bias
        let linear_output = input.dot(&self.weights.data) + &self.bias.data;
        self.pre_activation_cache = Some(linear_output.clone());

        // æ¿€æ´»å‡½æ•°
        self.activation.forward(&linear_output)
    }

    pub fn backward(&self, grad_output: &Array2<f64>) -> (Array2<f64>, Array2<f64>, Array2<f64>) {
        let input = self.input_cache.as_ref().unwrap();
        let pre_activation = self.pre_activation_cache.as_ref().unwrap();

        // æ¿€æ´»å‡½æ•°çš„æ¢¯åº¦
        let grad_pre_activation = self.activation.backward(pre_activation, grad_output);

        // æƒé‡æ¢¯åº¦ï¼šinput^T * grad_pre_activation
        let grad_weights = input.t().dot(&grad_pre_activation);

        // åç½®æ¢¯åº¦ï¼šæ²¿æ‰¹æ¬¡ç»´åº¦æ±‚å’Œ
        let grad_bias = grad_pre_activation.sum_axis(Axis(0)).insert_axis(Axis(0));

        // è¾“å…¥æ¢¯åº¦ï¼šgrad_pre_activation * weights^T
        let grad_input = grad_pre_activation.dot(&self.weights.data.t());

        (grad_weights, grad_bias, grad_input)
    }

    pub fn update_weights(&mut self, grad_weights: &Array2<f64>, grad_bias: &Array2<f64>, learning_rate: f64) {
        self.weights.data = &self.weights.data - &(grad_weights * learning_rate);
        self.bias.data = &self.bias.data - &(grad_bias * learning_rate);
    }
}
```

## 14. å¤šå±‚æ„ŸçŸ¥æœº

```rust
pub struct MLP {
    layers: Vec<DenseLayer>,
    learning_rate: f64,
}

impl MLP {
    pub fn new(layer_sizes: &[usize], activations: Vec<ActivationFunction>, learning_rate: f64) -> Self {
        let mut layers = Vec::new();

        for i in 0..layer_sizes.len() - 1 {
            let layer = DenseLayer::new(
                layer_sizes[i],
                layer_sizes[i + 1],
                activations[i].clone()
            );
            layers.push(layer);
        }

        Self {
            layers,
            learning_rate,
        }
    }

    pub fn forward(&mut self, input: &Array2<f64>) -> Array2<f64> {
        let mut output = input.clone();

        for layer in &mut self.layers {
            output = layer.forward(&output);
        }

        output
    }

    pub fn backward(&mut self, grad_output: &Array2<f64>) {
        let mut current_grad = grad_output.clone();

        // åå‘éå†æ‰€æœ‰å±‚
        for layer in self.layers.iter_mut().rev() {
            let (grad_weights, grad_bias, grad_input) = layer.backward(&current_grad);

            // æ›´æ–°æƒé‡
            layer.update_weights(&grad_weights, &grad_bias, self.learning_rate);

            // ä¼ é€’æ¢¯åº¦åˆ°å‰ä¸€å±‚
            current_grad = grad_input;
        }
    }

    pub fn train_step(&mut self, input: &Array2<f64>, target: &Array2<f64>) -> f64 {
        // å‰å‘ä¼ æ’­
        let output = self.forward(input);

        // è®¡ç®—æŸå¤±ï¼ˆMSEï¼‰
        let diff = &output - target;
        let loss = (&diff * &diff).mean().unwrap();

        // åå‘ä¼ æ’­
        let grad_output = 2.0 * &diff / (diff.len() as f64);
        self.backward(&grad_output);

        loss
    }
}
```

### 14.1. å·ç§¯å±‚å®ç°

```rust
# [derive(Debug)]
pub struct Conv2D {
    filters: Array2<f64>,
    bias: Array1<f64>,
    stride: usize,
    padding: usize,
    input_cache: Option<Array2<f64>>,
}

impl Conv2D {
    pub fn new(
        in_channels: usize,
        out_channels: usize,
        kernel_size: usize,
        stride: usize,
        padding: usize
    ) -> Self {
        let filter_shape = (out_channels, in_channels * kernel_size * kernel_size);
        let scale = (2.0 / (in_channels * kernel_size * kernel_size) as f64).sqrt();

        let mut rng = rand::thread_rng();
        let filters = Array2::from_shape_fn(filter_shape, |_| {
            rng.gen_range(-scale..scale)
        });

        Self {
            filters,
            bias: Array1::zeros(out_channels),
            stride,
            padding,
            input_cache: None,
        }
    }

    pub fn forward(&mut self, input: &Array2<f64>) -> Array2<f64> {
        self.input_cache = Some(input.clone());

        // ç®€åŒ–çš„2Då·ç§¯å®ç°
        // å®é™…å®ç°éœ€è¦å¤„ç†å¤šç»´å¼ é‡å’Œim2colè½¬æ¢
        let output_size = (input.nrows() - self.filters.ncols() + 2 * self.padding) / self.stride + 1;
        let mut output = Array2::zeros((output_size, self.filters.nrows()));

        // è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦å®Œæ•´çš„å·ç§¯å®ç°
        output
    }
}
```

## 15. è®­ç»ƒå¾ªç¯

```rust
pub struct Trainer {
    model: MLP,
    batch_size: usize,
    epochs: usize,
}

impl Trainer {
    pub fn new(model: MLP, batch_size: usize, epochs: usize) -> Self {
        Self {
            model,
            batch_size,
            epochs,
        }
    }

    pub fn train(&mut self, train_data: &[(Array2<f64>, Array2<f64>)]) {
        for epoch in 0..self.epochs {
            let mut total_loss = 0.0;
            let mut batch_count = 0;

            // å°æ‰¹æ¬¡è®­ç»ƒ
            for batch in train_data.chunks(self.batch_size) {
                let mut batch_input = Vec::new();
                let mut batch_target = Vec::new();

                for (input, target) in batch {
                    batch_input.push(input.clone());
                    batch_target.push(target.clone());
                }

                // åˆå¹¶æ‰¹æ¬¡æ•°æ®
                if let (Some(first_input), Some(first_target)) =
                   (batch_input.first(), batch_target.first()) {
                    let batch_input_tensor = Array2::from_shape_vec(
                        (batch_input.len(), first_input.len()),
                        batch_input.into_iter().flatten().collect()
                    ).unwrap();

                    let batch_target_tensor = Array2::from_shape_vec(
                        (batch_target.len(), first_target.len()),
                        batch_target.into_iter().flatten().collect()
                    ).unwrap();

                    let loss = self.model.train_step(&batch_input_tensor, &batch_target_tensor);
                    total_loss += loss;
                    batch_count += 1;
                }
            }

            let avg_loss = total_loss / batch_count as f64;
            println!("Epoch {}: Average Loss = {:.6}", epoch + 1, avg_loss);
        }
    }

    pub fn evaluate(&mut self, test_data: &[(Array2<f64>, Array2<f64>)]) -> f64 {
        let mut total_loss = 0.0;
        let mut count = 0;

        for (input, target) in test_data {
            let output = self.model.forward(input);
            let diff = &output - target;
            let loss = (&diff * &diff).mean().unwrap();
            total_loss += loss;
            count += 1;
        }

        total_loss / count as f64
    }
}

// ä½¿ç”¨ç¤ºä¾‹
pub fn example_usage() {
    // åˆ›å»ºç®€å•çš„MLPæ¨¡å‹
    let layer_sizes = vec![784, 128, 64, 10]; // MNISTåˆ†ç±»
    let activations = vec![
        ActivationFunction::ReLU,
        ActivationFunction::ReLU,
        ActivationFunction::Softmax,
    ];

    let model = MLP::new(&layer_sizes, activations, 0.001);
    let mut trainer = Trainer::new(model, 32, 100);

    // å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆè¿™é‡Œæ˜¯ç¤ºä¾‹æ•°æ®ï¼‰
    let mut train_data = Vec::new();
    for _ in 0..1000 {
        let input = Array2::random((1, 784));
        let target = Array2::random((1, 10));
        train_data.push((input, target));
    }

    // è®­ç»ƒæ¨¡å‹
    trainer.train(&train_data);

    // è¯„ä¼°æ¨¡å‹
    let test_loss = trainer.evaluate(&train_data[0..100]);
    println!("Test Loss: {:.6}", test_loss);
}
```

## 16. æœ¬åœ°è·³è½¬é“¾æ¥

- [ç±»å‹ç†è®ºåŸºç¡€](../../2-å½¢å¼ç§‘å­¦ç†è®º/2.1-ç±»å‹ç†è®º/2.1.1-ç±»å‹ç†è®ºåŸºç¡€.md)
- [æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º](./3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md)
- [ç›‘ç£å­¦ä¹ ç®—æ³•](./3.4.2-ç›‘ç£å­¦ä¹ ç®—æ³•.md)
- [å¼ºåŒ–å­¦ä¹ ç†è®º](./3.4.4-å¼ºåŒ–å­¦ä¹ ç†è®º.md)
- [ä¼˜åŒ–ç†è®ºä¸ç®—æ³•](./3.4.6-ä¼˜åŒ–ç†è®ºä¸ç®—æ³•.md)

## 17. å‚è€ƒæ–‡çŒ®

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.
3. Vaswani, A., et al. (2017). Attention is all you need. NIPS.
4. He, K., et al. (2016). Deep residual learning for image recognition. CVPR.
5. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation.

## 18. å¤šè¡¨å¾

æ·±åº¦å­¦ä¹ æ¶æ„æ”¯æŒå¤šç§è¡¨å¾æ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š

- ç¬¦å·è¡¨å¾ï¼ˆç½‘ç»œç»“æ„ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€ç®—æ³•ä¼ªä»£ç ç­‰ï¼‰
- å›¾ç»“æ„ï¼ˆç¥ç»ç½‘ç»œç»“æ„å›¾ã€å±‚æ¬¡ç»“æ„å›¾ã€æ•°æ®æµå›¾ç­‰ï¼‰
- å‘é‡/å¼ é‡ï¼ˆæƒé‡çŸ©é˜µã€ç‰¹å¾å‘é‡ã€åµŒå…¥ï¼‰
- è‡ªç„¶è¯­è¨€ï¼ˆå®šä¹‰ã€æ³¨é‡Šã€æè¿°ï¼‰
- å›¾åƒ/å¯è§†åŒ–ï¼ˆç½‘ç»œç»“æ„å›¾ã€è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ã€ç‰¹å¾å¯è§†åŒ–ç­‰ï¼‰
è¿™äº›è¡¨å¾å¯äº’æ˜ ï¼Œæå‡æ·±åº¦å­¦ä¹ æ¶æ„çš„è¡¨è¾¾åŠ›ã€‚

## 19. å½¢å¼åŒ–è¯­ä¹‰

- è¯­ä¹‰åŸŸï¼š$D$ï¼Œå¦‚è¾“å…¥ç©ºé—´ã€è¾“å‡ºç©ºé—´ã€å‚æ•°ç©ºé—´ã€ç½‘ç»œç©ºé—´ã€æŸå¤±ç©ºé—´
- è§£é‡Šå‡½æ•°ï¼š$I: S \to D$ï¼Œå°†ç¬¦å·/ç»“æ„æ˜ å°„åˆ°å…·ä½“è¯­ä¹‰å¯¹è±¡
- è¯­ä¹‰ä¸€è‡´æ€§ï¼šæ¯ä¸ªç½‘ç»œç»“æ„/å±‚/æ¿€æ´»å‡½æ•°/æŸå¤±å‡½æ•°/å…¬å¼åœ¨$D$ä¸­æœ‰æ˜ç¡®å®šä¹‰

## 20. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜

- è¯­æ³•è§„åˆ™ï¼šå¦‚ç½‘ç»œç»“æ„å®šä¹‰ã€å±‚æ¬¡è§„åˆ™ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€è®­ç»ƒè§„åˆ™ã€æ¨ç†è§„åˆ™ã€çº¦æŸæ¡ä»¶
- **å®šç†**ï¼šæ·±åº¦å­¦ä¹ æ¶æ„çš„è¯­æ³•ç³»ç»Ÿå…·ä¸€è‡´æ€§ä¸å¯æ‰©å±•æ€§ã€‚
- **è¯æ˜**ï¼šç”±ç½‘ç»œç»“æ„ã€å±‚æ¬¡è§„åˆ™ã€æ¿€æ´»ä¸æŸå¤±å‡½æ•°ã€æ¨ç†è§„åˆ™é€’å½’å®šä¹‰ï¼Œä¿è¯ç³»ç»Ÿä¸€è‡´ä¸å¯æ‰©å±•ã€‚
