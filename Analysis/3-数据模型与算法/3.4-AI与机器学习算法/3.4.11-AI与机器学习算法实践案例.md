# AIä¸æœºå™¨å­¦ä¹ ç®—æ³•å®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [AIä¸æœºå™¨å­¦ä¹ ç®—æ³•å®è·µæ¡ˆä¾‹](#aiä¸æœºå™¨å­¦ä¹ ç®—æ³•å®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. ç›‘ç£å­¦ä¹ å®è·µæ¡ˆä¾‹](#1-ç›‘ç£å­¦ä¹ å®è·µæ¡ˆä¾‹)
    - [1.1. çº¿æ€§å›å½’ä¸é€»è¾‘å›å½’](#11-çº¿æ€§å›å½’ä¸é€»è¾‘å›å½’)
  - [2. å†³ç­–æ ‘ä¸éšæœºæ£®æ—](#2-å†³ç­–æ ‘ä¸éšæœºæ£®æ—)
  - [3. æ— ç›‘ç£å­¦ä¹ å®è·µæ¡ˆä¾‹](#3-æ— ç›‘ç£å­¦ä¹ å®è·µæ¡ˆä¾‹)
    - [3.1. K-meansèšç±»](#31-k-meansèšç±»)
  - [4. ä¸»æˆåˆ†åˆ†æ(PCA)](#4-ä¸»æˆåˆ†åˆ†æpca)
  - [5. æ·±åº¦å­¦ä¹ å®è·µæ¡ˆä¾‹](#5-æ·±åº¦å­¦ä¹ å®è·µæ¡ˆä¾‹)
    - [5.1. ç®€å•ç¥ç»ç½‘ç»œ](#51-ç®€å•ç¥ç»ç½‘ç»œ)
  - [6. å·ç§¯ç¥ç»ç½‘ç»œ(CNN)](#6-å·ç§¯ç¥ç»ç½‘ç»œcnn)
  - [7. å¼ºåŒ–å­¦ä¹ å®è·µæ¡ˆä¾‹](#7-å¼ºåŒ–å­¦ä¹ å®è·µæ¡ˆä¾‹)
    - [7.1. Q-Learning](#71-q-learning)

---


## 1. ç›‘ç£å­¦ä¹ å®è·µæ¡ˆä¾‹

### 1.1. çº¿æ€§å›å½’ä¸é€»è¾‘å›å½’

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# çº¿æ€§å›å½’ç¤ºä¾‹
class LinearRegressionExample:
    def __init__(self):
        self.model = LinearRegression()
        self.scaler = StandardScaler()

    def generate_data(self, n_samples=100):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        X = np.random.randn(n_samples, 2)
# çœŸå®å…³ç³»: y = 2*x1 + 3*x2 + 1 + noise
        y = 2 * X[:, 0] + 3 * X[:, 1] + 1 + np.random.normal(0, 0.1, n_samples)
        return X, y

    def train(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y)

# æ‰“å°æ¨¡å‹å‚æ•°
        print(f"æˆªè·: {self.model.intercept_:.4f}")
        print(f"ç³»æ•°: {self.model.coef_}")

    def predict(self, X):
        """é¢„æµ‹"""
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def evaluate(self, X, y):
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.predict(X)
        mse = mean_squared_error(y, y_pred)
        print(f"å‡æ–¹è¯¯å·®: {mse:.4f}")
        return mse

# é€»è¾‘å›å½’ç¤ºä¾‹
class LogisticRegressionExample:
    def __init__(self):
        self.model = LogisticRegression(random_state=42)
        self.scaler = StandardScaler()

    def generate_data(self, n_samples=200):
        """ç”ŸæˆäºŒåˆ†ç±»æ•°æ®"""
        np.random.seed(42)
# ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒ
        X1 = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], n_samples//2)
        X2 = np.random.multivariate_normal([2, 2], [[1, 0.5], [0.5, 1]], n_samples//2)

        X = np.vstack([X1, X2])
        y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])

        return X, y

    def train(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y)

        print(f"æˆªè·: {self.model.intercept_[0]:.4f}")
        print(f"ç³»æ•°: {self.model.coef_[0]}")

    def predict(self, X):
        """é¢„æµ‹"""
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X_scaled = self.scaler.transform(X)
        return self.model.predict_proba(X_scaled)

    def evaluate(self, X, y):
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.predict(X)
        accuracy = accuracy_score(y, y_pred)
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y, y_pred))
        return accuracy

# ä½¿ç”¨ç¤ºä¾‹
print("=== çº¿æ€§å›å½’ç¤ºä¾‹ ===")
lr_example = LinearRegressionExample()
X, y = lr_example.generate_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr_example.train(X_train, y_train)
lr_example.evaluate(X_test, y_test)

print("\n=== é€»è¾‘å›å½’ç¤ºä¾‹ ===")
logreg_example = LogisticRegressionExample()
X, y = logreg_example.generate_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logreg_example.train(X_train, y_train)
logreg_example.evaluate(X_test, y_test)
```

## 2. å†³ç­–æ ‘ä¸éšæœºæ£®æ—

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import pandas as pd

class DecisionTreeExample:
    def __init__(self, max_depth=3):
        self.model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)

    def generate_data(self, n_samples=300):
        """ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®"""
        X, y = make_classification(
            n_samples=n_samples,
            n_features=4,
            n_informative=3,
            n_redundant=1,
            n_classes=3,
            random_state=42
        )
        return X, y

    def train(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        self.model.fit(X, y)

# ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': [f'Feature_{i}' for i in range(X.shape[1])],
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("ç‰¹å¾é‡è¦æ€§:")
        print(feature_importance)

    def predict(self, X):
        """é¢„æµ‹"""
        return self.model.predict(X)

    def evaluate(self, X, y):
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.predict(X)
        accuracy = accuracy_score(y, y_pred)
        print(f"å†³ç­–æ ‘å‡†ç¡®ç‡: {accuracy:.4f}")
        return accuracy

class RandomForestExample:
    def __init__(self, n_estimators=100, max_depth=5):
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42
        )

    def train(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        self.model.fit(X, y)

# ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': [f'Feature_{i}' for i in range(X.shape[1])],
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§:")
        print(feature_importance)

    def predict(self, X):
        """é¢„æµ‹"""
        return self.model.predict(X)

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        return self.model.predict_proba(X)

    def evaluate(self, X, y):
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.predict(X)
        accuracy = accuracy_score(y, y_pred)
        print(f"éšæœºæ£®æ—å‡†ç¡®ç‡: {accuracy:.4f}")
        return accuracy

# ä½¿ç”¨ç¤ºä¾‹
print("=== å†³ç­–æ ‘ç¤ºä¾‹ ===")
dt_example = DecisionTreeExample(max_depth=3)
X, y = dt_example.generate_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

dt_example.train(X_train, y_train)
dt_example.evaluate(X_test, y_test)

print("\n=== éšæœºæ£®æ—ç¤ºä¾‹ ===")
rf_example = RandomForestExample(n_estimators=50, max_depth=5)
rf_example.train(X_train, y_train)
rf_example.evaluate(X_test, y_test)
```

## 3. æ— ç›‘ç£å­¦ä¹ å®è·µæ¡ˆä¾‹

### 3.1. K-meansèšç±»

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

class KMeansExample:
    def __init__(self, n_clusters=3):
        self.model = KMeans(n_clusters=n_clusters, random_state=42)
        self.n_clusters = n_clusters

    def generate_data(self, n_samples=300):
        """ç”Ÿæˆèšç±»æ•°æ®"""
        X, _ = make_blobs(
            n_samples=n_samples,
            centers=4,
            cluster_std=0.6,
            random_state=42
        )
        return X

    def train(self, X):
        """è®­ç»ƒæ¨¡å‹"""
        self.model.fit(X)

# èšç±»ä¸­å¿ƒ
        print(f"èšç±»ä¸­å¿ƒ:\n{self.model.cluster_centers_}")

# è½®å»“ç³»æ•°
        labels = self.model.labels_
        silhouette_avg = silhouette_score(X, labels)
        print(f"å¹³å‡è½®å»“ç³»æ•°: {silhouette_avg:.4f}")

    def predict(self, X):
        """é¢„æµ‹èšç±»æ ‡ç­¾"""
        return self.model.predict(X)

    def find_optimal_k(self, X, max_k=10):
        """å¯»æ‰¾æœ€ä¼˜èšç±»æ•°"""
        silhouette_scores = []
        inertias = []

        for k in range(2, max_k + 1):
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(X)

            labels = kmeans.labels_
            silhouette_avg = silhouette_score(X, labels)
            silhouette_scores.append(silhouette_avg)
            inertias.append(kmeans.inertia_)

            print(f"k={k}: è½®å»“ç³»æ•°={silhouette_avg:.4f}, æƒ¯æ€§={kmeans.inertia_:.2f}")

        optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2
        print(f"\næœ€ä¼˜èšç±»æ•°: {optimal_k}")

        return optimal_k, silhouette_scores, inertias

# ä½¿ç”¨ç¤ºä¾‹
print("=== K-meansèšç±»ç¤ºä¾‹ ===")
kmeans_example = KMeansExample(n_clusters=4)
X = kmeans_example.generate_data()

# å¯»æ‰¾æœ€ä¼˜èšç±»æ•°
optimal_k, scores, inertias = kmeans_example.find_optimal_k(X)

# ä½¿ç”¨æœ€ä¼˜èšç±»æ•°é‡æ–°è®­ç»ƒ
kmeans_example = KMeansExample(n_clusters=optimal_k)
kmeans_example.train(X)
labels = kmeans_example.predict(X)

print(f"èšç±»ç»“æœ: {np.bincount(labels)}")
```

## 4. ä¸»æˆåˆ†åˆ†æ(PCA)

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

class PCAExample:
    def __init__(self, n_components=2):
        self.pca = PCA(n_components=n_components)
        self.scaler = StandardScaler()

    def generate_data(self, n_samples=1000):
        """ç”Ÿæˆé«˜ç»´æ•°æ®"""
        np.random.seed(42)
# ç”Ÿæˆç›¸å…³çš„é«˜ç»´æ•°æ®
        X = np.random.randn(n_samples, 10)
# æ·»åŠ ä¸€äº›ç›¸å…³æ€§
        X[:, 2] = X[:, 0] + X[:, 1] + np.random.normal(0, 0.1, n_samples)
        X[:, 3] = X[:, 0] * 2 + np.random.normal(0, 0.1, n_samples)
        return X

    def fit_transform(self, X):
        """æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®"""
        X_scaled = self.scaler.fit_transform(X)
        X_pca = self.pca.fit_transform(X_scaled)

# è§£é‡Šæ–¹å·®æ¯”
        explained_variance_ratio = self.pca.explained_variance_ratio_
        cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

        print("è§£é‡Šæ–¹å·®æ¯”:")
        for i, (var_ratio, cum_var_ratio) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):
            print(f"PC{i+1}: {var_ratio:.4f} (ç´¯è®¡: {cum_var_ratio:.4f})")

        return X_pca

    def transform(self, X):
        """è½¬æ¢æ–°æ•°æ®"""
        X_scaled = self.scaler.transform(X)
        return self.pca.transform(X_scaled)

    def inverse_transform(self, X_pca):
        """é€†è½¬æ¢"""
        X_scaled = self.pca.inverse_transform(X_pca)
        return self.scaler.inverse_transform(X_scaled)

    def find_optimal_components(self, X, threshold=0.95):
        """å¯»æ‰¾æœ€ä¼˜ä¸»æˆåˆ†æ•°"""
        X_scaled = self.scaler.fit_transform(X)

# ä½¿ç”¨æ‰€æœ‰ä¸»æˆåˆ†
        pca_full = PCA()
        pca_full.fit(X_scaled)

# è®¡ç®—ç´¯è®¡è§£é‡Šæ–¹å·®
        cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)

# æ‰¾åˆ°è¾¾åˆ°é˜ˆå€¼æ‰€éœ€çš„ä¸»æˆåˆ†æ•°
        n_components = np.argmax(cumulative_variance >= threshold) + 1

        print(f"è¾¾åˆ°{threshold*100}%è§£é‡Šæ–¹å·®éœ€è¦çš„ä¸»æˆåˆ†æ•°: {n_components}")

        return n_components

# ä½¿ç”¨ç¤ºä¾‹
print("=== PCAé™ç»´ç¤ºä¾‹ ===")
pca_example = PCAExample(n_components=2)
X = pca_example.generate_data()

# å¯»æ‰¾æœ€ä¼˜ä¸»æˆåˆ†æ•°
optimal_components = pca_example.find_optimal_components(X, threshold=0.9)

# ä½¿ç”¨2ä¸ªä¸»æˆåˆ†è¿›è¡Œé™ç»´
X_pca = pca_example.fit_transform(X)
print(f"åŸå§‹ç»´åº¦: {X.shape[1]}")
print(f"é™ç»´åç»´åº¦: {X_pca.shape[1]}")

# é€†è½¬æ¢ç¤ºä¾‹
X_reconstructed = pca_example.inverse_transform(X_pca)
reconstruction_error = np.mean((X - X_reconstructed) ** 2)
print(f"é‡æ„è¯¯å·®: {reconstruction_error:.6f}")
```

## 5. æ·±åº¦å­¦ä¹ å®è·µæ¡ˆä¾‹

### 5.1. ç®€å•ç¥ç»ç½‘ç»œ

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class SimpleNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, hidden_size)
        self.output = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.layer2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.output(x)
        return x

class DeepLearningExample:
    def __init__(self, input_size, hidden_size, output_size):
        self.model = SimpleNeuralNetwork(input_size, hidden_size, output_size)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def generate_data(self, n_samples=1000):
        """ç”Ÿæˆåˆ†ç±»æ•°æ®"""
        np.random.seed(42)
        X = np.random.randn(n_samples, 10)
        y = np.random.randint(0, 3, n_samples)
        return X, y

    def train(self, X, y, epochs=100, batch_size=32):
        """è®­ç»ƒæ¨¡å‹"""
# è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.LongTensor(y)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        train_losses = []

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0

            for batch_X, batch_y in dataloader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)

# å‰å‘ä¼ æ’­
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)

# åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            train_losses.append(avg_loss)

            if (epoch + 1) % 20 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')

        return train_losses

    def predict(self, X):
        """é¢„æµ‹"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self.model(X_tensor)
            _, predicted = torch.max(outputs.data, 1)
            return predicted.cpu().numpy()

    def evaluate(self, X, y):
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.predict(X)
        accuracy = accuracy_score(y, y_pred)
        print(f"ç¥ç»ç½‘ç»œå‡†ç¡®ç‡: {accuracy:.4f}")
        return accuracy

# ä½¿ç”¨ç¤ºä¾‹
print("=== æ·±åº¦å­¦ä¹ ç¤ºä¾‹ ===")
dl_example = DeepLearningExample(input_size=10, hidden_size=64, output_size=3)
X, y = dl_example.generate_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
train_losses = dl_example.train(X_train, y_train, epochs=100, batch_size=32)

# è¯„ä¼°æ¨¡å‹
dl_example.evaluate(X_test, y_test)
```

## 6. å·ç§¯ç¥ç»ç½‘ç»œ(CNN)

```python
import torchvision.transforms as transforms
from torchvision import datasets

class CNNExample:
    def __init__(self):
        self.model = self.build_cnn()
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def build_cnn(self):
        """æ„å»ºCNNæ¨¡å‹"""
        model = nn.Sequential(
# ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

# ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

# å…¨è¿æ¥å±‚
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 10)
        )
        return model

    def load_mnist_data(self, batch_size=64):
        """åŠ è½½MNISTæ•°æ®é›†"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

# åŠ è½½è®­ç»ƒæ•°æ®
        train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# åŠ è½½æµ‹è¯•æ•°æ®
        test_dataset = datasets.MNIST('./data', train=False, transform=transform)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        return train_loader, test_loader

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)

        return total_loss / len(train_loader), correct / total

    def test(self, test_loader):
        """æµ‹è¯•æ¨¡å‹"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

        test_loss /= len(test_loader)
        accuracy = correct / total

        print(f'æµ‹è¯•é›†: å¹³å‡æŸå¤±: {test_loss:.4f}, å‡†ç¡®ç‡: {accuracy:.4f}')
        return test_loss, accuracy

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…è¿è¡Œéœ€è¦ä¸‹è½½MNISTæ•°æ®é›†ï¼‰
print("=== CNNç¤ºä¾‹ ===")
print("æ³¨æ„: æ­¤ç¤ºä¾‹éœ€è¦ä¸‹è½½MNISTæ•°æ®é›†ï¼Œå®é™…è¿è¡Œæ—¶è¯·ç¡®ä¿ç½‘ç»œè¿æ¥")

# cnn_example = CNNExample()
# train_loader, test_loader = cnn_example.load_mnist_data()

# # è®­ç»ƒæ¨¡å‹
# for epoch in range(5):
# train_loss, train_acc = cnn_example.train_epoch(train_loader)
# print(f'Epoch {epoch+1}: è®­ç»ƒæŸå¤±={train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡={train_acc:.4f}')
# cnn_example.test(test_loader)
```

## 7. å¼ºåŒ–å­¦ä¹ å®è·µæ¡ˆä¾‹

### 7.1. Q-Learning

```python
import random
from collections import defaultdict

class QLearningExample:
    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = defaultdict(lambda: np.zeros(n_actions))

    def choose_action(self, state):
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        if random.random() < self.epsilon:
            return random.randint(0, self.n_actions - 1)
        else:
            return np.argmax(self.q_table[state])

    def update_q_value(self, state, action, reward, next_state):
        """æ›´æ–°Qå€¼"""
        current_q = self.q_table[state][action]
        max_next_q = np.max(self.q_table[next_state])
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        self.q_table[state][action] = new_q

    def train(self, episodes=1000):
        """è®­ç»ƒQ-Learningæ™ºèƒ½ä½“"""
# ç®€å•çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ
# 0: æ™®é€šæ ¼å­, 1: ç›®æ ‡, -1: é™·é˜±
        grid = [
            [0, 0, 0, 1],
            [0, -1, 0, -1],
            [0, 0, 0, 0],
            [-1, 0, 0, 0]
        ]

        total_rewards = []

        for episode in range(episodes):
            state = (0, 0)  # èµ·å§‹ä½ç½®
            total_reward = 0

            for step in range(100):  # æœ€å¤§æ­¥æ•°
# é€‰æ‹©åŠ¨ä½œ
                action = self.choose_action(state)

# æ‰§è¡ŒåŠ¨ä½œ
                next_state = self.get_next_state(state, action, grid)
                reward = grid[next_state[0]][next_state[1]]

# æ›´æ–°Qå€¼
                self.update_q_value(state, action, reward, next_state)

                total_reward += reward
                state = next_state

# æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡æˆ–é™·é˜±
                if reward == 1 or reward == -1:
                    break

            total_rewards.append(total_reward)

            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(total_rewards[-100:])
                print(f"Episode {episode+1}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")

        return total_rewards

    def get_next_state(self, state, action, grid):
        """è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€"""
        row, col = state
        n_rows, n_cols = len(grid), len(grid[0])

        if action == 0:  # ä¸Š
            next_row = max(0, row - 1)
            next_col = col
        elif action == 1:  # ä¸‹
            next_row = min(n_rows - 1, row + 1)
            next_col = col
        elif action == 2:  # å·¦
            next_row = row
            next_col = max(0, col - 1)
        else:  # å³
            next_row = row
            next_col = min(n_cols - 1, col + 1)

        return (next_row, next_col)

    def get_optimal_policy(self):
        """è·å–æœ€ä¼˜ç­–ç•¥"""
        policy = {}
        for state in self.q_table:
            policy[state] = np.argmax(self.q_table[state])
        return policy

# ä½¿ç”¨ç¤ºä¾‹
print("=== Q-Learningç¤ºä¾‹ ===")
ql_example = QLearningExample(n_states=16, n_actions=4, learning_rate=0.1, epsilon=0.1)
rewards = ql_example.train(episodes=500)

# è·å–æœ€ä¼˜ç­–ç•¥
optimal_policy = ql_example.get_optimal_policy()
print("\næœ€ä¼˜ç­–ç•¥:")
for state, action in optimal_policy.items():
    action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
    print(f"çŠ¶æ€ {state}: {action_names[action]}")
```

è¿™äº›å®è·µæ¡ˆä¾‹å±•ç¤ºäº†AIä¸æœºå™¨å­¦ä¹ ç®—æ³•ä¸­çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç­‰ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½æä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œä½¿ç”¨ç¤ºä¾‹ï¼Œå¸®åŠ©ç†è§£è¿™äº›ç®—æ³•çš„å®é™…åº”ç”¨ã€‚
