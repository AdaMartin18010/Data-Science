# 3.4.3 无监督学习算法

## 概述

无监督学习是机器学习的重要分支，在没有标注数据的情况下发现数据的内在结构和模式。本文档系统阐述无监督学习的核心算法、理论基础和实际应用。

## 目录

1. [聚类算法](#聚类算法)
2. [降维算法](#降维算法)
3. [密度估计](#密度估计)
4. [异常检测](#异常检测)
5. [关联规则学习](#关联规则学习)
6. [矩阵分解](#矩阵分解)
7. [自组织映射](#自组织映射)
8. [Rust实现示例](#rust实现示例)

## 聚类算法

### 1.1 K-Means聚类

**算法原理**：

目标函数（最小化簇内平方和）：
$$J = \sum_{i=1}^k \sum_{x \in C_i} ||x - \mu_i||^2$$

其中 $\mu_i$ 是第 $i$ 个簇的质心：
$$\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i} x$$

**算法步骤**：

1. 随机初始化 $k$ 个质心
2. 分配样本到最近的质心
3. 更新质心位置
4. 重复步骤2-3直到收敛

**收敛性证明**：

Lloyd算法保证目标函数单调递减：
$$J^{(t+1)} \leq J^{(t)}$$

### 1.2 层次聚类

**凝聚层次聚类**：

距离矩阵更新公式（Ward连接）：
$$d(C_i \cup C_j, C_k) = \sqrt{\frac{|C_i| + |C_k|}{|C_i| + |C_j| + |C_k|} d(C_i, C_k)^2 + \frac{|C_j| + |C_k|}{|C_i| + |C_j| + |C_k|} d(C_j, C_k)^2 - \frac{|C_k|}{|C_i| + |C_j| + |C_k|} d(C_i, C_j)^2}$$

### 1.3 DBSCAN密度聚类

**核心概念**：

- $\epsilon$-邻域：$N_\epsilon(p) = \{q \in D | \text{dist}(p,q) \leq \epsilon\}$
- 核心点：$|N_\epsilon(p)| \geq \text{MinPts}$
- 密度直达：$q$ 在 $p$ 的 $\epsilon$-邻域内且 $p$ 是核心点

**聚类质量**：

轮廓系数：
$$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$

其中：

- $a(i)$：样本 $i$ 到同簇其他点的平均距离
- $b(i)$：样本 $i$ 到最近其他簇的平均距离

## 降维算法

### 2.1 主成分分析（PCA）

**数学原理**：

协方差矩阵：
$$\Sigma = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T$$

特征值分解：
$$\Sigma = V\Lambda V^T$$

其中 $V$ 是特征向量矩阵，$\Lambda$ 是特征值对角矩阵。

**降维变换**：
$$y = V_k^T (x - \bar{x})$$

**重构误差**：
$$\text{Error} = \sum_{i=k+1}^d \lambda_i$$

### 2.2 t-SNE

**概率分布定义**：

高维空间中的条件概率：
$$p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$$

对称化：
$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$

低维空间中的概率：
$$q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}$$

**目标函数**（KL散度）：
$$C = \sum_i \text{KL}(P_i || Q_i) = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

### 2.3 线性判别分析（LDA）

**目标函数**：

最大化类间散度与类内散度的比值：
$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

其中：

- 类间散度矩阵：$S_B = \sum_{i=1}^c n_i (\mu_i - \mu)(\mu_i - \mu)^T$
- 类内散度矩阵：$S_W = \sum_{i=1}^c \sum_{x \in C_i} (x - \mu_i)(x - \mu_i)^T$

**解析解**：
$$S_W^{-1} S_B w = \lambda w$$

## 密度估计

### 3.1 高斯混合模型（GMM）

**模型定义**：
$$p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)$$

**EM算法**：

E步（期望）：
$$\gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}$$

M步（最大化）：
$$\begin{align}
N_k &= \sum_{n=1}^N \gamma_{nk} \\
\mu_k^{new} &= \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} x_n \\
\Sigma_k^{new} &= \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} (x_n - \mu_k^{new})(x_n - \mu_k^{new})^T \\
\pi_k^{new} &= \frac{N_k}{N}
\end{align}$$

### 3.2 核密度估计

**Parzen窗方法**：
$$\hat{p}(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h^d} K\left(\frac{x - x_i}{h}\right)$$

其中 $K$ 是核函数，$h$ 是带宽参数。

**高斯核**：
$$K(u) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{u^2}{2}\right)$$

## 异常检测

### 4.1 孤立森林

**异常分数**：
$$s(x,n) = 2^{-\frac{E(h(x))}{c(n)}}$$

其中：
- $E(h(x))$：路径长度的期望
- $c(n) = 2H(n-1) - \frac{2(n-1)}{n}$：归一化常数

### 4.2 One-Class SVM

**对偶问题**：
$$\min_\alpha \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j K(x_i, x_j)$$

约束条件：
$$0 \leq \alpha_i \leq \frac{1}{\nu n}, \quad \sum_{i=1}^n \alpha_i = 1$$

**决策函数**：
$$f(x) = \sum_{i=1}^n \alpha_i K(x_i, x) - \rho$$

## 关联规则学习

### 5.1 Apriori算法

**支持度**：
$$\text{support}(X) = \frac{|\{T \in D | X \subseteq T\}|}{|D|}$$

**置信度**：
$$\text{confidence}(X \Rightarrow Y) = \frac{\text{support}(X \cup Y)}{\text{support}(X)}$$

**提升度**：
$$\text{lift}(X \Rightarrow Y) = \frac{\text{confidence}(X \Rightarrow Y)}{\text{support}(Y)}$$

## 矩阵分解

### 6.1 非负矩阵分解（NMF）

**优化问题**：
$$\min_{W,H} ||V - WH||_F^2$$

约束条件：$W_{ik} \geq 0, H_{kj} \geq 0$

**乘性更新规则**：
$$W_{ik} \leftarrow W_{ik} \frac{(VH^T)_{ik}}{(WHH^T)_{ik}}$$
$$H_{kj} \leftarrow H_{kj} \frac{(W^TV)_{kj}}{(W^TWH)_{kj}}$$

### 6.2 独立成分分析（ICA）

**信号分离模型**：
$$x = As$$

其中 $s$ 是统计独立的源信号。

**FastICA算法**：

目标函数（负熵最大化）：
$$J(w) = E[G(w^Tx)] - E[G(\nu)]$$

其中 $\nu \sim \mathcal{N}(0,1)$，$G$ 是非线性函数。

## Rust实现示例

### 8.1 K-Means实现

```rust
use ndarray::{Array1, Array2, Axis};
use rand::Rng;
use std::collections::HashMap;

pub struct KMeans {
    k: usize,
    max_iters: usize,
    tolerance: f64,
    centroids: Option<Array2<f64>>,
}

impl KMeans {
    pub fn new(k: usize, max_iters: usize, tolerance: f64) -> Self {
        Self {
            k,
            max_iters,
            tolerance,
            centroids: None,
        }
    }

    pub fn fit(&mut self, data: &Array2<f64>) -> Vec<usize> {
        let (n_samples, n_features) = data.dim();

        // 随机初始化质心
        let mut centroids = self.initialize_centroids(data);
        let mut labels = vec![0; n_samples];

        for _ in 0..self.max_iters {
            let old_centroids = centroids.clone();

            // 分配样本到最近的质心
            for i in 0..n_samples {
                let sample = data.row(i);
                labels[i] = self.closest_centroid(&sample, &centroids);
            }

            // 更新质心
            centroids = self.update_centroids(data, &labels);

            // 检查收敛
            if self.converged(&old_centroids, &centroids) {
                break;
            }
        }

        self.centroids = Some(centroids);
        labels
    }

    fn initialize_centroids(&self, data: &Array2<f64>) -> Array2<f64> {
        let (_, n_features) = data.dim();
        let mut rng = rand::thread_rng();
        let mut centroids = Array2::zeros((self.k, n_features));

        for i in 0..self.k {
            for j in 0..n_features {
                // 在数据范围内随机初始化
                let min_val = data.column(j).fold(f64::INFINITY, |a, &b| a.min(b));
                let max_val = data.column(j).fold(f64::NEG_INFINITY, |a, &b| a.max(b));
                centroids[[i, j]] = rng.gen_range(min_val..max_val);
            }
        }

        centroids
    }

    fn closest_centroid(&self, sample: ndarray::ArrayView1<f64>, centroids: &Array2<f64>) -> usize {
        let mut min_dist = f64::INFINITY;
        let mut closest = 0;

        for i in 0..self.k {
            let centroid = centroids.row(i);
            let dist = self.euclidean_distance(&sample, &centroid);

            if dist < min_dist {
                min_dist = dist;
                closest = i;
            }
        }

        closest
    }

    fn euclidean_distance(&self, a: &ndarray::ArrayView1<f64>, b: &ndarray::ArrayView1<f64>) -> f64 {
        a.iter().zip(b.iter())
            .map(|(x, y)| (x - y).powi(2))
            .sum::<f64>()
            .sqrt()
    }

    fn update_centroids(&self, data: &Array2<f64>, labels: &[usize]) -> Array2<f64> {
        let (_, n_features) = data.dim();
        let mut centroids = Array2::zeros((self.k, n_features));
        let mut counts = vec![0; self.k];

        // 累加每个簇的样本
        for (i, &label) in labels.iter().enumerate() {
            let sample = data.row(i);
            for j in 0..n_features {
                centroids[[label, j]] += sample[j];
            }
            counts[label] += 1;
        }

        // 计算平均值
        for i in 0..self.k {
            if counts[i] > 0 {
                for j in 0..n_features {
                    centroids[[i, j]] /= counts[i] as f64;
                }
            }
        }

        centroids
    }

    fn converged(&self, old_centroids: &Array2<f64>, new_centroids: &Array2<f64>) -> bool {
        for i in 0..self.k {
            let old_centroid = old_centroids.row(i);
            let new_centroid = new_centroids.row(i);

            if self.euclidean_distance(&old_centroid, &new_centroid) > self.tolerance {
                return false;
            }
        }
        true
    }

    pub fn predict(&self, data: &Array2<f64>) -> Vec<usize> {
        let centroids = self.centroids.as_ref().expect("Model not fitted");
        let mut labels = Vec::new();

        for row in data.outer_iter() {
            labels.push(self.closest_centroid(row, centroids));
        }

        labels
    }
}
```

### 8.2 PCA实现

```rust
use ndarray::{Array1, Array2, Axis};
use ndarray_linalg::{Eig, UPLO};

pub struct PCA {
    n_components: usize,
    components: Option<Array2<f64>>,
    explained_variance: Option<Array1<f64>>,
    mean: Option<Array1<f64>>,
}

impl PCA {
    pub fn new(n_components: usize) -> Self {
        Self {
            n_components,
            components: None,
            explained_variance: None,
            mean: None,
        }
    }

    pub fn fit(&mut self, data: &Array2<f64>) {
        let (n_samples, n_features) = data.dim();

        // 计算均值
        let mean = data.mean_axis(Axis(0)).unwrap();

        // 中心化数据
        let centered_data = data - &mean.view().insert_axis(Axis(0));

        // 计算协方差矩阵
        let cov_matrix = centered_data.t().dot(&centered_data) / (n_samples - 1) as f64;

        // 特征值分解
        let (eigenvalues, eigenvectors) = cov_matrix.eig(UPLO::Upper).unwrap();

        // 按特征值降序排序
        let mut indices: Vec<usize> = (0..eigenvalues.len()).collect();
        indices.sort_by(|&i, &j| eigenvalues[j].partial_cmp(&eigenvalues[i]).unwrap());

        // 选择前n_components个主成分
        let mut components = Array2::zeros((self.n_components, n_features));
        let mut explained_variance = Array1::zeros(self.n_components);

        for (i, &idx) in indices.iter().take(self.n_components).enumerate() {
            components.row_mut(i).assign(&eigenvectors.column(idx));
            explained_variance[i] = eigenvalues[idx].re;
        }

        self.components = Some(components);
        self.explained_variance = Some(explained_variance);
        self.mean = Some(mean);
    }

    pub fn transform(&self, data: &Array2<f64>) -> Array2<f64> {
        let components = self.components.as_ref().expect("PCA not fitted");
        let mean = self.mean.as_ref().expect("PCA not fitted");

        // 中心化数据
        let centered_data = data - &mean.view().insert_axis(Axis(0));

        // 投影到主成分空间
        centered_data.dot(&components.t())
    }

    pub fn inverse_transform(&self, transformed_data: &Array2<f64>) -> Array2<f64> {
        let components = self.components.as_ref().expect("PCA not fitted");
        let mean = self.mean.as_ref().expect("PCA not fitted");

        // 从主成分空间重构
        let reconstructed = transformed_data.dot(components);

        // 添加均值
        reconstructed + &mean.view().insert_axis(Axis(0))
    }

    pub fn explained_variance_ratio(&self) -> Array1<f64> {
        let explained_variance = self.explained_variance.as_ref().expect("PCA not fitted");
        let total_variance = explained_variance.sum();
        explained_variance / total_variance
    }
}
```

### 8.3 DBSCAN实现

```rust
use std::collections::{HashSet, VecDeque};

# [derive(Debug, Clone, PartialEq)]
enum PointType {
    Core,
    Border,
    Noise,
}

pub struct DBSCAN {
    eps: f64,
    min_pts: usize,
}

impl DBSCAN {
    pub fn new(eps: f64, min_pts: usize) -> Self {
        Self { eps, min_pts }
    }

    pub fn fit_predict(&self, data: &Array2<f64>) -> Vec<i32> {
        let n_samples = data.nrows();
        let mut labels = vec![-1; n_samples]; // -1 表示噪声点
        let mut visited = vec![false; n_samples];
        let mut cluster_id = 0;

        for i in 0..n_samples {
            if visited[i] {
                continue;
            }

            visited[i] = true;
            let neighbors = self.get_neighbors(data, i);

            if neighbors.len() < self.min_pts {
                // 噪声点
                continue;
            }

            // 开始新簇
            self.expand_cluster(data, i, neighbors, cluster_id, &mut labels, &mut visited);
            cluster_id += 1;
        }

        labels
    }

    fn get_neighbors(&self, data: &Array2<f64>, point_idx: usize) -> Vec<usize> {
        let mut neighbors = Vec::new();
        let point = data.row(point_idx);

        for i in 0..data.nrows() {
            if i == point_idx {
                continue;
            }

            let other_point = data.row(i);
            let distance = self.euclidean_distance(&point, &other_point);

            if distance <= self.eps {
                neighbors.push(i);
            }
        }

        neighbors
    }

    fn expand_cluster(
        &self,
        data: &Array2<f64>,
        point_idx: usize,
        mut neighbors: Vec<usize>,
        cluster_id: i32,
        labels: &mut Vec<i32>,
        visited: &mut Vec<bool>,
    ) {
        labels[point_idx] = cluster_id;
        let mut queue = VecDeque::from(neighbors);

        while let Some(neighbor_idx) = queue.pop_front() {
            if !visited[neighbor_idx] {
                visited[neighbor_idx] = true;
                let neighbor_neighbors = self.get_neighbors(data, neighbor_idx);

                if neighbor_neighbors.len() >= self.min_pts {
                    queue.extend(neighbor_neighbors);
                }
            }

            if labels[neighbor_idx] == -1 {
                labels[neighbor_idx] = cluster_id;
            }
        }
    }

    fn euclidean_distance(&self, a: &ndarray::ArrayView1<f64>, b: &ndarray::ArrayView1<f64>) -> f64 {
        a.iter().zip(b.iter())
            .map(|(x, y)| (x - y).powi(2))
            .sum::<f64>()
            .sqrt()
    }
}
```

## 本地跳转链接

- [机器学习基础理论](./3.4.1-机器学习基础理论.md)
- [监督学习算法](./3.4.2-监督学习算法.md)
- [强化学习理论](./3.4.4-强化学习理论.md)
- [深度学习架构](./3.4.5-深度学习架构.md)
- [类型理论基础](../../2-形式科学理论/2.1-类型理论/2.1.1-类型理论基础.md)

## 参考文献

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning.
2. MacKay, D. J. (2003). Information theory, inference and learning algorithms.
3. Bishop, C. M. (2006). Pattern recognition and machine learning.
4. Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern classification.
