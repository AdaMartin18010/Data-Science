# 概率图模型

## 1. 目录

- [概率图模型](#概率图模型)
  - [1. 目录](#1-目录)
  - [2. 概率图模型基础](#2-概率图模型基础)
    - [2.1. 基本概念](#21-基本概念)
    - [2.2. 概率图模型的表示](#22-概率图模型的表示)
  - [3. 有向图模型（贝叶斯网络）](#3-有向图模型贝叶斯网络)
    - [3.1. 贝叶斯网络定义](#31-贝叶斯网络定义)
    - [3.2. d-分离](#32-d-分离)
    - [3.3. 常见贝叶斯网络模型](#33-常见贝叶斯网络模型)
  - [4. 无向图模型（马尔可夫随机场）](#4-无向图模型马尔可夫随机场)
    - [4.1. 马尔可夫随机场定义](#41-马尔可夫随机场定义)
    - [4.2. 条件随机场](#42-条件随机场)
    - [4.3. 常见马尔可夫随机场模型](#43-常见马尔可夫随机场模型)
  - [5. 概率图模型的推断](#5-概率图模型的推断)
    - [5.1. 精确推断](#51-精确推断)
    - [5.2. 近似推断](#52-近似推断)
  - [6. 概率图模型的学习](#6-概率图模型的学习)
    - [6.1. 参数学习](#61-参数学习)
    - [6.2. 结构学习](#62-结构学习)
  - [7. Rust实现的概率图模型](#7-rust实现的概率图模型)
    - [7.1. 贝叶斯网络实现](#71-贝叶斯网络实现)
    - [7.2. 马尔可夫随机场实现](#72-马尔可夫随机场实现)
  - [8. 概率图模型的应用](#8-概率图模型的应用)
    - [8.1. 自然语言处理](#81-自然语言处理)
    - [8.2. 计算机视觉](#82-计算机视觉)
    - [8.3. 生物信息学](#83-生物信息学)
    - [8.4. 推荐系统](#84-推荐系统)
  - [9. 总结与展望](#9-总结与展望)
  - [10. 多表征](#10-多表征)
  - [11. 形式化语义](#11-形式化语义)
  - [12. 形式化语法与证明](#12-形式化语法与证明)

## 2. 概率图模型基础

概率图模型(Probabilistic Graphical Models, PGMs)是结合概率论和图论的数学模型，用于表示随机变量之间的条件依赖关系。

### 2.1. 基本概念

**定义 1.1.1 (概率图模型)**
概率图模型是一个二元组 G = (V, E)，其中：

- V 是顶点集合，表示随机变量
- E 是边集合，表示随机变量之间的依赖关系

**定义 1.1.2 (条件独立性)**
给定随机变量集合 Z，如果随机变量集合 X 和 Y 满足：

```text
P(X, Y | Z) = P(X | Z) P(Y | Z)
```

则称 X 和 Y 在给定 Z 的条件下是条件独立的，记为 X ⊥ Y | Z。

**定义 1.1.3 (马尔可夫性质)**
在图模型中，给定一个节点的马尔可夫毯（父节点、子节点和子节点的其他父节点），该节点与其他所有节点条件独立。

### 2.2. 概率图模型的表示

**定义 1.2.1 (因子分解)**
联合概率分布可以分解为因子的乘积：

```text
P(X₁, X₂, ..., Xₙ) = (1/Z) ∏ᵢ Φᵢ(Xₖ)
```

其中 Z 是归一化常数，Φᵢ 是定义在变量子集上的因子。

**定理 1.2.1 (Hammersley-Clifford定理)**
如果概率分布 P 满足图 G 的马尔可夫性质，且 P > 0，则 P 可以分解为图 G 中最大团上的因子乘积。

## 3. 有向图模型（贝叶斯网络）

### 3.1. 贝叶斯网络定义

**定义 2.1.1 (贝叶斯网络)**
贝叶斯网络是一个有向无环图(DAG)，其中：

- 节点表示随机变量
- 边表示条件依赖关系
- 每个节点都有一个条件概率分布 P(Xᵢ | Pa(Xᵢ))，其中 Pa(Xᵢ) 是 Xᵢ 的父节点集合

**定义 2.1.2 (联合概率分布)**
贝叶斯网络中的联合概率分布可以表示为：

```text
P(X₁, X₂, ..., Xₙ) = ∏ᵢ P(Xᵢ | Pa(Xᵢ))
```

### 3.2. d-分离

**定义 2.2.1 (d-分离)**
在有向图中，如果路径 p 上的每个节点满足以下条件之一，则称节点集 Z d-分离节点集 X 和 Y：

1. 节点是连锁结构(X → Z → Y)，且 Z ∈ Z
2. 节点是分叉结构(X ← Z → Y)，且 Z ∈ Z
3. 节点是汇合结构(X → Z ← Y)，且 Z 及其所有后代都不在 Z 中

**定理 2.2.1 (d-分离与条件独立性)**
如果节点集 Z 在贝叶斯网络中 d-分离节点集 X 和 Y，则 X 和 Y 在给定 Z 的条件下是条件独立的。

### 3.3. 常见贝叶斯网络模型

1. **朴素贝叶斯模型**：假设所有特征在给定类别的条件下相互独立
2. **隐马尔可夫模型(HMM)**：包含观测序列和隐状态序列的时序模型
3. **动态贝叶斯网络(DBN)**：表示时序数据的贝叶斯网络扩展

## 4. 无向图模型（马尔可夫随机场）

### 4.1. 马尔可夫随机场定义

**定义 3.1.1 (马尔可夫随机场)**
马尔可夫随机场是一个无向图模型，其中：

- 节点表示随机变量
- 边表示变量之间的相互作用
- 满足局部马尔可夫性、成对马尔可夫性和全局马尔可夫性

**定义 3.1.2 (吉布斯分布)**
马尔可夫随机场中的联合概率分布可以表示为吉布斯分布：

```text
P(X) = (1/Z) exp(-Σᵏ ψₖ(Dₖ))
```

其中 Z 是归一化常数，ψₖ 是定义在最大团 Dₖ 上的势函数。

### 4.2. 条件随机场

**定义 3.2.1 (条件随机场)**
条件随机场(CRF)是一种判别式模型，定义为：

```text
P(Y | X) = (1/Z(X)) exp(Σₖ Σⱼ λₖfₖ(yⱼ₋₁, yⱼ, X, j))
```

其中 Z(X) 是归一化因子，fₖ 是特征函数，λₖ 是权重。

**定理 3.2.1 (CRF与最大熵模型)**
在适当的特征选择下，条件随机场等价于最大熵马尔可夫模型。

### 4.3. 常见马尔可夫随机场模型

1. **伊辛模型**：描述铁磁性物质中自旋相互作用的物理模型
2. **高斯马尔可夫随机场**：变量服从多元高斯分布的马尔可夫随机场
3. **条件随机场**：用于序列标注的判别式模型

## 5. 概率图模型的推断

### 5.1. 精确推断

**定义 4.1.1 (变量消除)**
变量消除是一种精确推断算法，通过按特定顺序消除变量来计算边缘概率。

**算法 4.1.1 (变量消除算法)**:

```text
输入：因子集合 F，查询变量 Q，消除顺序 O
输出：边缘概率 P(Q)
1. 对于消除顺序中的每个变量 X：
   a. 找出所有包含 X 的因子
   b. 将这些因子相乘得到新因子
   c. 对 X 求和，得到新的因子
   d. 用新因子替换原来的因子
2. 将剩余的因子相乘并归一化
```

**定义 4.1.2 (信念传播)**
信念传播是一种在树结构图上进行精确推断的算法。

**算法 4.1.2 (和积算法)**:

```text
1. 选择一个根节点，并定向树
2. 从叶节点向根节点传递消息（上行传递）
3. 从根节点向叶节点传递消息（下行传递）
4. 计算每个节点的边缘概率
```

**定理 4.1.1 (信念传播的收敛性)**
在树结构图上，信念传播算法在一次上行传递和一次下行传递后收敛到精确边缘概率。

### 5.2. 近似推断

**定义 4.2.1 (变分推断)**
变分推断通过优化一个变分分布 Q 来近似真实后验分布 P：

```text
min KL(Q || P)
```

**算法 4.2.1 (平均场变分推断)**:

```text
1. 假设 Q 可以分解为 Q(X) = ∏ᵢ Qᵢ(Xᵢ)
2. 对每个变量 Xᵢ，更新：
   Qᵢ(Xᵢ) ∝ exp(E_{Q_{-i}}[log P(X)])
3. 重复直到收敛
```

**定义 4.2.2 (蒙特卡洛方法)**
蒙特卡洛方法通过抽样来近似计算边缘概率和期望。

**算法 4.2.2 (吉布斯抽样)**:

```text
1. 初始化所有变量
2. 对于每次迭代：
   a. 对于每个变量 Xᵢ：
      i. 从条件分布 P(Xᵢ | X_{-i}) 中抽样
3. 使用样本计算统计量
```

**算法 4.2.3 (粒子滤波)**:

```text
1. 初始化粒子集合
2. 对于每个时间步：
   a. 根据转移模型预测粒子
   b. 根据观测更新粒子权重
   c. 重采样
```

## 6. 概率图模型的学习

### 6.1. 参数学习

**定义 5.1.1 (最大似然估计)**
给定数据 D，参数 θ 的最大似然估计为：

```text
θ_MLE = argmax_θ P(D | θ)
```

**算法 5.1.1 (期望最大化算法)**:

```text
1. 初始化参数 θ
2. E步：计算隐变量的后验分布 Q(Z) = P(Z | X, θ)
3. M步：更新参数 θ = argmax_θ E_Q[log P(X, Z | θ)]
4. 重复直到收敛
```

**定理 5.1.1 (EM算法的收敛性)**
EM算法保证每次迭代后似然函数单调递增，最终收敛到局部最优解。

### 6.2. 结构学习

**定义 5.2.1 (结构评分)**
结构评分函数用于评估图结构与数据的匹配程度：

```text
Score(G, D) = LL(G, D) - Penalty(G)
```

其中 LL 是对数似然，Penalty 是复杂度惩罚项。

**算法 5.2.1 (贪婪结构搜索)**:

```text
1. 初始化图结构（空图或完全图）
2. 迭代：
   a. 尝试添加、删除或反转一条边
   b. 选择使评分最大提升的操作
3. 直到评分不再提高
```

**定理 5.2.1 (结构可识别性)**
在大样本极限下，如果数据是从图 G 生成的，且评分函数一致，则学习算法将恢复出与 G 等价的图结构。

## 7. Rust实现的概率图模型

### 7.1. 贝叶斯网络实现

```rust
use std::collections::{HashMap, HashSet};

#[derive(Debug, Clone)]
struct BayesianNetwork {
    nodes: Vec<String>,
    edges: HashMap<String, HashSet<String>>,
    cpds: HashMap<String, ConditionalProbabilityTable>,
}

#[derive(Debug, Clone)]
struct ConditionalProbabilityTable {
    variable: String,
    parents: Vec<String>,
    values: HashMap<String, HashMap<String, f64>>,
}

impl BayesianNetwork {
    fn new() -> Self {
        BayesianNetwork {
            nodes: Vec::new(),
            edges: HashMap::new(),
            cpds: HashMap::new(),
        }
    }

    fn add_node(&mut self, node: &str) {
        if !self.nodes.contains(&node.to_string()) {
            self.nodes.push(node.to_string());
            self.edges.insert(node.to_string(), HashSet::new());
        }
    }

    fn add_edge(&mut self, from: &str, to: &str) -> Result<(), &'static str> {
        // 检查节点是否存在
        if !self.nodes.contains(&from.to_string()) || !self.nodes.contains(&to.to_string()) {
            return Err("节点不存在");
        }

        // 检查是否形成环
        if self.would_create_cycle(from, to) {
            return Err("添加边会形成环");
        }

        self.edges.get_mut(from).unwrap().insert(to.to_string());
        Ok(())
    }

    fn would_create_cycle(&self, from: &str, to: &str) -> bool {
        // 检查是否从to可以到达from
        let mut visited = HashSet::new();
        let mut queue = vec![to.to_string()];

        while let Some(node) = queue.pop() {
            if node == from {
                return true;
            }

            if visited.contains(&node) {
                continue;
            }

            visited.insert(node.clone());

            if let Some(children) = self.edges.get(&node) {
                for child in children {
                    queue.push(child.clone());
                }
            }
        }

        false
    }

    fn set_cpd(&mut self, variable: &str, cpd: ConditionalProbabilityTable) -> Result<(), &'static str> {
        if !self.nodes.contains(&variable.to_string()) {
            return Err("节点不存在");
        }

        self.cpds.insert(variable.to_string(), cpd);
        Ok(())
    }

    fn joint_probability(&self, assignment: &HashMap<String, String>) -> f64 {
        let mut prob = 1.0;

        for node in &self.nodes {
            if let Some(value) = assignment.get(node) {
                let parent_values = self.get_parent_values(node, assignment);
                if let Some(cpd) = self.cpds.get(node) {
                    prob *= cpd.probability(value, &parent_values);
                }
            }
        }

        prob
    }

    fn get_parent_values(&self, node: &str, assignment: &HashMap<String, String>) -> HashMap<String, String> {
        let mut parent_values = HashMap::new();

        for (parent, children) in &self.edges {
            if children.contains(&node.to_string()) {
                if let Some(value) = assignment.get(parent) {
                    parent_values.insert(parent.clone(), value.clone());
                }
            }
        }

        parent_values
    }
}

impl ConditionalProbabilityTable {
    fn new(variable: &str, parents: Vec<String>) -> Self {
        ConditionalProbabilityTable {
            variable: variable.to_string(),
            parents,
            values: HashMap::new(),
        }
    }

    fn set_probability(&mut self, value: &str, parent_values: &HashMap<String, String>, prob: f64) {
        let key = self.parent_values_key(parent_values);

        if !self.values.contains_key(&key) {
            self.values.insert(key.clone(), HashMap::new());
        }

        self.values.get_mut(&key).unwrap().insert(value.to_string(), prob);
    }

    fn probability(&self, value: &str, parent_values: &HashMap<String, String>) -> f64 {
        let key = self.parent_values_key(parent_values);

        if let Some(probs) = self.values.get(&key) {
            if let Some(prob) = probs.get(value) {
                return *prob;
            }
        }

        0.0
    }

    fn parent_values_key(&self, parent_values: &HashMap<String, String>) -> String {
        let mut parts = Vec::new();

        for parent in &self.parents {
            if let Some(value) = parent_values.get(parent) {
                parts.push(format!("{}={}", parent, value));
            } else {
                parts.push(format!("{}=?", parent));
            }
        }

        parts.join(",")
    }
}
```

### 7.2. 马尔可夫随机场实现

```rust
use std::collections::{HashMap, HashSet};

#[derive(Debug, Clone)]
struct MarkovRandomField {
    nodes: Vec<String>,
    edges: HashMap<String, HashSet<String>>,
    node_potentials: HashMap<String, HashMap<String, f64>>,
    edge_potentials: HashMap<(String, String), HashMap<(String, String), f64>>,
}

impl MarkovRandomField {
    fn new() -> Self {
        MarkovRandomField {
            nodes: Vec::new(),
            edges: HashMap::new(),
            node_potentials: HashMap::new(),
            edge_potentials: HashMap::new(),
        }
    }

    fn add_node(&mut self, node: &str) {
        if !self.nodes.contains(&node.to_string()) {
            self.nodes.push(node.to_string());
            self.edges.insert(node.to_string(), HashSet::new());
        }
    }

    fn add_edge(&mut self, node1: &str, node2: &str) -> Result<(), &'static str> {
        // 检查节点是否存在
        if !self.nodes.contains(&node1.to_string()) || !self.nodes.contains(&node2.to_string()) {
            return Err("节点不存在");
        }

        self.edges.get_mut(node1).unwrap().insert(node2.to_string());
        self.edges.get_mut(node2).unwrap().insert(node1.to_string());
        Ok(())
    }

    fn set_node_potential(&mut self, node: &str, value: &str, potential: f64) -> Result<(), &'static str> {
        if !self.nodes.contains(&node.to_string()) {
            return Err("节点不存在");
        }

        if !self.node_potentials.contains_key(&node.to_string()) {
            self.node_potentials.insert(node.to_string(), HashMap::new());
        }

        self.node_potentials.get_mut(&node.to_string()).unwrap().insert(value.to_string(), potential);
        Ok(())
    }

    fn set_edge_potential(&mut self, node1: &str, value1: &str, node2: &str, value2: &str, potential: f64) -> Result<(), &'static str> {
        // 检查边是否存在
        if !self.edges.get(&node1.to_string()).unwrap().contains(&node2.to_string()) {
            return Err("边不存在");
        }

        let key = if node1 < node2 {
            (node1.to_string(), node2.to_string())
        } else {
            (node2.to_string(), node1.to_string())
        };

        if !self.edge_potentials.contains_key(&key) {
            self.edge_potentials.insert(key.clone(), HashMap::new());
        }

        let value_key = if node1 < node2 {
            (value1.to_string(), value2.to_string())
        } else {
            (value2.to_string(), value1.to_string())
        };

        self.edge_potentials.get_mut(&key).unwrap().insert(value_key, potential);
        Ok(())
    }

    fn unnormalized_probability(&self, assignment: &HashMap<String, String>) -> f64 {
        let mut prob = 1.0;

        // 节点势能
        for node in &self.nodes {
            if let Some(value) = assignment.get(node) {
                if let Some(potentials) = self.node_potentials.get(node) {
                    if let Some(potential) = potentials.get(value) {
                        prob *= *potential;
                    }
                }
            }
        }

        // 边势能
        for i in 0..self.nodes.len() {
            let node1 = &self.nodes[i];
            if let Some(neighbors) = self.edges.get(node1) {
                for node2 in neighbors {
                    if self.nodes.iter().position(|n| n == node2).unwrap() > i {
                        if let (Some(value1), Some(value2)) = (assignment.get(node1), assignment.get(node2)) {
                            let key = if node1 < node2 {
                                (node1.clone(), node2.clone())
                            } else {
                                (node2.clone(), node1.clone())
                            };

                            let value_key = if node1 < node2 {
                                (value1.clone(), value2.clone())
                            } else {
                                (value2.clone(), value1.clone())
                            };

                            if let Some(potentials) = self.edge_potentials.get(&key) {
                                if let Some(potential) = potentials.get(&value_key) {
                                    prob *= *potential;
                                }
                            }
                        }
                    }
                }
            }
        }

        prob
    }
}
```

## 8. 概率图模型的应用

### 8.1. 自然语言处理

1. **词性标注**：使用条件随机场模型
2. **命名实体识别**：使用半马尔可夫条件随机场
3. **语法分析**：使用概率上下文无关文法

### 8.2. 计算机视觉

1. **图像分割**：使用马尔可夫随机场模型
2. **目标检测**：使用条件随机场模型
3. **场景理解**：使用层次化贝叶斯模型

### 8.3. 生物信息学

1. **基因调控网络**：使用贝叶斯网络模型
2. **蛋白质结构预测**：使用马尔可夫随机场模型
3. **系统生物学**：使用动态贝叶斯网络

### 8.4. 推荐系统

1. **协同过滤**：使用概率矩阵分解
2. **上下文感知推荐**：使用条件随机场模型
3. **序列推荐**：使用动态贝叶斯网络

## 9. 总结与展望

概率图模型是一种强大的表示和推理工具，结合了概率论的严谨性和图论的直观性。从贝叶斯网络到马尔可夫随机场，从精确推断到近似推断，从参数学习到结构学习，形成了一个完整的理论体系。随着深度学习的发展，概率图模型也在不断演进，如深度生成模型、变分自编码器、图神经网络等。未来的研究方向包括大规模概率图模型、因果推断、概率编程等，以应对更加复杂的人工智能任务。

## 10. 多表征

概率图模型支持多种表征方式，包括：

- 符号表征（概率分布、图结构、模型公式、推断规则等）
- 图结构（贝叶斯网络、马尔可夫网络、因子图等）
- 向量/张量（概率向量、参数矩阵、嵌入）
- 自然语言（定义、注释、描述）
- 图像/可视化（网络结构图、推断流程图、概率分布可视化等）
这些表征可互映，提升概率图模型的表达力。

## 11. 形式化语义

- 语义域：$D$，如变量空间、概率分布空间、图结构空间、参数空间
- 解释函数：$I: S \to D$，将符号/结构映射到具体概率语义对象
- 语义一致性：每个变量/分布/图结构/公式在$D$中有明确定义

## 12. 形式化语法与证明

- 语法规则：如图结构定义、概率分布规则、推断规则、约束条件
- **定理**：概率图模型的语法系统具一致性与可扩展性。
- **证明**：由图结构、概率分布与推断规则递归定义，保证系统一致与可扩展。
