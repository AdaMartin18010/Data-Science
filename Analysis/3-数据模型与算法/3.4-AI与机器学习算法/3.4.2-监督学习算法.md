# 3.4.2 ç›‘ç£å­¦ä¹ ç®—æ³•

## ğŸ“‘ ç›®å½•

- [3.4.2 ç›‘ç£å­¦ä¹ ç®—æ³•](#342-ç›‘ç£å­¦ä¹ ç®—æ³•)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ‘˜è¦](#1-æ‘˜è¦)
  - [3. å¼•è¨€](#3-å¼•è¨€)
    - [3.1. ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶](#31-ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶)
    - [3.2. é—®é¢˜ç±»å‹åˆ†ç±»](#32-é—®é¢˜ç±»å‹åˆ†ç±»)
  - [4. åˆ†ç±»ç®—æ³•](#4-åˆ†ç±»ç®—æ³•)
    - [4.1. çº¿æ€§åˆ†ç±»å™¨](#41-çº¿æ€§åˆ†ç±»å™¨)
      - [4.1.1. é€»è¾‘å›å½’ (Logistic Regression)](#411-é€»è¾‘å›å½’-logistic-regression)
      - [4.1.2. æ”¯æŒå‘é‡æœº (SVM)](#412-æ”¯æŒå‘é‡æœº-svm)
    - [4.2. å†³ç­–æ ‘](#42-å†³ç­–æ ‘)
      - [4.2.1. ä¿¡æ¯å¢ç›Š](#421-ä¿¡æ¯å¢ç›Š)
      - [4.2.2. åŸºå°¼ä¸çº¯åº¦](#422-åŸºå°¼ä¸çº¯åº¦)
      - [4.2.3. å†³ç­–æ ‘æ„å»ºç®—æ³•](#423-å†³ç­–æ ‘æ„å»ºç®—æ³•)
    - [4.3. kè¿‘é‚» (k-NN)](#43-kè¿‘é‚»-k-nn)
  - [5. å›å½’ç®—æ³•](#5-å›å½’ç®—æ³•)
    - [5.1. çº¿æ€§å›å½’](#51-çº¿æ€§å›å½’)
    - [5.2. æ­£åˆ™åŒ–å›å½’](#52-æ­£åˆ™åŒ–å›å½’)
  - [6. é›†æˆå­¦ä¹ æ–¹æ³•](#6-é›†æˆå­¦ä¹ æ–¹æ³•)
    - [6.1. Bagging (Bootstrap Aggregating)](#61-bagging-bootstrap-aggregating)
    - [6.2. Boosting](#62-boosting)
    - [6.3. æ¢¯åº¦æå‡](#63-æ¢¯åº¦æå‡)
  - [7. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©](#7-æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©)
    - [7.1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡](#71-åˆ†ç±»æ€§èƒ½æŒ‡æ ‡)
    - [7.2. å›å½’æ€§èƒ½æŒ‡æ ‡](#72-å›å½’æ€§èƒ½æŒ‡æ ‡)
    - [7.3. äº¤å‰éªŒè¯](#73-äº¤å‰éªŒè¯)
  - [8. å·¥ç¨‹å®ç°](#8-å·¥ç¨‹å®ç°)
    - [8.1. Rustå®ç°ç¤ºä¾‹](#81-rustå®ç°ç¤ºä¾‹)
    - [8.2. æ¨¡å‹è¯„ä¼°å·¥å…·](#82-æ¨¡å‹è¯„ä¼°å·¥å…·)
    - [8.3. äº¤å‰éªŒè¯å®ç°](#83-äº¤å‰éªŒè¯å®ç°)
  - [9. ä¸å…¶ä»–ç†è®ºçš„å…³è”](#9-ä¸å…¶ä»–ç†è®ºçš„å…³è”)
    - [9.1. ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»](#91-ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»)
    - [9.2. ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ](#92-ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ)
    - [9.3. ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»](#93-ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»)
    - [9.4. ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ](#94-ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ)
  - [10. æœ¬åœ°è·³è½¬](#10-æœ¬åœ°è·³è½¬)
  - [11. ç‰ˆæœ¬ä¿¡æ¯](#11-ç‰ˆæœ¬ä¿¡æ¯)
  - [12. å¤šè¡¨å¾](#12-å¤šè¡¨å¾)
  - [13. å½¢å¼åŒ–è¯­ä¹‰](#13-å½¢å¼åŒ–è¯­ä¹‰)
  - [14. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜](#14-å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜)

---


## 1. æ‘˜è¦

æœ¬æ–‡ç³»ç»Ÿé˜è¿°ç›‘ç£å­¦ä¹ ç®—æ³•çš„ç†è®ºåŸºç¡€ã€æ•°å­¦æ¨¡å‹å’Œå·¥ç¨‹å®ç°ã€‚ä»ç»å…¸çš„çº¿æ€§æ¨¡å‹åˆ°ç°ä»£æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä»å•ä¸€ç®—æ³•åˆ°é›†æˆå­¦ä¹ ï¼Œå…¨é¢æ„å»ºç›‘ç£å­¦ä¹ çš„çŸ¥è¯†ä½“ç³»ã€‚ç»“åˆæ•°å­¦å½¢å¼åŒ–å’Œå®é™…åº”ç”¨æ¡ˆä¾‹ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½æœºå™¨å­¦ä¹ ç³»ç»Ÿæä¾›å®Œæ•´çš„ç†è®ºå’Œå®è·µæŒ‡å¯¼ã€‚

- [3.4.2 ç›‘ç£å­¦ä¹ ç®—æ³•](#342-ç›‘ç£å­¦ä¹ ç®—æ³•)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ‘˜è¦](#1-æ‘˜è¦)
  - [3. å¼•è¨€](#3-å¼•è¨€)
    - [3.1. ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶](#31-ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶)
    - [3.2. é—®é¢˜ç±»å‹åˆ†ç±»](#32-é—®é¢˜ç±»å‹åˆ†ç±»)
  - [4. åˆ†ç±»ç®—æ³•](#4-åˆ†ç±»ç®—æ³•)
    - [4.1. çº¿æ€§åˆ†ç±»å™¨](#41-çº¿æ€§åˆ†ç±»å™¨)
      - [4.1.1. é€»è¾‘å›å½’ (Logistic Regression)](#411-é€»è¾‘å›å½’-logistic-regression)
      - [4.1.2. æ”¯æŒå‘é‡æœº (SVM)](#412-æ”¯æŒå‘é‡æœº-svm)
    - [4.2. å†³ç­–æ ‘](#42-å†³ç­–æ ‘)
      - [4.2.1. ä¿¡æ¯å¢ç›Š](#421-ä¿¡æ¯å¢ç›Š)
      - [4.2.2. åŸºå°¼ä¸çº¯åº¦](#422-åŸºå°¼ä¸çº¯åº¦)
      - [4.2.3. å†³ç­–æ ‘æ„å»ºç®—æ³•](#423-å†³ç­–æ ‘æ„å»ºç®—æ³•)
    - [4.3. kè¿‘é‚» (k-NN)](#43-kè¿‘é‚»-k-nn)
  - [5. å›å½’ç®—æ³•](#5-å›å½’ç®—æ³•)
    - [5.1. çº¿æ€§å›å½’](#51-çº¿æ€§å›å½’)
    - [5.2. æ­£åˆ™åŒ–å›å½’](#52-æ­£åˆ™åŒ–å›å½’)
  - [6. é›†æˆå­¦ä¹ æ–¹æ³•](#6-é›†æˆå­¦ä¹ æ–¹æ³•)
    - [6.1. Bagging (Bootstrap Aggregating)](#61-bagging-bootstrap-aggregating)
    - [6.2. Boosting](#62-boosting)
    - [6.3. æ¢¯åº¦æå‡](#63-æ¢¯åº¦æå‡)
  - [7. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©](#7-æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©)
    - [7.1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡](#71-åˆ†ç±»æ€§èƒ½æŒ‡æ ‡)
    - [7.2. å›å½’æ€§èƒ½æŒ‡æ ‡](#72-å›å½’æ€§èƒ½æŒ‡æ ‡)
    - [7.3. äº¤å‰éªŒè¯](#73-äº¤å‰éªŒè¯)
  - [8. å·¥ç¨‹å®ç°](#8-å·¥ç¨‹å®ç°)
    - [8.1. Rustå®ç°ç¤ºä¾‹](#81-rustå®ç°ç¤ºä¾‹)
    - [8.2. æ¨¡å‹è¯„ä¼°å·¥å…·](#82-æ¨¡å‹è¯„ä¼°å·¥å…·)
    - [8.3. äº¤å‰éªŒè¯å®ç°](#83-äº¤å‰éªŒè¯å®ç°)
  - [9. ä¸å…¶ä»–ç†è®ºçš„å…³è”](#9-ä¸å…¶ä»–ç†è®ºçš„å…³è”)
    - [9.1. ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»](#91-ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»)
    - [9.2. ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ](#92-ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ)
    - [9.3. ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»](#93-ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»)
    - [9.4. ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ](#94-ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ)
  - [10. æœ¬åœ°è·³è½¬](#10-æœ¬åœ°è·³è½¬)
  - [11. ç‰ˆæœ¬ä¿¡æ¯](#11-ç‰ˆæœ¬ä¿¡æ¯)
  - [12. å¤šè¡¨å¾](#12-å¤šè¡¨å¾)
  - [13. å½¢å¼åŒ–è¯­ä¹‰](#13-å½¢å¼åŒ–è¯­ä¹‰)
  - [14. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜](#14-å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜)

## 3. å¼•è¨€

ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒèŒƒå¼ï¼Œé€šè¿‡æ ‡æ³¨æ•°æ®å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚ç»™å®šè®­ç»ƒé›† $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ï¼Œç›®æ ‡æ˜¯å­¦ä¹ å‡½æ•° $f: \mathcal{X} \rightarrow \mathcal{Y}$ï¼Œä½¿å¾—åœ¨æ–°æ ·æœ¬ä¸Šçš„æ³›åŒ–è¯¯å·®æœ€å°ã€‚

### 3.1. ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶

**æŸå¤±å‡½æ•°**: è¡¡é‡é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
$$L(f, \mathcal{D}) = \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)$$

**ç»éªŒé£é™©æœ€å°åŒ–**:
$$\hat{f} = \arg\min_{f \in \mathcal{F}} L(f, \mathcal{D}) + \lambda R(f)$$

å…¶ä¸­ $R(f)$ æ˜¯æ­£åˆ™åŒ–é¡¹ï¼Œ$\lambda$ æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚

### 3.2. é—®é¢˜ç±»å‹åˆ†ç±»

- **äºŒåˆ†ç±»**: $\mathcal{Y} = \{0, 1\}$ æˆ– $\{-1, +1\}$
- **å¤šåˆ†ç±»**: $\mathcal{Y} = \{1, 2, \ldots, K\}$ï¼Œ$K > 2$
- **å¤šæ ‡ç­¾**: æ¯ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾
- **å›å½’**: $\mathcal{Y} = \mathbb{R}$ æˆ– $\mathcal{Y} = \mathbb{R}^d$

## 4. åˆ†ç±»ç®—æ³•

### 4.1. çº¿æ€§åˆ†ç±»å™¨

#### 4.1.1. é€»è¾‘å›å½’ (Logistic Regression)

é€»è¾‘å›å½’æ˜¯ç”¨äºäºŒåˆ†ç±»é—®é¢˜çš„çº¿æ€§æ¨¡å‹ï¼Œé€šè¿‡sigmoidå‡½æ•°å°†çº¿æ€§ç»„åˆæ˜ å°„åˆ°æ¦‚ç‡ç©ºé—´ã€‚

**sigmoidå‡½æ•°**:
$$\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}$$

**æ€§è´¨**ï¼š

- å€¼åŸŸï¼š$(0, 1)$
- å•è°ƒé€’å¢
- å¯¼æ•°ï¼š$\sigma'(z) = \sigma(z)(1-\sigma(z))$

**æ¦‚ç‡æ¨¡å‹**:
$$P(y=1|x; \theta) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}$$
$$P(y=0|x; \theta) = 1 - \sigma(\theta^T x) = \frac{e^{-\theta^T x}}{1 + e^{-\theta^T x}}$$

**å¯¹æ•°å‡ ç‡ï¼ˆLogitï¼‰**:
$$\text{logit}(p) = \log\frac{p}{1-p} = \theta^T x$$

**æŸå¤±å‡½æ•°** (å¯¹æ•°ä¼¼ç„¶):
$$\ell(\theta) = -\sum_{i=1}^n [y_i \log \sigma(\theta^T x_i) + (1-y_i) \log(1-\sigma(\theta^T x_i))]$$

**æ¢¯åº¦**:
$$\nabla_\theta \ell = \sum_{i=1}^n (\sigma(\theta^T x_i) - y_i) x_i$$

**HessiançŸ©é˜µ**:
$$H = \sum_{i=1}^n \sigma(\theta^T x_i)(1-\sigma(\theta^T x_i)) x_i x_i^T$$

HessiançŸ©é˜µæ˜¯åŠæ­£å®šçš„ï¼Œå› æ­¤æŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œä¿è¯å…¨å±€æœ€ä¼˜è§£ã€‚

**å®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from scipy.optimize import minimize

class LogisticRegression:
    """é€»è¾‘å›å½’å®ç°"""

    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.tol = tol
        self.theta = None

    def sigmoid(self, z):
        """Sigmoidå‡½æ•°"""
        # é˜²æ­¢æº¢å‡º
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    def loss(self, theta, X, y):
        """æŸå¤±å‡½æ•°ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰"""
        h = self.sigmoid(X @ theta)
        # é¿å…log(0)
        epsilon = 1e-15
        h = np.clip(h, epsilon, 1 - epsilon)
        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))
        return loss

    def gradient(self, theta, X, y):
        """æ¢¯åº¦è®¡ç®—"""
        h = self.sigmoid(X @ theta)
        grad = X.T @ (h - y) / len(y)
        return grad

    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        # æ·»åŠ åç½®é¡¹
        X = np.column_stack([np.ones(len(X)), X])

        # åˆå§‹åŒ–å‚æ•°
        self.theta = np.zeros(X.shape[1])

        # ä½¿ç”¨æ¢¯åº¦ä¸‹é™
        for i in range(self.max_iter):
            grad = self.gradient(self.theta, X, y)
            self.theta -= self.learning_rate * grad

            # æ£€æŸ¥æ”¶æ•›
            if np.linalg.norm(grad) < self.tol:
                break

        return self

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X = np.column_stack([np.ones(len(X)), X])
        return self.sigmoid(X @ self.theta)

    def predict(self, X, threshold=0.5):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba >= threshold).astype(int)

# ä½¿ç”¨ç¤ºä¾‹
X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 2]])
y = np.array([0, 0, 1, 1, 1])

model = LogisticRegression()
model.fit(X, y)
predictions = model.predict(X)
probabilities = model.predict_proba(X)
print(f"é¢„æµ‹: {predictions}")
print(f"æ¦‚ç‡: {probabilities}")
```

#### 4.1.2. æ”¯æŒå‘é‡æœº (SVM)

æ”¯æŒå‘é‡æœºé€šè¿‡å¯»æ‰¾æœ€å¤§é—´éš”è¶…å¹³é¢æ¥å®ç°åˆ†ç±»ã€‚

**ç¡¬é—´éš”SVM**ï¼š

å¯¹äºçº¿æ€§å¯åˆ†æ•°æ®ï¼Œå¯»æ‰¾æœ€å¤§é—´éš”è¶…å¹³é¢ï¼š
$$\min_{\theta, b} \frac{1}{2}\|\theta\|^2$$
$$\text{s.t.} \quad y_i(\theta^T x_i + b) \geq 1, \quad \forall i$$

**å¯¹å¶é—®é¢˜**ï¼ˆæ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼‰ï¼š
$$\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j$$
$$\text{s.t.} \quad \sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \geq 0$$

**KKTæ¡ä»¶**ï¼š

- $\alpha_i \geq 0$
- $y_i(\theta^T x_i + b) - 1 \geq 0$
- $\alpha_i[y_i(\theta^T x_i + b) - 1] = 0$

**æ”¯æŒå‘é‡**ï¼š$\alpha_i > 0$å¯¹åº”çš„æ ·æœ¬ç‚¹ã€‚

**è½¯é—´éš”SVM**ï¼š

å…è®¸åˆ†ç±»é”™è¯¯ï¼Œå¼•å…¥æ¾å¼›å˜é‡ï¼š
$$\min_{\theta, b, \xi} \frac{1}{2}\|\theta\|^2 + C \sum_{i=1}^n \xi_i$$
$$\text{s.t.} \quad y_i(\theta^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

å…¶ä¸­$C > 0$æ˜¯æƒ©ç½šå‚æ•°ï¼Œæ§åˆ¶é—´éš”å¤§å°å’Œåˆ†ç±»é”™è¯¯çš„æƒè¡¡ã€‚

**å¯¹å¶é—®é¢˜**ï¼š
$$\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j$$
$$\text{s.t.} \quad \sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C$$

**æ ¸æŠ€å·§**: é€šè¿‡æ ¸å‡½æ•° $K(x, x') = \phi(x)^T \phi(x')$ å®ç°éçº¿æ€§åˆ†ç±»

**å¸¸ç”¨æ ¸å‡½æ•°**ï¼š

1. **çº¿æ€§æ ¸**: $K(x, x') = x^T x'$
2. **å¤šé¡¹å¼æ ¸**: $K(x, x') = (\gamma x^T x' + r)^d$
3. **RBFæ ¸ï¼ˆé«˜æ–¯æ ¸ï¼‰**: $K(x, x') = \exp(-\gamma \|x - x'\|^2)$
4. **Sigmoidæ ¸**: $K(x, x') = \tanh(\gamma x^T x' + r)$

**å®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from cvxopt import matrix, solvers

class SVM:
    """æ”¯æŒå‘é‡æœºå®ç°ï¼ˆä½¿ç”¨äºŒæ¬¡è§„åˆ’æ±‚è§£ï¼‰"""

    def __init__(self, C=1.0, kernel='linear', gamma='scale'):
        self.C = C
        self.kernel = kernel
        self.gamma = gamma
        self.alpha = None
        self.support_vectors = None
        self.support_vector_labels = None
        self.b = None

    def linear_kernel(self, X1, X2):
        """çº¿æ€§æ ¸"""
        return X1 @ X2.T

    def rbf_kernel(self, X1, X2):
        """RBFæ ¸"""
        if self.gamma == 'scale':
            gamma = 1.0 / (X1.shape[1] * X1.var())
        else:
            gamma = self.gamma

        pairwise_dists = np.sum(X1**2, axis=1).reshape(-1, 1) + \
                        np.sum(X2**2, axis=1) - 2 * X1 @ X2.T
        return np.exp(-gamma * pairwise_dists)

    def polynomial_kernel(self, X1, X2, degree=3, gamma=1.0, coef0=0.0):
        """å¤šé¡¹å¼æ ¸"""
        return (gamma * X1 @ X2.T + coef0) ** degree

    def get_kernel_matrix(self, X1, X2):
        """è·å–æ ¸çŸ©é˜µ"""
        if self.kernel == 'linear':
            return self.linear_kernel(X1, X2)
        elif self.kernel == 'rbf':
            return self.rbf_kernel(X1, X2)
        elif self.kernel == 'polynomial':
            return self.polynomial_kernel(X1, X2)
        else:
            raise ValueError(f"Unknown kernel: {self.kernel}")

    def fit(self, X, y):
        """è®­ç»ƒSVM"""
        n_samples, n_features = X.shape

        # è½¬æ¢ä¸º{-1, +1}æ ‡ç­¾
        y = np.where(y == 0, -1, 1)

        # è®¡ç®—æ ¸çŸ©é˜µ
        K = self.get_kernel_matrix(X, X)

        # æ„å»ºäºŒæ¬¡è§„åˆ’é—®é¢˜
        # min (1/2) alpha^T P alpha - q^T alpha
        # s.t. G alpha <= h, A alpha = b
        P = matrix(np.outer(y, y) * K)
        q = matrix(-np.ones(n_samples))
        G = matrix(np.vstack([-np.eye(n_samples), np.eye(n_samples)]))
        h = matrix(np.hstack([np.zeros(n_samples), np.ones(n_samples) * self.C]))
        A = matrix(y.astype(float).reshape(1, -1))
        b = matrix(0.0)

        # æ±‚è§£
        solvers.options['show_progress'] = False
        solution = solvers.qp(P, q, G, h, A, b)
        self.alpha = np.array(solution['x']).flatten()

        # æ‰¾åˆ°æ”¯æŒå‘é‡
        sv_indices = self.alpha > 1e-5
        self.support_vectors = X[sv_indices]
        self.support_vector_labels = y[sv_indices]
        self.alpha_sv = self.alpha[sv_indices]

        # è®¡ç®—åç½®b
        if self.kernel == 'linear':
            self.w = np.sum(self.alpha_sv * self.support_vector_labels *
                           self.support_vectors.T, axis=1)
            self.b = np.mean(self.support_vector_labels -
                            self.support_vectors @ self.w)
        else:
            # å¯¹äºéçº¿æ€§æ ¸ï¼Œä½¿ç”¨æ”¯æŒå‘é‡è®¡ç®—b
            K_sv = self.get_kernel_matrix(self.support_vectors, self.support_vectors)
            self.b = np.mean(self.support_vector_labels -
                            K_sv.T @ (self.alpha_sv * self.support_vector_labels))

        return self

    def predict(self, X):
        """é¢„æµ‹"""
        if self.kernel == 'linear':
            scores = X @ self.w + self.b
        else:
            K = self.get_kernel_matrix(X, self.support_vectors)
            scores = K @ (self.alpha_sv * self.support_vector_labels) + self.b

        return np.sign(scores).astype(int)

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
                          n_informative=2, random_state=1, n_clusters_per_class=1)

svm = SVM(C=1.0, kernel='rbf')
svm.fit(X, y)
predictions = svm.predict(X)
print(f"å‡†ç¡®ç‡: {np.mean(predictions == y):.4f}")
```

### 4.2. å†³ç­–æ ‘

å†³ç­–æ ‘é€šè¿‡é€’å½’åˆ†å‰²æ•°æ®æ„å»ºæ ‘å½¢ç»“æ„è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚

#### 4.2.1. ä¿¡æ¯å¢ç›Š

**ä¿¡æ¯ç†µ**:
$$H(S) = -\sum_{c \in C} p_c \log_2 p_c$$

å…¶ä¸­$p_c$æ˜¯ç±»åˆ«$c$åœ¨é›†åˆ$S$ä¸­çš„æ¯”ä¾‹ã€‚

**ä¿¡æ¯å¢ç›Š**:
$$\text{IG}(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)$$

å…¶ä¸­$S_v$æ˜¯å±æ€§$A$å–å€¼ä¸º$v$çš„æ ·æœ¬å­é›†ã€‚

**ä¿¡æ¯å¢ç›Šæ¯”**ï¼ˆC4.5ç®—æ³•ï¼‰:
$$\text{IGR}(S, A) = \frac{\text{IG}(S, A)}{H_A(S)}$$

å…¶ä¸­$H_A(S) = -\sum_{v} \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}$æ˜¯å±æ€§$A$çš„ç†µã€‚

#### 4.2.2. åŸºå°¼ä¸çº¯åº¦

**åŸºå°¼ä¸çº¯åº¦**:
$$\text{Gini}(S) = 1 - \sum_{c \in C} p_c^2$$

**åŸºå°¼å¢ç›Š**:
$$\text{GiniGain}(S, A) = \text{Gini}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Gini}(S_v)$$

#### 4.2.3. å†³ç­–æ ‘æ„å»ºç®—æ³•

**ID3ç®—æ³•**ï¼ˆä½¿ç”¨ä¿¡æ¯å¢ç›Šï¼‰ï¼š

1. å¦‚æœæ‰€æœ‰æ ·æœ¬å±äºåŒä¸€ç±»ï¼Œè¿”å›å¶èŠ‚ç‚¹
2. å¦‚æœå±æ€§é›†ä¸ºç©ºï¼Œè¿”å›å¤šæ•°ç±»
3. é€‰æ‹©ä¿¡æ¯å¢ç›Šæœ€å¤§çš„å±æ€§ä½œä¸ºåˆ†è£‚å±æ€§
4. å¯¹æ¯ä¸ªå±æ€§å€¼åˆ›å»ºåˆ†æ”¯ï¼Œé€’å½’æ„å»ºå­æ ‘

**CARTç®—æ³•**ï¼ˆä½¿ç”¨åŸºå°¼ä¸çº¯åº¦ï¼‰ï¼š

- æ”¯æŒåˆ†ç±»å’Œå›å½’
- ä½¿ç”¨äºŒå‰æ ‘ç»“æ„
- æ”¯æŒè¿ç»­å€¼å±æ€§

**å®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from collections import Counter

class DecisionTree:
    """å†³ç­–æ ‘å®ç°ï¼ˆID3ç®—æ³•ï¼‰"""

    def __init__(self, max_depth=10, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def entropy(self, y):
        """è®¡ç®—ç†µ"""
        if len(y) == 0:
            return 0
        counts = Counter(y)
        probs = [count / len(y) for count in counts.values()]
        return -sum(p * np.log2(p) for p in probs if p > 0)

    def information_gain(self, X, y, feature_idx):
        """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
        parent_entropy = self.entropy(y)

        # æŒ‰ç‰¹å¾å€¼åˆ†ç»„
        feature_values = X[:, feature_idx]
        unique_values = np.unique(feature_values)

        weighted_entropy = 0
        for value in unique_values:
            mask = feature_values == value
            subset_y = y[mask]
            weight = len(subset_y) / len(y)
            weighted_entropy += weight * self.entropy(subset_y)

        return parent_entropy - weighted_entropy

    def best_feature(self, X, y, used_features):
        """é€‰æ‹©æœ€ä½³åˆ†è£‚ç‰¹å¾"""
        best_gain = -1
        best_feature = None

        for i in range(X.shape[1]):
            if i in used_features:
                continue
            gain = self.information_gain(X, y, i)
            if gain > best_gain:
                best_gain = gain
                best_feature = i

        return best_feature

    def build_tree(self, X, y, depth=0, used_features=set()):
        """é€’å½’æ„å»ºå†³ç­–æ ‘"""
        # ç»ˆæ­¢æ¡ä»¶
        if depth >= self.max_depth or len(y) < self.min_samples_split:
            return Counter(y).most_common(1)[0][0]

        # å¦‚æœæ‰€æœ‰æ ·æœ¬å±äºåŒä¸€ç±»
        if len(set(y)) == 1:
            return y[0]

        # å¦‚æœæ²¡æœ‰å¯ç”¨ç‰¹å¾
        if len(used_features) >= X.shape[1]:
            return Counter(y).most_common(1)[0][0]

        # é€‰æ‹©æœ€ä½³ç‰¹å¾
        best_feature = self.best_feature(X, y, used_features)
        if best_feature is None:
            return Counter(y).most_common(1)[0][0]

        # åˆ›å»ºèŠ‚ç‚¹
        tree = {'feature': best_feature, 'children': {}}

        # é€’å½’æ„å»ºå­æ ‘
        feature_values = X[:, best_feature]
        unique_values = np.unique(feature_values)
        new_used_features = used_features | {best_feature}

        for value in unique_values:
            mask = feature_values == value
            subset_X = X[mask]
            subset_y = y[mask]

            if len(subset_y) == 0:
                tree['children'][value] = Counter(y).most_common(1)[0][0]
            else:
                tree['children'][value] = self.build_tree(
                    subset_X, subset_y, depth + 1, new_used_features
                )

        return tree

    def fit(self, X, y):
        """è®­ç»ƒå†³ç­–æ ‘"""
        self.tree = self.build_tree(X, y)
        return self

    def predict_sample(self, x, tree):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if isinstance(tree, dict):
            feature_idx = tree['feature']
            feature_value = x[feature_idx]
            if feature_value in tree['children']:
                return self.predict_sample(x, tree['children'][feature_value])
            else:
                # å¦‚æœç‰¹å¾å€¼æœªè§è¿‡ï¼Œè¿”å›æœ€å¸¸è§çš„ç±»åˆ«
                return None
        else:
            return tree

    def predict(self, X):
        """é¢„æµ‹"""
        return np.array([self.predict_sample(x, self.tree) for x in X])

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target

tree = DecisionTree(max_depth=5)
tree.fit(X, y)
predictions = tree.predict(X)
print(f"å‡†ç¡®ç‡: {np.mean(predictions == y):.4f}")
```

### 4.3. kè¿‘é‚» (k-NN)

kè¿‘é‚»æ˜¯ä¸€ç§åŸºäºå®ä¾‹çš„æ‡’æƒ°å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡æŸ¥æ‰¾è®­ç»ƒé›†ä¸­æœ€è¿‘çš„kä¸ªæ ·æœ¬æ¥è¿›è¡Œé¢„æµ‹ã€‚

**ç®—æ³•åŸç†**ï¼š

å¯¹äºæµ‹è¯•æ ·æœ¬$x$ï¼Œæ‰¾åˆ°è®­ç»ƒé›†ä¸­è·ç¦»æœ€è¿‘çš„$k$ä¸ªæ ·æœ¬$N_k(x)$ï¼Œç„¶åï¼š

- **åˆ†ç±»**ï¼šå¤šæ•°æŠ•ç¥¨
- **å›å½’**ï¼šå¹³å‡å€¼æˆ–åŠ æƒå¹³å‡

**è·ç¦»åº¦é‡**ï¼š

1. **æ¬§æ°è·ç¦»**ï¼š$d(x, x') = \sqrt{\sum_{i=1}^d (x_i - x'_i)^2}$
2. **æ›¼å“ˆé¡¿è·ç¦»**ï¼š$d(x, x') = \sum_{i=1}^d |x_i - x'_i|$
3. **é—µå¯å¤«æ–¯åŸºè·ç¦»**ï¼š$d(x, x') = \left(\sum_{i=1}^d |x_i - x'_i|^p\right)^{1/p}$
4. **ä½™å¼¦ç›¸ä¼¼åº¦**ï¼š$\cos(\theta) = \frac{x \cdot x'}{\|x\| \|x'\|}$

**åˆ†ç±»è§„åˆ™**:
$$\hat{y} = \arg\max_{c} \sum_{x_i \in N_k(x)} w_i \cdot \mathbb{I}(y_i = c)$$

å…¶ä¸­ $N_k(x)$ æ˜¯ $x$ çš„ $k$ ä¸ªæœ€è¿‘é‚»ï¼Œ$w_i$ æ˜¯æƒé‡ï¼ˆé€šå¸¸ä¸ºè·ç¦»çš„å€’æ•°ï¼‰ã€‚

**å›å½’è§„åˆ™**:
$$\hat{y} = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i$$

æˆ–åŠ æƒå¹³å‡ï¼š
$$\hat{y} = \frac{\sum_{x_i \in N_k(x)} w_i y_i}{\sum_{x_i \in N_k(x)} w_i}$$

å…¶ä¸­ $w_i = \frac{1}{d(x, x_i) + \epsilon}$ æ˜¯è·ç¦»æƒé‡ã€‚

**å¤æ‚åº¦åˆ†æ**ï¼š

- **è®­ç»ƒ**ï¼š$O(1)$ï¼ˆæ‡’æƒ°å­¦ä¹ ï¼Œåªéœ€å­˜å‚¨æ•°æ®ï¼‰
- **é¢„æµ‹**ï¼š$O(nd)$ï¼ˆçº¿æ€§æœç´¢ï¼‰ï¼Œä½¿ç”¨KDæ ‘å¯ä¼˜åŒ–åˆ°$O(\log n)$

**ä¼˜ç¼ºç‚¹**ï¼š

**ä¼˜ç‚¹**ï¼š

- ç®€å•ç›´è§‚ï¼Œæ— éœ€è®­ç»ƒ
- å¯¹éçº¿æ€§é—®é¢˜æœ‰æ•ˆ
- é€‚åˆå¤šåˆ†ç±»é—®é¢˜

**ç¼ºç‚¹**ï¼š

- é¢„æµ‹æ—¶è®¡ç®—é‡å¤§
- å¯¹é«˜ç»´æ•°æ®æ•ˆæœå·®ï¼ˆç»´åº¦ç¾éš¾ï¼‰
- å¯¹å™ªå£°å’Œæ— å…³ç‰¹å¾æ•æ„Ÿ
- éœ€è¦é€‰æ‹©åˆé€‚çš„$k$å€¼

**å®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from collections import Counter
from sklearn.neighbors import KDTree

class KNearestNeighbors:
    """kè¿‘é‚»å®ç°"""

    def __init__(self, k=3, distance_metric='euclidean', weights='uniform'):
        self.k = k
        self.distance_metric = distance_metric
        self.weights = weights
        self.X_train = None
        self.y_train = None
        self.tree = None

    def euclidean_distance(self, x1, x2):
        """æ¬§æ°è·ç¦»"""
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def manhattan_distance(self, x1, x2):
        """æ›¼å“ˆé¡¿è·ç¦»"""
        return np.sum(np.abs(x1 - x2))

    def distance(self, x1, x2):
        """è®¡ç®—è·ç¦»"""
        if self.distance_metric == 'euclidean':
            return self.euclidean_distance(x1, x2)
        elif self.distance_metric == 'manhattan':
            return self.manhattan_distance(x1, x2)
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")

    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹ï¼ˆå­˜å‚¨æ•°æ®ï¼‰"""
        self.X_train = X
        self.y_train = y
        # ä½¿ç”¨KDæ ‘åŠ é€Ÿæœç´¢
        try:
            self.tree = KDTree(X)
        except:
            self.tree = None
        return self

    def find_k_neighbors(self, x):
        """æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»"""
        if self.tree is not None:
            # ä½¿ç”¨KDæ ‘åŠ é€Ÿ
            distances, indices = self.tree.query([x], k=self.k)
            return distances[0], indices[0]
        else:
            # çº¿æ€§æœç´¢
            distances = []
            for i, x_train in enumerate(self.X_train):
                dist = self.distance(x, x_train)
                distances.append((dist, i))
            distances.sort(key=lambda x: x[0])
            k_nearest = distances[:self.k]
            return [d[0] for d in k_nearest], [d[1] for d in k_nearest]

    def predict_classification(self, X):
        """åˆ†ç±»é¢„æµ‹"""
        predictions = []
        for x in X:
            distances, indices = self.find_k_neighbors(x)
            k_labels = self.y_train[indices]

            if self.weights == 'uniform':
                # å¤šæ•°æŠ•ç¥¨
                counter = Counter(k_labels)
                prediction = counter.most_common(1)[0][0]
            else:
                # åŠ æƒæŠ•ç¥¨
                weights = 1 / (np.array(distances) + 1e-10)
                weighted_votes = {}
                for label, weight in zip(k_labels, weights):
                    weighted_votes[label] = weighted_votes.get(label, 0) + weight
                prediction = max(weighted_votes, key=weighted_votes.get)

            predictions.append(prediction)
        return np.array(predictions)

    def predict_regression(self, X):
        """å›å½’é¢„æµ‹"""
        predictions = []
        for x in X:
            distances, indices = self.find_k_neighbors(x)
            k_values = self.y_train[indices]

            if self.weights == 'uniform':
                prediction = np.mean(k_values)
            else:
                # åŠ æƒå¹³å‡
                weights = 1 / (np.array(distances) + 1e-10)
                prediction = np.average(k_values, weights=weights)

            predictions.append(prediction)
        return np.array(predictions)

    def predict(self, X, task='classification'):
        """é¢„æµ‹"""
        if task == 'classification':
            return self.predict_classification(X)
        else:
            return self.predict_regression(X)

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆä»…åˆ†ç±»ï¼‰"""
        probabilities = []
        for x in X:
            distances, indices = self.find_k_neighbors(x)
            k_labels = self.y_train[indices]

            if self.weights == 'uniform':
                counter = Counter(k_labels)
                total = len(k_labels)
                proba = {label: count / total for label, count in counter.items()}
            else:
                # åŠ æƒæ¦‚ç‡
                weights = 1 / (np.array(distances) + 1e-10)
                weighted_votes = {}
                total_weight = np.sum(weights)
                for label, weight in zip(k_labels, weights):
                    weighted_votes[label] = weighted_votes.get(label, 0) + weight
                proba = {label: weight / total_weight
                        for label, weight in weighted_votes.items()}

            probabilities.append(proba)
        return probabilities

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.datasets import load_iris, make_regression
from sklearn.model_selection import train_test_split

# åˆ†ç±»ç¤ºä¾‹
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

knn_classifier = KNearestNeighbors(k=5, weights='distance')
knn_classifier.fit(X_train, y_train)
predictions = knn_classifier.predict(X_test, task='classification')
print(f"åˆ†ç±»å‡†ç¡®ç‡: {np.mean(predictions == y_test):.4f}")

# å›å½’ç¤ºä¾‹
X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

knn_regressor = KNearestNeighbors(k=5, weights='distance')
knn_regressor.fit(X_train, y_train)
predictions = knn_regressor.predict(X_test, task='regression')
mse = np.mean((predictions - y_test) ** 2)
print(f"å›å½’MSE: {mse:.4f}")
```

## 5. å›å½’ç®—æ³•

### 5.1. çº¿æ€§å›å½’

**æ¨¡å‹**: $y = \theta^T x + \epsilon$ï¼Œå…¶ä¸­ $\epsilon \sim \mathcal{N}(0, \sigma^2)$

**æŸå¤±å‡½æ•°**:
$$J(\theta) = \frac{1}{2n} \sum_{i=1}^n (y_i - \theta^T x_i)^2$$

**è§£æè§£**:
$$\hat{\theta} = (X^T X)^{-1} X^T y$$

### 5.2. æ­£åˆ™åŒ–å›å½’

**Ridgeå›å½’** (L2æ­£åˆ™åŒ–):
$$J(\theta) = \frac{1}{2n} \|y - X\theta\|_2^2 + \lambda \|\theta\|_2^2$$

**Lassoå›å½’** (L1æ­£åˆ™åŒ–):
$$J(\theta) = \frac{1}{2n} \|y - X\theta\|_2^2 + \lambda \|\theta\|_1$$

## 6. é›†æˆå­¦ä¹ æ–¹æ³•

### 6.1. Bagging (Bootstrap Aggregating)

**æ€æƒ³**: é€šè¿‡è‡ªåŠ©é‡‡æ ·è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œé™ä½æ–¹å·®

**éšæœºæ£®æ—**: åœ¨BaggingåŸºç¡€ä¸Šå¢åŠ ç‰¹å¾éšæœºé€‰æ‹©

### 6.2. Boosting

**AdaBoostç®—æ³•**:

1. åˆå§‹åŒ–æ ·æœ¬æƒé‡: $w_i^{(1)} = \frac{1}{n}$
2. å¯¹ $t = 1, \ldots, T$:
   - è®­ç»ƒå¼±å­¦ä¹ å™¨ $h_t$
   - è®¡ç®—é”™è¯¯ç‡: $\epsilon_t = \sum_{i: h_t(x_i) \neq y_i} w_i^{(t)}$
   - è®¡ç®—æƒé‡: $\alpha_t = \frac{1}{2} \ln\frac{1-\epsilon_t}{\epsilon_t}$
   - æ›´æ–°æƒé‡: $w_i^{(t+1)} = \frac{w_i^{(t)} \exp(-\alpha_t y_i h_t(x_i))}{Z_t}$

**æœ€ç»ˆåˆ†ç±»å™¨**:
$$H(x) = \text{sign}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$$

### 6.3. æ¢¯åº¦æå‡

**å‡½æ•°ç©ºé—´ä¸­çš„æ¢¯åº¦ä¸‹é™**:
$$F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)$$

å…¶ä¸­ $h_m$ æ‹Ÿåˆè´Ÿæ¢¯åº¦ã€‚

## 7. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

### 7.1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡

**æ··æ·†çŸ©é˜µ**:
$$
\begin{pmatrix}
\text{TN} & \text{FP} \\
\text{FN} & \text{TP}
\end{pmatrix}
$$

å…¶ä¸­ï¼š

- **TP (True Positive)**: çœŸæ­£ä¾‹ï¼Œé¢„æµ‹ä¸ºæ­£ä¸”å®é™…ä¸ºæ­£
- **TN (True Negative)**: çœŸè´Ÿä¾‹ï¼Œé¢„æµ‹ä¸ºè´Ÿä¸”å®é™…ä¸ºè´Ÿ
- **FP (False Positive)**: å‡æ­£ä¾‹ï¼Œé¢„æµ‹ä¸ºæ­£ä½†å®é™…ä¸ºè´Ÿ
- **FN (False Negative)**: å‡è´Ÿä¾‹ï¼Œé¢„æµ‹ä¸ºè´Ÿä½†å®é™…ä¸ºæ­£

**æ ¸å¿ƒæŒ‡æ ‡**:

- **å‡†ç¡®ç‡**: $\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$
  - æ‰€æœ‰æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

- **ç²¾ç¡®ç‡**: $\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$
  - é¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä¸­å®é™…ä¸ºæ­£çš„æ¯”ä¾‹

- **å¬å›ç‡**: $\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$
  - å®é™…ä¸ºæ­£çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

- **F1åˆ†æ•°**: $F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$
  - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

- **ç‰¹å¼‚æ€§**: $\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}$
  - å®é™…ä¸ºè´Ÿçš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

**ROCæ›²çº¿å’ŒAUC**:

**ROCæ›²çº¿**ï¼ˆReceiver Operating Characteristicï¼‰ï¼š

- æ¨ªè½´ï¼šå‡æ­£ä¾‹ç‡ï¼ˆFPRï¼‰= $\frac{\text{FP}}{\text{FP} + \text{TN}}$
- çºµè½´ï¼šçœŸæ­£ä¾‹ç‡ï¼ˆTPRï¼‰= $\frac{\text{TP}}{\text{TP} + \text{FN}}$ = Recall

**AUC**ï¼ˆArea Under Curveï¼‰ï¼š

- ROCæ›²çº¿ä¸‹çš„é¢ç§¯
- å–å€¼èŒƒå›´ï¼š[0, 1]
- AUC = 1ï¼šå®Œç¾åˆ†ç±»å™¨
- AUC = 0.5ï¼šéšæœºåˆ†ç±»å™¨

**å®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt

class ClassificationMetrics:
    """åˆ†ç±»æ€§èƒ½æŒ‡æ ‡è®¡ç®—"""

    def __init__(self, y_true, y_pred, y_proba=None):
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_proba = y_proba
        self.cm = confusion_matrix(y_true, y_pred)

    def accuracy(self):
        """å‡†ç¡®ç‡"""
        return np.trace(self.cm) / np.sum(self.cm)

    def precision(self, average='macro'):
        """ç²¾ç¡®ç‡"""
        if average == 'macro':
            # å®å¹³å‡
            precisions = []
            for i in range(len(self.cm)):
                tp = self.cm[i, i]
                fp = np.sum(self.cm[:, i]) - tp
                if tp + fp > 0:
                    precisions.append(tp / (tp + fp))
            return np.mean(precisions)
        elif average == 'micro':
            # å¾®å¹³å‡
            tp = np.trace(self.cm)
            fp = np.sum(self.cm) - np.sum(np.diag(self.cm))
            return tp / (tp + fp)

    def recall(self, average='macro'):
        """å¬å›ç‡"""
        if average == 'macro':
            recalls = []
            for i in range(len(self.cm)):
                tp = self.cm[i, i]
                fn = np.sum(self.cm[i, :]) - tp
                if tp + fn > 0:
                    recalls.append(tp / (tp + fn))
            return np.mean(recalls)
        elif average == 'micro':
            tp = np.trace(self.cm)
            fn = np.sum(self.cm) - np.sum(np.diag(self.cm))
            return tp / (tp + fn)

    def f1_score(self, average='macro'):
        """F1åˆ†æ•°"""
        precision = self.precision(average)
        recall = self.recall(average)
        return 2 * (precision * recall) / (precision + recall)

    def roc_auc(self):
        """è®¡ç®—ROC-AUCï¼ˆäºŒåˆ†ç±»ï¼‰"""
        if self.y_proba is None:
            raise ValueError("éœ€è¦æ¦‚ç‡é¢„æµ‹å€¼")

        if len(np.unique(self.y_true)) != 2:
            raise ValueError("ROC-AUCä»…é€‚ç”¨äºäºŒåˆ†ç±»")

        fpr, tpr, thresholds = roc_curve(self.y_true, self.y_proba)
        roc_auc = auc(fpr, tpr)
        return fpr, tpr, roc_auc

    def plot_roc_curve(self):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        fpr, tpr, roc_auc = self.roc_auc()

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'ROC curve (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
                label='Random classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")
        plt.show()

    def print_report(self):
        """æ‰“å°å®Œæ•´æŠ¥å‘Š"""
        print("=== æ··æ·†çŸ©é˜µ ===")
        print(self.cm)
        print(f"\nå‡†ç¡®ç‡: {self.accuracy():.4f}")
        print(f"ç²¾ç¡®ç‡ (å®å¹³å‡): {self.precision('macro'):.4f}")
        print(f"å¬å›ç‡ (å®å¹³å‡): {self.recall('macro'):.4f}")
        print(f"F1åˆ†æ•° (å®å¹³å‡): {self.f1_score('macro'):.4f}")

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,
                          random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

metrics = ClassificationMetrics(y_test, y_pred, y_proba)
metrics.print_report()
metrics.plot_roc_curve()
```

### 7.2. å›å½’æ€§èƒ½æŒ‡æ ‡

**å‡æ–¹è¯¯å·®**: $\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$

**å†³å®šç³»æ•°**: $R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$

### 7.3. äº¤å‰éªŒè¯

äº¤å‰éªŒè¯æ˜¯è¯„ä¼°æ¨¡å‹æ³›åŒ–æ€§èƒ½çš„é‡è¦æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¬¡åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†æ¥è·å¾—æ›´å¯é çš„æ€§èƒ½ä¼°è®¡ã€‚

**kæŠ˜äº¤å‰éªŒè¯**:
$$\text{CV}_k = \frac{1}{k} \sum_{i=1}^k L(f^{(-i)}, D_i)$$

å…¶ä¸­ $f^{(-i)}$ æ˜¯åœ¨é™¤ç¬¬ $i$ æŠ˜å¤–çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œ$D_i$ æ˜¯ç¬¬ $i$ æŠ˜éªŒè¯é›†ã€‚

**ç•™ä¸€äº¤å‰éªŒè¯ï¼ˆLOOCVï¼‰**:
$$\text{LOOCV} = \frac{1}{n} \sum_{i=1}^n L(f^{(-i)}, \{x_i\})$$

æ¯æ¬¡ç•™ä¸€ä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†ã€‚

**åˆ†å±‚kæŠ˜äº¤å‰éªŒè¯**:
ä¿æŒæ¯æŠ˜ä¸­å„ç±»åˆ«æ¯”ä¾‹ä¸åŸå§‹æ•°æ®é›†ç›¸åŒï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

**æ—¶é—´åºåˆ—äº¤å‰éªŒè¯**:
è€ƒè™‘æ—¶é—´é¡ºåºï¼Œä½¿ç”¨å†å²æ•°æ®è®­ç»ƒï¼Œæœªæ¥æ•°æ®éªŒè¯ã€‚

**å®ç°ç¤ºä¾‹**ï¼š

```python
import numpy as np
from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit
from sklearn.metrics import accuracy_score, mean_squared_error

class CrossValidation:
    """äº¤å‰éªŒè¯å®ç°"""

    def __init__(self, cv_type='kfold', n_splits=5, shuffle=True, random_state=42):
        self.cv_type = cv_type
        self.n_splits = n_splits
        self.shuffle = shuffle
        self.random_state = random_state

    def kfold_cv(self, X, y, model, task='classification'):
        """kæŠ˜äº¤å‰éªŒè¯"""
        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle,
                  random_state=self.random_state)
        scores = []

        for train_idx, val_idx in kf.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)

            if task == 'classification':
                score = accuracy_score(y_val, y_pred)
            else:
                score = mean_squared_error(y_val, y_pred)

            scores.append(score)

        return np.array(scores)

    def stratified_kfold_cv(self, X, y, model, task='classification'):
        """åˆ†å±‚kæŠ˜äº¤å‰éªŒè¯"""
        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=self.shuffle,
                             random_state=self.random_state)
        scores = []

        for train_idx, val_idx in skf.split(X, y):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)

            if task == 'classification':
                score = accuracy_score(y_val, y_pred)
            else:
                score = mean_squared_error(y_val, y_pred)

            scores.append(score)

        return np.array(scores)

    def time_series_cv(self, X, y, model, task='classification'):
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        scores = []

        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)

            if task == 'classification':
                score = accuracy_score(y_val, y_pred)
            else:
                score = mean_squared_error(y_val, y_pred)

            scores.append(score)

        return np.array(scores)

    def cross_validate(self, X, y, model, task='classification'):
        """æ‰§è¡Œäº¤å‰éªŒè¯"""
        if self.cv_type == 'kfold':
            scores = self.kfold_cv(X, y, model, task)
        elif self.cv_type == 'stratified':
            scores = self.stratified_kfold_cv(X, y, model, task)
        elif self.cv_type == 'timeseries':
            scores = self.time_series_cv(X, y, model, task)
        else:
            raise ValueError(f"Unknown CV type: {self.cv_type}")

        return {
            'scores': scores,
            'mean': np.mean(scores),
            'std': np.std(scores),
            'min': np.min(scores),
            'max': np.max(scores)
        }

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression

iris = load_iris()
X, y = iris.data, iris.target

model = LogisticRegression(max_iter=1000)
cv = CrossValidation(cv_type='stratified', n_splits=5)

results = cv.cross_validate(X, y, model, task='classification')
print(f"äº¤å‰éªŒè¯ç»“æœ:")
print(f"  å¹³å‡å‡†ç¡®ç‡: {results['mean']:.4f}")
print(f"  æ ‡å‡†å·®: {results['std']:.4f}")
print(f"  æœ€å°å€¼: {results['min']:.4f}")
print(f"  æœ€å¤§å€¼: {results['max']:.4f}")
```

## 8. å·¥ç¨‹å®ç°

### 8.1. Rustå®ç°ç¤ºä¾‹

```rust
use ndarray::{Array1, Array2};

// é€»è¾‘å›å½’å®ç°
pub struct LogisticRegression {
    weights: Array1<f64>,
    learning_rate: f64,
    max_iterations: usize,
}

impl LogisticRegression {
    pub fn new(features: usize, learning_rate: f64, max_iterations: usize) -> Self {
        Self {
            weights: Array1::zeros(features),
            learning_rate,
            max_iterations,
        }
    }

    fn sigmoid(&self, z: f64) -> f64 {
        1.0 / (1.0 + (-z).exp())
    }

    fn sigmoid_array(&self, z: &Array1<f64>) -> Array1<f64> {
        z.mapv(|x| self.sigmoid(x))
    }

    pub fn fit(&mut self, X: &Array2<f64>, y: &Array1<f64>) -> Result<(), String> {
        let n_samples = X.nrows();

        for _iteration in 0..self.max_iterations {
            // å‰å‘ä¼ æ’­
            let z = X.dot(&self.weights);
            let predictions = self.sigmoid_array(&z);

            // è®¡ç®—æ¢¯åº¦
            let errors = &predictions - y;
            let gradient = X.t().dot(&errors) / n_samples as f64;

            // æ›´æ–°æƒé‡
            self.weights = &self.weights - self.learning_rate * &gradient;
        }

        Ok(())
    }

    pub fn predict_proba(&self, X: &Array2<f64>) -> Array1<f64> {
        let z = X.dot(&self.weights);
        self.sigmoid_array(&z)
    }

    pub fn predict(&self, X: &Array2<f64>) -> Array1<i32> {
        self.predict_proba(X).mapv(|p| if p >= 0.5 { 1 } else { 0 })
    }
}

// çº¿æ€§å›å½’å®ç°
pub struct LinearRegression {
    weights: Array1<f64>,
}

impl LinearRegression {
    pub fn new() -> Self {
        Self {
            weights: Array1::zeros(0),
        }
    }

    pub fn fit(&mut self, X: &Array2<f64>, y: &Array1<f64>) -> Result<(), String> {
        // ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ±‚è§£: w = (X^T X)^(-1) X^T y
        let xt = X.t();
        let xtx = xt.dot(X);
        let xty = xt.dot(y);

        // åœ¨å®é™…å®ç°ä¸­åº”ä½¿ç”¨SVDæˆ–QRåˆ†è§£æ¥æ•°å€¼ç¨³å®šåœ°æ±‚è§£
        // è¿™é‡Œç®€åŒ–ä¸ºä¼ªé€†è®¡ç®—
        self.weights = solve_linear_system(&xtx, &xty)?;
        Ok(())
    }

    pub fn predict(&self, X: &Array2<f64>) -> Array1<f64> {
        X.dot(&self.weights)
    }
}

// ç®€åŒ–çš„çº¿æ€§ç³»ç»Ÿæ±‚è§£ï¼ˆå®é™…åº”ä½¿ç”¨ä¸“ä¸šæ•°å€¼åº“ï¼‰
fn solve_linear_system(A: &Array2<f64>, b: &Array1<f64>) -> Result<Array1<f64>, String> {
    // è¿™é‡Œåº”è¯¥ä½¿ç”¨å¦‚nalgebraç­‰æ•°å€¼çº¿æ€§ä»£æ•°åº“
    // ç®€åŒ–å®ç°
    Err("Use proper linear algebra library".to_string())
}
```

### 8.2. æ¨¡å‹è¯„ä¼°å·¥å…·

```rust
use std::collections::HashMap;

pub struct ModelEvaluator;

impl ModelEvaluator {
    pub fn confusion_matrix(y_true: &Array1<i32>, y_pred: &Array1<i32>) -> Array2<i32> {
        let classes: Vec<i32> = {
            let mut unique: Vec<i32> = y_true.iter().chain(y_pred.iter()).cloned().collect();
            unique.sort_unstable();
            unique.dedup();
            unique
        };

        let n_classes = classes.len();
        let mut matrix = Array2::zeros((n_classes, n_classes));

        for (&true_label, &pred_label) in y_true.iter().zip(y_pred.iter()) {
            let true_idx = classes.iter().position(|&x| x == true_label).unwrap();
            let pred_idx = classes.iter().position(|&x| x == pred_label).unwrap();
            matrix[[true_idx, pred_idx]] += 1;
        }

        matrix
    }

    pub fn accuracy(y_true: &Array1<i32>, y_pred: &Array1<i32>) -> f64 {
        let correct = y_true.iter().zip(y_pred.iter())
            .filter(|(&true_val, &pred_val)| true_val == pred_val)
            .count();
        correct as f64 / y_true.len() as f64
    }

    pub fn precision_recall_f1(y_true: &Array1<i32>, y_pred: &Array1<i32>, positive_class: i32) -> (f64, f64, f64) {
        let mut tp = 0;
        let mut fp = 0;
        let mut fn_count = 0;

        for (&true_val, &pred_val) in y_true.iter().zip(y_pred.iter()) {
            match (true_val == positive_class, pred_val == positive_class) {
                (true, true) => tp += 1,
                (false, true) => fp += 1,
                (true, false) => fn_count += 1,
                (false, false) => {} // TN
            }
        }

        let precision = if tp + fp > 0 { tp as f64 / (tp + fp) as f64 } else { 0.0 };
        let recall = if tp + fn_count > 0 { tp as f64 / (tp + fn_count) as f64 } else { 0.0 };
        let f1 = if precision + recall > 0.0 { 2.0 * precision * recall / (precision + recall) } else { 0.0 };

        (precision, recall, f1)
    }

    pub fn mse(y_true: &Array1<f64>, y_pred: &Array1<f64>) -> f64 {
        let diff = y_true - y_pred;
        diff.mapv(|x| x * x).mean().unwrap()
    }

    pub fn r2_score(y_true: &Array1<f64>, y_pred: &Array1<f64>) -> f64 {
        let y_mean = y_true.mean().unwrap();
        let ss_res: f64 = y_true.iter().zip(y_pred.iter())
            .map(|(&true_val, &pred_val)| (true_val - pred_val).powi(2))
            .sum();
        let ss_tot: f64 = y_true.iter()
            .map(|&val| (val - y_mean).powi(2))
            .sum();

        1.0 - ss_res / ss_tot
    }
}
```

### 8.3. äº¤å‰éªŒè¯å®ç°

```rust
use rand::seq::SliceRandom;
use rand::thread_rng;

pub struct CrossValidator;

impl CrossValidator {
    pub fn k_fold_split(n_samples: usize, k: usize) -> Vec<(Vec<usize>, Vec<usize>)> {
        let mut indices: Vec<usize> = (0..n_samples).collect();
        let mut rng = thread_rng();
        indices.shuffle(&mut rng);

        let fold_size = n_samples / k;
        let mut folds = Vec::new();

        for i in 0..k {
            let start = i * fold_size;
            let end = if i == k - 1 { n_samples } else { (i + 1) * fold_size };

            let test_indices = indices[start..end].to_vec();
            let train_indices = indices[..start].iter()
                .chain(indices[end..].iter())
                .cloned()
                .collect();

            folds.push((train_indices, test_indices));
        }

        folds
    }

    pub fn cross_validate<F, M>(
        X: &Array2<f64>,
        y: &Array1<f64>,
        k: usize,
        mut train_fn: F,
    ) -> Vec<f64>
    where
        F: FnMut(&Array2<f64>, &Array1<f64>, &Array2<f64>, &Array1<f64>) -> f64,
    {
        let n_samples = X.nrows();
        let folds = Self::k_fold_split(n_samples, k);
        let mut scores = Vec::new();

        for (train_indices, test_indices) in folds {
            let X_train = X.select(ndarray::Axis(0), &train_indices);
            let y_train = Array1::from_iter(train_indices.iter().map(|&i| y[i]));
            let X_test = X.select(ndarray::Axis(0), &test_indices);
            let y_test = Array1::from_iter(test_indices.iter().map(|&i| y[i]));

            let score = train_fn(&X_train, &y_train, &X_test, &y_test);
            scores.push(score);
        }

        scores
    }
}
```

## 9. ä¸å…¶ä»–ç†è®ºçš„å…³è”

### 9.1. ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»

**ç»Ÿè®¡å­¦ä¹ ç†è®º** â†’ [3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md](./3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md)

- PACå­¦ä¹ ç†è®ºåœ¨åˆ†ç±»ç®—æ³•ä¸­çš„åº”ç”¨
- VCç»´å¯¹æ¨¡å‹å¤æ‚åº¦çš„ç†è®ºæŒ‡å¯¼
- æ­£åˆ™åŒ–ç†è®ºçš„å®é™…å®ç°

### 9.2. ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ

**å…³ç³»æ¨¡å‹åº”ç”¨** â†’ [3.2.1-æ•°æ®æ¨¡å‹çš„å½¢å¼åŒ–ç†è®º.md](../3.2-å½¢å¼åŒ–æ¨¡å‹/3.2.1-æ•°æ®æ¨¡å‹çš„å½¢å¼åŒ–ç†è®º.md)

- SQLä¸æœºå™¨å­¦ä¹ ç®¡é“çš„ç»“åˆ
- ç‰¹å¾å·¥ç¨‹çš„å…³ç³»ä»£æ•°å®ç°

**ç®—æ³•ä¼˜åŒ–** â†’ [3.3.2-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•.md](../3.3-ç®—æ³•å®ç°/3.3.2-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•.md)

- åŸºäºæˆæœ¬çš„æ¨¡å‹é€‰æ‹©
- å¹¶è¡Œè®¡ç®—ä¼˜åŒ–ç­–ç•¥

### 9.3. ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»

**ç±»å‹ç†è®º** â†’ [2.1-ç±»å‹ç†è®º/](../../2-å½¢å¼ç§‘å­¦ç†è®º/2.1-ç±»å‹ç†è®º/)

- ç±»å‹å®‰å…¨çš„æœºå™¨å­¦ä¹ æ¥å£è®¾è®¡
- ä¾èµ–ç±»å‹åœ¨æ¨¡å‹éªŒè¯ä¸­çš„åº”ç”¨

**æ¦‚ç‡è®ºåŸºç¡€** â†’ [æ•°å­¦åŸºç¡€](../../5-æ•°å­¦åŸºç¡€/)

- è´å¶æ–¯æ¨ç†åœ¨åˆ†ç±»ä¸­çš„åº”ç”¨
- æ¦‚ç‡å›¾æ¨¡å‹çš„ç†è®ºåŸºç¡€

### 9.4. ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ

**è®¾è®¡æ¨¡å¼** â†’ [4.2-è®¾è®¡æ¨¡å¼/](../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.2-è®¾è®¡æ¨¡å¼/)

- æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ¶æ„æ¨¡å¼
- æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„å·¥ä½œæµæ¨¡å¼

**æŒç»­é›†æˆ** â†’ [7-æŒç»­é›†æˆä¸æ¼”è¿›/](../../7-æŒç»­é›†æˆä¸æ¼”è¿›/)

- MLæ¨¡å‹çš„ç‰ˆæœ¬æ§åˆ¶å’Œè‡ªåŠ¨åŒ–æµ‹è¯•
- A/Bæµ‹è¯•å’Œå®éªŒç®¡ç†

## 10. æœ¬åœ°è·³è½¬

- [ä¸Šä¸€èŠ‚: 3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º](./3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md)
- [ä¸‹ä¸€èŠ‚: 3.4.3-æ— ç›‘ç£å­¦ä¹ ç®—æ³•](./3.4.3-æ— ç›‘ç£å­¦ä¹ ç®—æ³•.md)
- [è¿”å›ç›®å½•: 3.4-AIä¸æœºå™¨å­¦ä¹ ç®—æ³•](./README.md)

---

## 11. ç‰ˆæœ¬ä¿¡æ¯

- **åˆ›å»ºæ—¥æœŸ**: 2024-12-19
- **ç‰ˆæœ¬**: v1.0
- **è´¡çŒ®è€…**: Data Science Analysis Team
- **æœ€åæ›´æ–°**: 2024-12-19

---

*æœ¬æ–‡æ¡£å°†æŒç»­æ›´æ–°ï¼Œè·Ÿè¿›ç›‘ç£å­¦ä¹ ç®—æ³•çš„æœ€æ–°å‘å±•å’Œå·¥ç¨‹å®è·µã€‚*

## 12. å¤šè¡¨å¾

ç›‘ç£å­¦ä¹ ç®—æ³•æ”¯æŒå¤šç§è¡¨å¾æ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š

- ç¬¦å·è¡¨å¾ï¼ˆç®—æ³•ä¼ªä»£ç ã€æŸå¤±å‡½æ•°ã€æ¨¡å‹å…¬å¼ç­‰ï¼‰
- å›¾ç»“æ„ï¼ˆæ¨¡å‹ç»“æ„å›¾ã€è®­ç»ƒæµç¨‹å›¾ã€æ•°æ®æµå›¾ç­‰ï¼‰
- å‘é‡/å¼ é‡ï¼ˆç‰¹å¾å‘é‡ã€å‚æ•°çŸ©é˜µã€åµŒå…¥ï¼‰
- è‡ªç„¶è¯­è¨€ï¼ˆå®šä¹‰ã€æ³¨é‡Šã€æè¿°ï¼‰
- å›¾åƒ/å¯è§†åŒ–ï¼ˆç»“æ„å›¾ã€æµç¨‹å›¾ã€æ¨¡å‹å¯è§†åŒ–ç­‰ï¼‰
è¿™äº›è¡¨å¾å¯äº’æ˜ ï¼Œæå‡ç›‘ç£å­¦ä¹ ç®—æ³•çš„è¡¨è¾¾åŠ›ã€‚

## 13. å½¢å¼åŒ–è¯­ä¹‰

- è¯­ä¹‰åŸŸï¼š$D$ï¼Œå¦‚è¾“å…¥ç©ºé—´ã€è¾“å‡ºç©ºé—´ã€æ¨¡å‹ç©ºé—´ã€æŸå¤±ç©ºé—´
- è§£é‡Šå‡½æ•°ï¼š$I: S \to D$ï¼Œå°†ç¬¦å·/ç»“æ„æ˜ å°„åˆ°å…·ä½“è¯­ä¹‰å¯¹è±¡
- è¯­ä¹‰ä¸€è‡´æ€§ï¼šæ¯ä¸ªæ¨¡å‹/ç®—æ³•/æŸå¤±å‡½æ•°/å…¬å¼åœ¨$D$ä¸­æœ‰æ˜ç¡®å®šä¹‰

## 14. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜

- è¯­æ³•è§„åˆ™ï¼šå¦‚æ¨¡å‹å®šä¹‰ã€æŸå¤±å‡½æ•°ã€è®­ç»ƒè§„åˆ™ã€æ¨ç†è§„åˆ™ã€çº¦æŸæ¡ä»¶
- **å®šç†**ï¼šç›‘ç£å­¦ä¹ ç®—æ³•çš„è¯­æ³•ç³»ç»Ÿå…·ä¸€è‡´æ€§ä¸å¯æ‰©å±•æ€§ã€‚
- **è¯æ˜**ï¼šç”±æ¨¡å‹å®šä¹‰ã€æŸå¤±å‡½æ•°ä¸æ¨ç†è§„åˆ™é€’å½’å®šä¹‰ï¼Œä¿è¯ç³»ç»Ÿä¸€è‡´ä¸å¯æ‰©å±•ã€‚
