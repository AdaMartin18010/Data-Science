# 3.4.2 ç›‘ç£å­¦ä¹ ç®—æ³•

## ğŸ“‘ ç›®å½•

- [3.4.2 ç›‘ç£å­¦ä¹ ç®—æ³•](#342-ç›‘ç£å­¦ä¹ ç®—æ³•)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ‘˜è¦](#1-æ‘˜è¦)
  - [3. å¼•è¨€](#3-å¼•è¨€)
    - [3.1. ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶](#31-ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶)
    - [3.2. é—®é¢˜ç±»å‹åˆ†ç±»](#32-é—®é¢˜ç±»å‹åˆ†ç±»)
  - [4. åˆ†ç±»ç®—æ³•](#4-åˆ†ç±»ç®—æ³•)
    - [4.1. çº¿æ€§åˆ†ç±»å™¨](#41-çº¿æ€§åˆ†ç±»å™¨)
      - [4.1.1. é€»è¾‘å›å½’ (Logistic Regression)](#411-é€»è¾‘å›å½’-logistic-regression)
      - [4.1.2. æ”¯æŒå‘é‡æœº (SVM)](#412-æ”¯æŒå‘é‡æœº-svm)
    - [4.2. å†³ç­–æ ‘](#42-å†³ç­–æ ‘)
    - [4.3. kè¿‘é‚» (k-NN)](#43-kè¿‘é‚»-k-nn)
  - [5. å›å½’ç®—æ³•](#5-å›å½’ç®—æ³•)
    - [5.1. çº¿æ€§å›å½’](#51-çº¿æ€§å›å½’)
    - [5.2. æ­£åˆ™åŒ–å›å½’](#52-æ­£åˆ™åŒ–å›å½’)
  - [6. é›†æˆå­¦ä¹ æ–¹æ³•](#6-é›†æˆå­¦ä¹ æ–¹æ³•)
    - [6.1. Bagging (Bootstrap Aggregating)](#61-bagging-bootstrap-aggregating)
    - [6.2. Boosting](#62-boosting)
    - [6.3. æ¢¯åº¦æå‡](#63-æ¢¯åº¦æå‡)
  - [7. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©](#7-æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©)
    - [7.1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡](#71-åˆ†ç±»æ€§èƒ½æŒ‡æ ‡)
    - [7.2. å›å½’æ€§èƒ½æŒ‡æ ‡](#72-å›å½’æ€§èƒ½æŒ‡æ ‡)
    - [7.3. äº¤å‰éªŒè¯](#73-äº¤å‰éªŒè¯)
  - [8. å·¥ç¨‹å®ç°](#8-å·¥ç¨‹å®ç°)
    - [8.1. Rustå®ç°ç¤ºä¾‹](#81-rustå®ç°ç¤ºä¾‹)
    - [8.2. æ¨¡å‹è¯„ä¼°å·¥å…·](#82-æ¨¡å‹è¯„ä¼°å·¥å…·)
    - [8.3. äº¤å‰éªŒè¯å®ç°](#83-äº¤å‰éªŒè¯å®ç°)
  - [9. ä¸å…¶ä»–ç†è®ºçš„å…³è”](#9-ä¸å…¶ä»–ç†è®ºçš„å…³è”)
    - [9.1. ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»](#91-ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»)
    - [9.2. ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ](#92-ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ)
    - [9.3. ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»](#93-ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»)
    - [9.4. ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ](#94-ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ)
  - [10. æœ¬åœ°è·³è½¬](#10-æœ¬åœ°è·³è½¬)
  - [11. ç‰ˆæœ¬ä¿¡æ¯](#11-ç‰ˆæœ¬ä¿¡æ¯)
  - [12. å¤šè¡¨å¾](#12-å¤šè¡¨å¾)
  - [13. å½¢å¼åŒ–è¯­ä¹‰](#13-å½¢å¼åŒ–è¯­ä¹‰)
  - [14. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜](#14-å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜)

---


## 1. æ‘˜è¦

æœ¬æ–‡ç³»ç»Ÿé˜è¿°ç›‘ç£å­¦ä¹ ç®—æ³•çš„ç†è®ºåŸºç¡€ã€æ•°å­¦æ¨¡å‹å’Œå·¥ç¨‹å®ç°ã€‚ä»ç»å…¸çš„çº¿æ€§æ¨¡å‹åˆ°ç°ä»£æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä»å•ä¸€ç®—æ³•åˆ°é›†æˆå­¦ä¹ ï¼Œå…¨é¢æ„å»ºç›‘ç£å­¦ä¹ çš„çŸ¥è¯†ä½“ç³»ã€‚ç»“åˆæ•°å­¦å½¢å¼åŒ–å’Œå®é™…åº”ç”¨æ¡ˆä¾‹ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½æœºå™¨å­¦ä¹ ç³»ç»Ÿæä¾›å®Œæ•´çš„ç†è®ºå’Œå®è·µæŒ‡å¯¼ã€‚

- [3.4.2 ç›‘ç£å­¦ä¹ ç®—æ³•](#342-ç›‘ç£å­¦ä¹ ç®—æ³•)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ‘˜è¦](#1-æ‘˜è¦)
  - [3. å¼•è¨€](#3-å¼•è¨€)
    - [3.1. ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶](#31-ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶)
    - [3.2. é—®é¢˜ç±»å‹åˆ†ç±»](#32-é—®é¢˜ç±»å‹åˆ†ç±»)
  - [4. åˆ†ç±»ç®—æ³•](#4-åˆ†ç±»ç®—æ³•)
    - [4.1. çº¿æ€§åˆ†ç±»å™¨](#41-çº¿æ€§åˆ†ç±»å™¨)
      - [4.1.1. é€»è¾‘å›å½’ (Logistic Regression)](#411-é€»è¾‘å›å½’-logistic-regression)
      - [4.1.2. æ”¯æŒå‘é‡æœº (SVM)](#412-æ”¯æŒå‘é‡æœº-svm)
    - [4.2. å†³ç­–æ ‘](#42-å†³ç­–æ ‘)
    - [4.3. kè¿‘é‚» (k-NN)](#43-kè¿‘é‚»-k-nn)
  - [5. å›å½’ç®—æ³•](#5-å›å½’ç®—æ³•)
    - [5.1. çº¿æ€§å›å½’](#51-çº¿æ€§å›å½’)
    - [5.2. æ­£åˆ™åŒ–å›å½’](#52-æ­£åˆ™åŒ–å›å½’)
  - [6. é›†æˆå­¦ä¹ æ–¹æ³•](#6-é›†æˆå­¦ä¹ æ–¹æ³•)
    - [6.1. Bagging (Bootstrap Aggregating)](#61-bagging-bootstrap-aggregating)
    - [6.2. Boosting](#62-boosting)
    - [6.3. æ¢¯åº¦æå‡](#63-æ¢¯åº¦æå‡)
  - [7. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©](#7-æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©)
    - [7.1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡](#71-åˆ†ç±»æ€§èƒ½æŒ‡æ ‡)
    - [7.2. å›å½’æ€§èƒ½æŒ‡æ ‡](#72-å›å½’æ€§èƒ½æŒ‡æ ‡)
    - [7.3. äº¤å‰éªŒè¯](#73-äº¤å‰éªŒè¯)
  - [8. å·¥ç¨‹å®ç°](#8-å·¥ç¨‹å®ç°)
    - [8.1. Rustå®ç°ç¤ºä¾‹](#81-rustå®ç°ç¤ºä¾‹)
    - [8.2. æ¨¡å‹è¯„ä¼°å·¥å…·](#82-æ¨¡å‹è¯„ä¼°å·¥å…·)
    - [8.3. äº¤å‰éªŒè¯å®ç°](#83-äº¤å‰éªŒè¯å®ç°)
  - [9. ä¸å…¶ä»–ç†è®ºçš„å…³è”](#9-ä¸å…¶ä»–ç†è®ºçš„å…³è”)
    - [9.1. ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»](#91-ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»)
    - [9.2. ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ](#92-ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ)
    - [9.3. ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»](#93-ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»)
    - [9.4. ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ](#94-ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ)
  - [10. æœ¬åœ°è·³è½¬](#10-æœ¬åœ°è·³è½¬)
  - [11. ç‰ˆæœ¬ä¿¡æ¯](#11-ç‰ˆæœ¬ä¿¡æ¯)
  - [12. å¤šè¡¨å¾](#12-å¤šè¡¨å¾)
  - [13. å½¢å¼åŒ–è¯­ä¹‰](#13-å½¢å¼åŒ–è¯­ä¹‰)
  - [14. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜](#14-å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜)

## 3. å¼•è¨€

ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒèŒƒå¼ï¼Œé€šè¿‡æ ‡æ³¨æ•°æ®å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚ç»™å®šè®­ç»ƒé›† $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ï¼Œç›®æ ‡æ˜¯å­¦ä¹ å‡½æ•° $f: \mathcal{X} \rightarrow \mathcal{Y}$ï¼Œä½¿å¾—åœ¨æ–°æ ·æœ¬ä¸Šçš„æ³›åŒ–è¯¯å·®æœ€å°ã€‚

### 3.1. ç›‘ç£å­¦ä¹ çš„æ•°å­¦æ¡†æ¶

**æŸå¤±å‡½æ•°**: è¡¡é‡é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
$$L(f, \mathcal{D}) = \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)$$

**ç»éªŒé£é™©æœ€å°åŒ–**:
$$\hat{f} = \arg\min_{f \in \mathcal{F}} L(f, \mathcal{D}) + \lambda R(f)$$

å…¶ä¸­ $R(f)$ æ˜¯æ­£åˆ™åŒ–é¡¹ï¼Œ$\lambda$ æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚

### 3.2. é—®é¢˜ç±»å‹åˆ†ç±»

- **äºŒåˆ†ç±»**: $\mathcal{Y} = \{0, 1\}$ æˆ– $\{-1, +1\}$
- **å¤šåˆ†ç±»**: $\mathcal{Y} = \{1, 2, \ldots, K\}$ï¼Œ$K > 2$
- **å¤šæ ‡ç­¾**: æ¯ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾
- **å›å½’**: $\mathcal{Y} = \mathbb{R}$ æˆ– $\mathcal{Y} = \mathbb{R}^d$

## 4. åˆ†ç±»ç®—æ³•

### 4.1. çº¿æ€§åˆ†ç±»å™¨

#### 4.1.1. é€»è¾‘å›å½’ (Logistic Regression)

**sigmoidå‡½æ•°**:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**æ¦‚ç‡æ¨¡å‹**:
$$P(y=1|x; \theta) = \sigma(\theta^T x)$$

**æŸå¤±å‡½æ•°** (å¯¹æ•°ä¼¼ç„¶):
$$\ell(\theta) = -\sum_{i=1}^n [y_i \log \sigma(\theta^T x_i) + (1-y_i) \log(1-\sigma(\theta^T x_i))]$$

**æ¢¯åº¦**:
$$\nabla_\theta \ell = \sum_{i=1}^n (\sigma(\theta^T x_i) - y_i) x_i$$

#### 4.1.2. æ”¯æŒå‘é‡æœº (SVM)

**çº¿æ€§SVMç›®æ ‡å‡½æ•°**:
$$\min_{\theta, b, \xi} \frac{1}{2}\|\theta\|^2 + C \sum_{i=1}^n \xi_i$$

**çº¦æŸæ¡ä»¶**:
$$y_i(\theta^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

**æ ¸æŠ€å·§**: é€šè¿‡æ ¸å‡½æ•° $K(x, x') = \phi(x)^T \phi(x')$ å®ç°éçº¿æ€§åˆ†ç±»

- **RBFæ ¸**: $K(x, x') = \exp(-\gamma \|x - x'\|^2)$
- **å¤šé¡¹å¼æ ¸**: $K(x, x') = (\gamma x^T x' + r)^d$

### 4.2. å†³ç­–æ ‘

**ä¿¡æ¯å¢ç›Š**:
$$\text{IG}(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)$$

å…¶ä¸­ç†µ $H(S) = -\sum_{c \in C} p_c \log_2 p_c$

**åŸºå°¼ä¸çº¯åº¦**:
$$\text{Gini}(S) = 1 - \sum_{c \in C} p_c^2$$

### 4.3. kè¿‘é‚» (k-NN)

**åˆ†ç±»è§„åˆ™**:
$$\hat{y} = \arg\max_{c} \sum_{x_i \in N_k(x)} \mathbb{I}(y_i = c)$$

å…¶ä¸­ $N_k(x)$ æ˜¯ $x$ çš„ $k$ ä¸ªæœ€è¿‘é‚»ã€‚

## 5. å›å½’ç®—æ³•

### 5.1. çº¿æ€§å›å½’

**æ¨¡å‹**: $y = \theta^T x + \epsilon$ï¼Œå…¶ä¸­ $\epsilon \sim \mathcal{N}(0, \sigma^2)$

**æŸå¤±å‡½æ•°**:
$$J(\theta) = \frac{1}{2n} \sum_{i=1}^n (y_i - \theta^T x_i)^2$$

**è§£æè§£**:
$$\hat{\theta} = (X^T X)^{-1} X^T y$$

### 5.2. æ­£åˆ™åŒ–å›å½’

**Ridgeå›å½’** (L2æ­£åˆ™åŒ–):
$$J(\theta) = \frac{1}{2n} \|y - X\theta\|_2^2 + \lambda \|\theta\|_2^2$$

**Lassoå›å½’** (L1æ­£åˆ™åŒ–):
$$J(\theta) = \frac{1}{2n} \|y - X\theta\|_2^2 + \lambda \|\theta\|_1$$

## 6. é›†æˆå­¦ä¹ æ–¹æ³•

### 6.1. Bagging (Bootstrap Aggregating)

**æ€æƒ³**: é€šè¿‡è‡ªåŠ©é‡‡æ ·è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œé™ä½æ–¹å·®

**éšæœºæ£®æ—**: åœ¨BaggingåŸºç¡€ä¸Šå¢åŠ ç‰¹å¾éšæœºé€‰æ‹©

### 6.2. Boosting

**AdaBoostç®—æ³•**:

1. åˆå§‹åŒ–æ ·æœ¬æƒé‡: $w_i^{(1)} = \frac{1}{n}$
2. å¯¹ $t = 1, \ldots, T$:
   - è®­ç»ƒå¼±å­¦ä¹ å™¨ $h_t$
   - è®¡ç®—é”™è¯¯ç‡: $\epsilon_t = \sum_{i: h_t(x_i) \neq y_i} w_i^{(t)}$
   - è®¡ç®—æƒé‡: $\alpha_t = \frac{1}{2} \ln\frac{1-\epsilon_t}{\epsilon_t}$
   - æ›´æ–°æƒé‡: $w_i^{(t+1)} = \frac{w_i^{(t)} \exp(-\alpha_t y_i h_t(x_i))}{Z_t}$

**æœ€ç»ˆåˆ†ç±»å™¨**:
$$H(x) = \text{sign}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$$

### 6.3. æ¢¯åº¦æå‡

**å‡½æ•°ç©ºé—´ä¸­çš„æ¢¯åº¦ä¸‹é™**:
$$F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)$$

å…¶ä¸­ $h_m$ æ‹Ÿåˆè´Ÿæ¢¯åº¦ã€‚

## 7. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

### 7.1. åˆ†ç±»æ€§èƒ½æŒ‡æ ‡

**æ··æ·†çŸ©é˜µ**:
$$
\begin{pmatrix}
\text{TN} & \text{FP} \\
\text{FN} & \text{TP}
\end{pmatrix}
$$

**æ ¸å¿ƒæŒ‡æ ‡**:

- **å‡†ç¡®ç‡**: $\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$
- **ç²¾ç¡®ç‡**: $\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$
- **å¬å›ç‡**: $\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$
- **F1åˆ†æ•°**: $F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

### 7.2. å›å½’æ€§èƒ½æŒ‡æ ‡

**å‡æ–¹è¯¯å·®**: $\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$

**å†³å®šç³»æ•°**: $R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$

### 7.3. äº¤å‰éªŒè¯

**kæŠ˜äº¤å‰éªŒè¯**:
$$\text{CV}_k = \frac{1}{k} \sum_{i=1}^k L(f^{(-i)}, D_i)$$

å…¶ä¸­ $f^{(-i)}$ æ˜¯åœ¨é™¤ç¬¬ $i$ æŠ˜å¤–çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚

## 8. å·¥ç¨‹å®ç°

### 8.1. Rustå®ç°ç¤ºä¾‹

```rust
use ndarray::{Array1, Array2};

// é€»è¾‘å›å½’å®ç°
pub struct LogisticRegression {
    weights: Array1<f64>,
    learning_rate: f64,
    max_iterations: usize,
}

impl LogisticRegression {
    pub fn new(features: usize, learning_rate: f64, max_iterations: usize) -> Self {
        Self {
            weights: Array1::zeros(features),
            learning_rate,
            max_iterations,
        }
    }

    fn sigmoid(&self, z: f64) -> f64 {
        1.0 / (1.0 + (-z).exp())
    }

    fn sigmoid_array(&self, z: &Array1<f64>) -> Array1<f64> {
        z.mapv(|x| self.sigmoid(x))
    }

    pub fn fit(&mut self, X: &Array2<f64>, y: &Array1<f64>) -> Result<(), String> {
        let n_samples = X.nrows();

        for _iteration in 0..self.max_iterations {
            // å‰å‘ä¼ æ’­
            let z = X.dot(&self.weights);
            let predictions = self.sigmoid_array(&z);

            // è®¡ç®—æ¢¯åº¦
            let errors = &predictions - y;
            let gradient = X.t().dot(&errors) / n_samples as f64;

            // æ›´æ–°æƒé‡
            self.weights = &self.weights - self.learning_rate * &gradient;
        }

        Ok(())
    }

    pub fn predict_proba(&self, X: &Array2<f64>) -> Array1<f64> {
        let z = X.dot(&self.weights);
        self.sigmoid_array(&z)
    }

    pub fn predict(&self, X: &Array2<f64>) -> Array1<i32> {
        self.predict_proba(X).mapv(|p| if p >= 0.5 { 1 } else { 0 })
    }
}

// çº¿æ€§å›å½’å®ç°
pub struct LinearRegression {
    weights: Array1<f64>,
}

impl LinearRegression {
    pub fn new() -> Self {
        Self {
            weights: Array1::zeros(0),
        }
    }

    pub fn fit(&mut self, X: &Array2<f64>, y: &Array1<f64>) -> Result<(), String> {
        // ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ±‚è§£: w = (X^T X)^(-1) X^T y
        let xt = X.t();
        let xtx = xt.dot(X);
        let xty = xt.dot(y);

        // åœ¨å®é™…å®ç°ä¸­åº”ä½¿ç”¨SVDæˆ–QRåˆ†è§£æ¥æ•°å€¼ç¨³å®šåœ°æ±‚è§£
        // è¿™é‡Œç®€åŒ–ä¸ºä¼ªé€†è®¡ç®—
        self.weights = solve_linear_system(&xtx, &xty)?;
        Ok(())
    }

    pub fn predict(&self, X: &Array2<f64>) -> Array1<f64> {
        X.dot(&self.weights)
    }
}

// ç®€åŒ–çš„çº¿æ€§ç³»ç»Ÿæ±‚è§£ï¼ˆå®é™…åº”ä½¿ç”¨ä¸“ä¸šæ•°å€¼åº“ï¼‰
fn solve_linear_system(A: &Array2<f64>, b: &Array1<f64>) -> Result<Array1<f64>, String> {
    // è¿™é‡Œåº”è¯¥ä½¿ç”¨å¦‚nalgebraç­‰æ•°å€¼çº¿æ€§ä»£æ•°åº“
    // ç®€åŒ–å®ç°
    Err("Use proper linear algebra library".to_string())
}
```

### 8.2. æ¨¡å‹è¯„ä¼°å·¥å…·

```rust
use std::collections::HashMap;

pub struct ModelEvaluator;

impl ModelEvaluator {
    pub fn confusion_matrix(y_true: &Array1<i32>, y_pred: &Array1<i32>) -> Array2<i32> {
        let classes: Vec<i32> = {
            let mut unique: Vec<i32> = y_true.iter().chain(y_pred.iter()).cloned().collect();
            unique.sort_unstable();
            unique.dedup();
            unique
        };

        let n_classes = classes.len();
        let mut matrix = Array2::zeros((n_classes, n_classes));

        for (&true_label, &pred_label) in y_true.iter().zip(y_pred.iter()) {
            let true_idx = classes.iter().position(|&x| x == true_label).unwrap();
            let pred_idx = classes.iter().position(|&x| x == pred_label).unwrap();
            matrix[[true_idx, pred_idx]] += 1;
        }

        matrix
    }

    pub fn accuracy(y_true: &Array1<i32>, y_pred: &Array1<i32>) -> f64 {
        let correct = y_true.iter().zip(y_pred.iter())
            .filter(|(&true_val, &pred_val)| true_val == pred_val)
            .count();
        correct as f64 / y_true.len() as f64
    }

    pub fn precision_recall_f1(y_true: &Array1<i32>, y_pred: &Array1<i32>, positive_class: i32) -> (f64, f64, f64) {
        let mut tp = 0;
        let mut fp = 0;
        let mut fn_count = 0;

        for (&true_val, &pred_val) in y_true.iter().zip(y_pred.iter()) {
            match (true_val == positive_class, pred_val == positive_class) {
                (true, true) => tp += 1,
                (false, true) => fp += 1,
                (true, false) => fn_count += 1,
                (false, false) => {} // TN
            }
        }

        let precision = if tp + fp > 0 { tp as f64 / (tp + fp) as f64 } else { 0.0 };
        let recall = if tp + fn_count > 0 { tp as f64 / (tp + fn_count) as f64 } else { 0.0 };
        let f1 = if precision + recall > 0.0 { 2.0 * precision * recall / (precision + recall) } else { 0.0 };

        (precision, recall, f1)
    }

    pub fn mse(y_true: &Array1<f64>, y_pred: &Array1<f64>) -> f64 {
        let diff = y_true - y_pred;
        diff.mapv(|x| x * x).mean().unwrap()
    }

    pub fn r2_score(y_true: &Array1<f64>, y_pred: &Array1<f64>) -> f64 {
        let y_mean = y_true.mean().unwrap();
        let ss_res: f64 = y_true.iter().zip(y_pred.iter())
            .map(|(&true_val, &pred_val)| (true_val - pred_val).powi(2))
            .sum();
        let ss_tot: f64 = y_true.iter()
            .map(|&val| (val - y_mean).powi(2))
            .sum();

        1.0 - ss_res / ss_tot
    }
}
```

### 8.3. äº¤å‰éªŒè¯å®ç°

```rust
use rand::seq::SliceRandom;
use rand::thread_rng;

pub struct CrossValidator;

impl CrossValidator {
    pub fn k_fold_split(n_samples: usize, k: usize) -> Vec<(Vec<usize>, Vec<usize>)> {
        let mut indices: Vec<usize> = (0..n_samples).collect();
        let mut rng = thread_rng();
        indices.shuffle(&mut rng);

        let fold_size = n_samples / k;
        let mut folds = Vec::new();

        for i in 0..k {
            let start = i * fold_size;
            let end = if i == k - 1 { n_samples } else { (i + 1) * fold_size };

            let test_indices = indices[start..end].to_vec();
            let train_indices = indices[..start].iter()
                .chain(indices[end..].iter())
                .cloned()
                .collect();

            folds.push((train_indices, test_indices));
        }

        folds
    }

    pub fn cross_validate<F, M>(
        X: &Array2<f64>,
        y: &Array1<f64>,
        k: usize,
        mut train_fn: F,
    ) -> Vec<f64>
    where
        F: FnMut(&Array2<f64>, &Array1<f64>, &Array2<f64>, &Array1<f64>) -> f64,
    {
        let n_samples = X.nrows();
        let folds = Self::k_fold_split(n_samples, k);
        let mut scores = Vec::new();

        for (train_indices, test_indices) in folds {
            let X_train = X.select(ndarray::Axis(0), &train_indices);
            let y_train = Array1::from_iter(train_indices.iter().map(|&i| y[i]));
            let X_test = X.select(ndarray::Axis(0), &test_indices);
            let y_test = Array1::from_iter(test_indices.iter().map(|&i| y[i]));

            let score = train_fn(&X_train, &y_train, &X_test, &y_test);
            scores.push(score);
        }

        scores
    }
}
```

## 9. ä¸å…¶ä»–ç†è®ºçš„å…³è”

### 9.1. ä¸æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºçš„å…³ç³»

**ç»Ÿè®¡å­¦ä¹ ç†è®º** â†’ [3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md](./3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md)

- PACå­¦ä¹ ç†è®ºåœ¨åˆ†ç±»ç®—æ³•ä¸­çš„åº”ç”¨
- VCç»´å¯¹æ¨¡å‹å¤æ‚åº¦çš„ç†è®ºæŒ‡å¯¼
- æ­£åˆ™åŒ–ç†è®ºçš„å®é™…å®ç°

### 9.2. ä¸æ•°æ®æ¨¡å‹çš„é›†æˆ

**å…³ç³»æ¨¡å‹åº”ç”¨** â†’ [3.2.1-æ•°æ®æ¨¡å‹çš„å½¢å¼åŒ–ç†è®º.md](../3.2-å½¢å¼åŒ–æ¨¡å‹/3.2.1-æ•°æ®æ¨¡å‹çš„å½¢å¼åŒ–ç†è®º.md)

- SQLä¸æœºå™¨å­¦ä¹ ç®¡é“çš„ç»“åˆ
- ç‰¹å¾å·¥ç¨‹çš„å…³ç³»ä»£æ•°å®ç°

**ç®—æ³•ä¼˜åŒ–** â†’ [3.3.2-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•.md](../3.3-ç®—æ³•å®ç°/3.3.2-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•.md)

- åŸºäºæˆæœ¬çš„æ¨¡å‹é€‰æ‹©
- å¹¶è¡Œè®¡ç®—ä¼˜åŒ–ç­–ç•¥

### 9.3. ä¸å½¢å¼ç§‘å­¦ç†è®ºçš„è”ç³»

**ç±»å‹ç†è®º** â†’ [2.1-ç±»å‹ç†è®º/](../../2-å½¢å¼ç§‘å­¦ç†è®º/2.1-ç±»å‹ç†è®º/)

- ç±»å‹å®‰å…¨çš„æœºå™¨å­¦ä¹ æ¥å£è®¾è®¡
- ä¾èµ–ç±»å‹åœ¨æ¨¡å‹éªŒè¯ä¸­çš„åº”ç”¨

**æ¦‚ç‡è®ºåŸºç¡€** â†’ [æ•°å­¦åŸºç¡€](../../5-æ•°å­¦åŸºç¡€/)

- è´å¶æ–¯æ¨ç†åœ¨åˆ†ç±»ä¸­çš„åº”ç”¨
- æ¦‚ç‡å›¾æ¨¡å‹çš„ç†è®ºåŸºç¡€

### 9.4. ä¸è½¯ä»¶å·¥ç¨‹çš„ç»“åˆ

**è®¾è®¡æ¨¡å¼** â†’ [4.2-è®¾è®¡æ¨¡å¼/](../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.2-è®¾è®¡æ¨¡å¼/)

- æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ¶æ„æ¨¡å¼
- æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„å·¥ä½œæµæ¨¡å¼

**æŒç»­é›†æˆ** â†’ [7-æŒç»­é›†æˆä¸æ¼”è¿›/](../../7-æŒç»­é›†æˆä¸æ¼”è¿›/)

- MLæ¨¡å‹çš„ç‰ˆæœ¬æ§åˆ¶å’Œè‡ªåŠ¨åŒ–æµ‹è¯•
- A/Bæµ‹è¯•å’Œå®éªŒç®¡ç†

## 10. æœ¬åœ°è·³è½¬

- [ä¸Šä¸€èŠ‚: 3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º](./3.4.1-æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º.md)
- [ä¸‹ä¸€èŠ‚: 3.4.3-æ— ç›‘ç£å­¦ä¹ ç®—æ³•](./3.4.3-æ— ç›‘ç£å­¦ä¹ ç®—æ³•.md)
- [è¿”å›ç›®å½•: 3.4-AIä¸æœºå™¨å­¦ä¹ ç®—æ³•](./README.md)

---

## 11. ç‰ˆæœ¬ä¿¡æ¯

- **åˆ›å»ºæ—¥æœŸ**: 2024-12-19
- **ç‰ˆæœ¬**: v1.0
- **è´¡çŒ®è€…**: Data Science Analysis Team
- **æœ€åæ›´æ–°**: 2024-12-19

---

*æœ¬æ–‡æ¡£å°†æŒç»­æ›´æ–°ï¼Œè·Ÿè¿›ç›‘ç£å­¦ä¹ ç®—æ³•çš„æœ€æ–°å‘å±•å’Œå·¥ç¨‹å®è·µã€‚*

## 12. å¤šè¡¨å¾

ç›‘ç£å­¦ä¹ ç®—æ³•æ”¯æŒå¤šç§è¡¨å¾æ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š

- ç¬¦å·è¡¨å¾ï¼ˆç®—æ³•ä¼ªä»£ç ã€æŸå¤±å‡½æ•°ã€æ¨¡å‹å…¬å¼ç­‰ï¼‰
- å›¾ç»“æ„ï¼ˆæ¨¡å‹ç»“æ„å›¾ã€è®­ç»ƒæµç¨‹å›¾ã€æ•°æ®æµå›¾ç­‰ï¼‰
- å‘é‡/å¼ é‡ï¼ˆç‰¹å¾å‘é‡ã€å‚æ•°çŸ©é˜µã€åµŒå…¥ï¼‰
- è‡ªç„¶è¯­è¨€ï¼ˆå®šä¹‰ã€æ³¨é‡Šã€æè¿°ï¼‰
- å›¾åƒ/å¯è§†åŒ–ï¼ˆç»“æ„å›¾ã€æµç¨‹å›¾ã€æ¨¡å‹å¯è§†åŒ–ç­‰ï¼‰
è¿™äº›è¡¨å¾å¯äº’æ˜ ï¼Œæå‡ç›‘ç£å­¦ä¹ ç®—æ³•çš„è¡¨è¾¾åŠ›ã€‚

## 13. å½¢å¼åŒ–è¯­ä¹‰

- è¯­ä¹‰åŸŸï¼š$D$ï¼Œå¦‚è¾“å…¥ç©ºé—´ã€è¾“å‡ºç©ºé—´ã€æ¨¡å‹ç©ºé—´ã€æŸå¤±ç©ºé—´
- è§£é‡Šå‡½æ•°ï¼š$I: S \to D$ï¼Œå°†ç¬¦å·/ç»“æ„æ˜ å°„åˆ°å…·ä½“è¯­ä¹‰å¯¹è±¡
- è¯­ä¹‰ä¸€è‡´æ€§ï¼šæ¯ä¸ªæ¨¡å‹/ç®—æ³•/æŸå¤±å‡½æ•°/å…¬å¼åœ¨$D$ä¸­æœ‰æ˜ç¡®å®šä¹‰

## 14. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜

- è¯­æ³•è§„åˆ™ï¼šå¦‚æ¨¡å‹å®šä¹‰ã€æŸå¤±å‡½æ•°ã€è®­ç»ƒè§„åˆ™ã€æ¨ç†è§„åˆ™ã€çº¦æŸæ¡ä»¶
- **å®šç†**ï¼šç›‘ç£å­¦ä¹ ç®—æ³•çš„è¯­æ³•ç³»ç»Ÿå…·ä¸€è‡´æ€§ä¸å¯æ‰©å±•æ€§ã€‚
- **è¯æ˜**ï¼šç”±æ¨¡å‹å®šä¹‰ã€æŸå¤±å‡½æ•°ä¸æ¨ç†è§„åˆ™é€’å½’å®šä¹‰ï¼Œä¿è¯ç³»ç»Ÿä¸€è‡´ä¸å¯æ‰©å±•ã€‚
