# 3.3-ç®—æ³•å®ç° åˆ†æ”¯å¯¼èˆª

## ğŸ“‘ ç›®å½•

- [3.3-ç®—æ³•å®ç° åˆ†æ”¯å¯¼èˆª](#33-ç®—æ³•å®ç°-åˆ†æ”¯å¯¼èˆª)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. ç›®å½•ç»“æ„ä¸æœ¬åœ°è·³è½¬](#2-ç›®å½•ç»“æ„ä¸æœ¬åœ°è·³è½¬)
  - [3. æ ¸å¿ƒæ¦‚å¿µ](#3-æ ¸å¿ƒæ¦‚å¿µ)
    - [3.1. æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•](#31-æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•)
    - [3.2. æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•](#32-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•)
    - [3.3. å¹¶å‘æ§åˆ¶ç®—æ³•](#33-å¹¶å‘æ§åˆ¶ç®—æ³•)
  - [4. ç†è®ºåŸºç¡€](#4-ç†è®ºåŸºç¡€)
    - [4.1. ç®—æ³•å¤æ‚åº¦](#41-ç®—æ³•å¤æ‚åº¦)
    - [4.2. ç®—æ³•æ­£ç¡®æ€§](#42-ç®—æ³•æ­£ç¡®æ€§)
    - [4.3. åˆ†å¸ƒå¼ç®—æ³•](#43-åˆ†å¸ƒå¼ç®—æ³•)
  - [5. åº”ç”¨åœºæ™¯](#5-åº”ç”¨åœºæ™¯)
    - [5.1. æ•°æ®åº“ç³»ç»Ÿ](#51-æ•°æ®åº“ç³»ç»Ÿ)
    - [5.2. å¤§æ•°æ®å¤„ç†](#52-å¤§æ•°æ®å¤„ç†)
    - [5.3. åˆ†å¸ƒå¼ç³»ç»Ÿ](#53-åˆ†å¸ƒå¼ç³»ç»Ÿ)
  - [6. è¡Œä¸šæ¡ˆä¾‹ä¸å¤šè¡¨å¾](#6-è¡Œä¸šæ¡ˆä¾‹ä¸å¤šè¡¨å¾)
    - [6.1. å…¸å‹è¡Œä¸šæ¡ˆä¾‹](#61-å…¸å‹è¡Œä¸šæ¡ˆä¾‹)
    - [6.2. å¤šè¡¨å¾ç¤ºä¾‹](#62-å¤šè¡¨å¾ç¤ºä¾‹)
  - [7. ä¸»é¢˜äº¤å‰å¼•ç”¨](#7-ä¸»é¢˜äº¤å‰å¼•ç”¨)
  - [8. å…¨é“¾è·¯çŸ¥è¯†æµ](#8-å…¨é“¾è·¯çŸ¥è¯†æµ)
  - [9. å¤šè¡¨å¾](#9-å¤šè¡¨å¾)
  - [10. å½¢å¼åŒ–è¯­ä¹‰](#10-å½¢å¼åŒ–è¯­ä¹‰)
  - [11. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜](#11-å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜)
  - [12. å·¥å…·ä¸å®ç°](#12-å·¥å…·ä¸å®ç°)
    - [12.1. ç®—æ³•åº“](#121-ç®—æ³•åº“)
    - [12.2. æ€§èƒ½åˆ†æå·¥å…·](#122-æ€§èƒ½åˆ†æå·¥å…·)
    - [12.3. ç®—æ³•å¯è§†åŒ–](#123-ç®—æ³•å¯è§†åŒ–)
  - [13. å­¦ä¹ ä¸ç ”ç©¶è·¯å¾„](#13-å­¦ä¹ ä¸ç ”ç©¶è·¯å¾„)
    - [13.1. åŸºç¡€çŸ¥è¯†](#131-åŸºç¡€çŸ¥è¯†)
    - [13.2. æ ¸å¿ƒæŠ€èƒ½](#132-æ ¸å¿ƒæŠ€èƒ½)
    - [13.3. å®è·µåº”ç”¨](#133-å®è·µåº”ç”¨)
  - [14. æ€»ç»“](#14-æ€»ç»“)

---

## 1. æ¦‚è¿°

ç®—æ³•å®ç°æ˜¯æ•°æ®ç§‘å­¦å’Œè®¡ç®—æœºç§‘å­¦çš„æ ¸å¿ƒï¼Œæ¶µç›–äº†ä»åŸºç¡€æ•°æ®å¤„ç†ç®—æ³•åˆ°é«˜çº§æŸ¥è¯¢ä¼˜åŒ–å’Œå¹¶å‘æ§åˆ¶çš„å®Œæ•´ç®—æ³•ä½“ç³»ã€‚
ç®—æ³•å®ç°ä¸ºæ•°æ®å¤„ç†ã€æ•°æ®åº“ç³»ç»Ÿã€åˆ†å¸ƒå¼ç³»ç»Ÿç­‰æä¾›äº†é«˜æ•ˆçš„ç®—æ³•æ”¯æŒã€‚

**æ ¸å¿ƒç‰¹å¾**ï¼š

1. **é«˜æ•ˆæ€§**ï¼šè¿½æ±‚ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦ä¼˜åŒ–
2. **æ­£ç¡®æ€§**ï¼šä¿è¯ç®—æ³•çš„æ­£ç¡®æ€§å’Œå¯é æ€§
3. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒå¤§è§„æ¨¡æ•°æ®å’Œåˆ†å¸ƒå¼å¤„ç†
4. **å®ç”¨æ€§**ï¼šé¢å‘å®é™…åº”ç”¨å’Œå·¥ç¨‹å®è·µ

**åº”ç”¨é¢†åŸŸ**ï¼š

- æ•°æ®åº“ç³»ç»Ÿ
- å¤§æ•°æ®å¤„ç†
- åˆ†å¸ƒå¼ç³»ç»Ÿ
- å®æ—¶ç³»ç»Ÿ
- æœç´¢å¼•æ“
- æœºå™¨å­¦ä¹ 

---

## 2. ç›®å½•ç»“æ„ä¸æœ¬åœ°è·³è½¬

- [3.3.1-æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•](3.3.1-æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•.md) - æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•æ–‡æ¡£
- [3.3.2-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•](3.3.2-æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•.md) - æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•æ–‡æ¡£
- [3.3.3-å¹¶å‘æ§åˆ¶ç®—æ³•](3.3.3-å¹¶å‘æ§åˆ¶ç®—æ³•.md) - å¹¶å‘æ§åˆ¶ç®—æ³•æ–‡æ¡£
- [3.3.4-ç®—æ³•å®ç°å®è·µæ¡ˆä¾‹](3.3.4-ç®—æ³•å®ç°å®è·µæ¡ˆä¾‹.md) - å®è·µæ¡ˆä¾‹æ–‡æ¡£

---

## 3. æ ¸å¿ƒæ¦‚å¿µ

### 3.1. æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•

æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•åŒ…æ‹¬æ’åºã€æŸ¥æ‰¾ã€èšåˆã€åˆ†ç»„ç­‰åŸºç¡€ç®—æ³•ã€‚

**æ’åºç®—æ³•**ï¼š

- å¿«é€Ÿæ’åºï¼šå¹³å‡$O(n \log n)$ï¼Œæœ€å$O(n^2)$
- å½’å¹¶æ’åºï¼šç¨³å®š$O(n \log n)$
- å †æ’åºï¼š$O(n \log n)$ï¼ŒåŸåœ°æ’åº

**æŸ¥æ‰¾ç®—æ³•**ï¼š

- äºŒåˆ†æŸ¥æ‰¾ï¼š$O(\log n)$ï¼Œè¦æ±‚æœ‰åº
- å“ˆå¸ŒæŸ¥æ‰¾ï¼šå¹³å‡$O(1)$ï¼Œæœ€å$O(n)$

### 3.2. æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•

æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•ç”¨äºä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ã€‚

**ä¼˜åŒ–æŠ€æœ¯**ï¼š

- æŸ¥è¯¢è®¡åˆ’é€‰æ‹©
- ç´¢å¼•ä¼˜åŒ–
- è¿æ¥é¡ºåºä¼˜åŒ–
- ç»Ÿè®¡ä¿¡æ¯ä½¿ç”¨

### 3.3. å¹¶å‘æ§åˆ¶ç®—æ³•

å¹¶å‘æ§åˆ¶ç®—æ³•ç”¨äºä¿è¯å¹¶å‘è®¿é—®çš„æ­£ç¡®æ€§ã€‚

**å¹¶å‘æ§åˆ¶æ–¹æ³•**ï¼š

- é”æœºåˆ¶ï¼šä¸¤é˜¶æ®µé”ã€å¤šç²’åº¦é”
- æ—¶é—´æˆ³ï¼šæ—¶é—´æˆ³æ’åº
- å¤šç‰ˆæœ¬ï¼šMVCC
- ä¹è§‚æ§åˆ¶ï¼šä¹è§‚å¹¶å‘æ§åˆ¶

---

## 4. ç†è®ºåŸºç¡€

### 4.1. ç®—æ³•å¤æ‚åº¦

**æ—¶é—´å¤æ‚åº¦**ï¼š

- æœ€å¥½æƒ…å†µï¼š$T_{best}(n)$
- å¹³å‡æƒ…å†µï¼š$T_{avg}(n)$
- æœ€åæƒ…å†µï¼š$T_{worst}(n)$

**ç©ºé—´å¤æ‚åº¦**ï¼š

- é¢å¤–ç©ºé—´ï¼š$S(n)$
- åŸåœ°ç®—æ³•ï¼š$S(n) = O(1)$

### 4.2. ç®—æ³•æ­£ç¡®æ€§

**ç®—æ³•æ­£ç¡®æ€§è¯æ˜**ï¼š

- å¾ªç¯ä¸å˜é‡
- æ•°å­¦å½’çº³æ³•
- å½¢å¼åŒ–éªŒè¯

### 4.3. åˆ†å¸ƒå¼ç®—æ³•

**åˆ†å¸ƒå¼ç®—æ³•ç‰¹æ€§**ï¼š

- å®¹é”™æ€§
- ä¸€è‡´æ€§
- å¯ç”¨æ€§
- åˆ†åŒºå®¹é”™æ€§

---

## 5. åº”ç”¨åœºæ™¯

### 5.1. æ•°æ®åº“ç³»ç»Ÿ

ç®—æ³•åœ¨æ•°æ®åº“ç³»ç»Ÿä¸­çš„åº”ç”¨åŒ…æ‹¬æŸ¥è¯¢å¤„ç†ã€ç´¢å¼•ç®¡ç†ã€äº‹åŠ¡å¤„ç†ç­‰ã€‚

### 5.2. å¤§æ•°æ®å¤„ç†

ç®—æ³•åœ¨å¤§æ•°æ®å¤„ç†ä¸­çš„åº”ç”¨åŒ…æ‹¬MapReduceã€Sparkã€Flinkç­‰æ¡†æ¶ã€‚

### 5.3. åˆ†å¸ƒå¼ç³»ç»Ÿ

ç®—æ³•åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„åº”ç”¨åŒ…æ‹¬ä¸€è‡´æ€§ç®—æ³•ã€å…±è¯†ç®—æ³•ã€åˆ†å¸ƒå¼é”ç­‰ã€‚

---

## 6. è¡Œä¸šæ¡ˆä¾‹ä¸å¤šè¡¨å¾

### 6.1. å…¸å‹è¡Œä¸šæ¡ˆä¾‹

- **åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º**ï¼šç®—æ³•åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼ˆè¯¦è§[2.5-åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º](../../../2-å½¢å¼ç§‘å­¦ç†è®º/2.5-åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º/README.md)ï¼‰
- **æ•°æ®åº“ç³»ç»Ÿ**ï¼šç®—æ³•åœ¨æ•°æ®åº“ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼ˆè¯¦è§[1-æ•°æ®åº“ç³»ç»Ÿ](../../../1-æ•°æ®åº“ç³»ç»Ÿ/README.md)ï¼‰
- **å¾®æœåŠ¡æ¶æ„**ï¼šç®—æ³•åœ¨å¾®æœåŠ¡ä¸­çš„åº”ç”¨ï¼ˆè¯¦è§[4.3-å¾®æœåŠ¡æ¶æ„](../../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.3-å¾®æœåŠ¡æ¶æ„/README.md)ï¼‰

### 6.2. å¤šè¡¨å¾ç¤ºä¾‹

- **ç¬¦å·è¡¨å¾**ï¼šç®—æ³•ä¼ªä»£ç ã€æ•°æ®ç»“æ„ã€å…¬å¼ã€å¤æ‚åº¦åˆ†æ
- **å›¾ç»“æ„**ï¼šç®—æ³•æµç¨‹å›¾ã€æ•°æ®æµå›¾ã€ä¾èµ–å›¾ã€æ‰§è¡Œè®¡åˆ’
- **å‘é‡/å¼ é‡**ï¼šç‰¹å¾å‘é‡ã€å‚æ•°çŸ©é˜µã€åµŒå…¥
- **è‡ªç„¶è¯­è¨€**ï¼šå®šä¹‰ã€æ³¨é‡Šã€æè¿°ã€ç®—æ³•è¯´æ˜
- **å›¾åƒ/å¯è§†åŒ–**ï¼šæµç¨‹å›¾ã€ç»“æ„å›¾ã€ç®—æ³•å¯è§†åŒ–ã€æ€§èƒ½å›¾è¡¨

---

## 7. ä¸»é¢˜äº¤å‰å¼•ç”¨

| ä¸»é¢˜      | åŸºç¡€ç†è®º | å½¢å¼åŒ–æ¨¡å‹ | åº”ç”¨åœºæ™¯ | ç®—æ³•å®ç° | è¡Œä¸šæ¡ˆä¾‹ | å¤šè¡¨å¾ |
|-----------|----------|------------|----------|----------|----------|--------|
| æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•| âœ… | âœ…       | âœ…     | âœ…     | âœ…     | âœ…   |
| æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•| âœ… | âœ…       | âœ…     | âœ…     | âœ…     | âœ…   |
| å¹¶å‘æ§åˆ¶ç®—æ³•| âœ… | âœ…       | âœ…     | âœ…     | âœ…     | âœ…   |

**äº¤å‰å¼•ç”¨**ï¼š

- [2.5-åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º](../../../2-å½¢å¼ç§‘å­¦ç†è®º/2.5-åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º/README.md)ï¼šåˆ†å¸ƒå¼ç®—æ³•
- [1-æ•°æ®åº“ç³»ç»Ÿ](../../../1-æ•°æ®åº“ç³»ç»Ÿ/README.md)ï¼šæ•°æ®åº“ç®—æ³•
- [4.3-å¾®æœåŠ¡æ¶æ„](../../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.3-å¾®æœåŠ¡æ¶æ„/README.md)ï¼šå¾®æœåŠ¡ç®—æ³•

---

## 8. å…¨é“¾è·¯çŸ¥è¯†æµ

```mermaid
flowchart TD
  A[ç®—æ³•å®ç°] --> B[åŸºç¡€ç†è®ºæ¡†æ¶]
  B --> C1[æ ¸å¿ƒæ•°æ®å¤„ç†ç®—æ³•]
  B --> C2[æŸ¥è¯¢ä¼˜åŒ–ç®—æ³•]
  B --> C3[å¹¶å‘æ§åˆ¶ç®—æ³•]
  C1 --> D[æ’åºç®—æ³•]
  C1 --> E[æœç´¢ç®—æ³•]
  C1 --> F[å›¾ç®—æ³•]
  C2 --> G[æŸ¥è¯¢è®¡åˆ’]
  C2 --> H[ç´¢å¼•ä¼˜åŒ–]
  C2 --> I[ç»Ÿè®¡ä¿¡æ¯]
  C3 --> J[é”æœºåˆ¶]
  C3 --> K[äº‹åŠ¡ç®¡ç†]
  C3 --> L[æ­»é”æ£€æµ‹]
  D & E & F & G & H & I & J & K & L --> M[æ€§èƒ½ä¼˜åŒ–]
  M --> N[åˆ†å¸ƒå¼ç³»ç»Ÿ]
  N --> O[å¾®æœåŠ¡æ¶æ„]
  O --> P[è¡Œä¸šåº”ç”¨]
  P --> Q[å¤šè¡¨å¾ä½“ç³»]
```

---

## 9. å¤šè¡¨å¾

ç®—æ³•å®ç°åˆ†æ”¯æ”¯æŒå¤šç§è¡¨å¾æ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š

- **ç¬¦å·è¡¨å¾**ï¼šç®—æ³•ä¼ªä»£ç ã€æ•°æ®ç»“æ„ã€å…¬å¼ã€å¤æ‚åº¦åˆ†æã€ç®—æ³•è§„èŒƒ
- **å›¾ç»“æ„**ï¼šç®—æ³•æµç¨‹å›¾ã€æ•°æ®æµå›¾ã€ä¾èµ–å›¾ã€æ‰§è¡Œè®¡åˆ’ã€çŠ¶æ€è½¬æ¢å›¾
- **å‘é‡/å¼ é‡**ï¼šç‰¹å¾å‘é‡ã€å‚æ•°çŸ©é˜µã€åµŒå…¥ã€ç®—æ³•å‚æ•°
- **è‡ªç„¶è¯­è¨€**ï¼šå®šä¹‰ã€æ³¨é‡Šã€æè¿°ã€ç®—æ³•è¯´æ˜ã€æ€§èƒ½åˆ†æ
- **å›¾åƒ/å¯è§†åŒ–**ï¼šæµç¨‹å›¾ã€ç»“æ„å›¾ã€ç®—æ³•å¯è§†åŒ–ã€æ€§èƒ½å›¾è¡¨ã€æ‰§è¡Œè½¨è¿¹

è¿™äº›è¡¨å¾å¯äº’æ˜ ï¼Œæå‡ç®—æ³•å®ç°çš„è¡¨è¾¾åŠ›ã€‚

---

## 10. å½¢å¼åŒ–è¯­ä¹‰

**è¯­ä¹‰åŸŸ**ï¼š$D$ï¼ŒåŒ…æ‹¬ï¼š

- æ•°æ®å¯¹è±¡é›†ï¼š$\mathcal{D} = \{d_1, d_2, \ldots\}$
- ç®—æ³•æ“ä½œç©ºé—´ï¼š$\mathcal{A}$
- çŠ¶æ€ç©ºé—´ï¼š$\mathcal{S}$
- å¤æ‚åº¦ç©ºé—´ï¼š$\mathcal{C}$

**è§£é‡Šå‡½æ•°**ï¼š$I: \mathcal{L} \to D$ï¼Œå°†ç®—æ³•è§„èŒƒæ˜ å°„åˆ°è¯­ä¹‰å¯¹è±¡ï¼š

- $I(\text{algorithm}) \in \mathcal{A}$ï¼šç®—æ³•çš„è§£é‡Š
- $I(\text{data}) \in \mathcal{D}$ï¼šæ•°æ®çš„è§£é‡Š
- $I(\text{state}) \in \mathcal{S}$ï¼šçŠ¶æ€çš„è§£é‡Š
- $I(\text{complexity}) \in \mathcal{C}$ï¼šå¤æ‚åº¦çš„è§£é‡Š

**è¯­ä¹‰ä¸€è‡´æ€§**ï¼šæ¯ä¸ªç®—æ³•/æ•°æ®ç»“æ„/æ“ä½œåœ¨$D$ä¸­æœ‰æ˜ç¡®å®šä¹‰ï¼Œæ»¡è¶³ç®—æ³•æ­£ç¡®æ€§ã€å¤æ‚åº¦ä¿è¯ã€æ€§èƒ½è¦æ±‚ã€‚

---

## 11. å½¢å¼åŒ–è¯­æ³•ä¸è¯æ˜

**è¯­æ³•è§„åˆ™**ï¼š

- **ç®—æ³•è¯­æ³•**ï¼š$Algorithm ::= Sort \mid Search \mid Aggregate \mid Optimize$
- **æ•°æ®ç»“æ„è¯­æ³•**ï¼š$DS ::= Array \mid List \mid Tree \mid HashTable$
- **æ“ä½œè¯­æ³•**ï¼š$Operation ::= Insert \mid Delete \mid Update \mid Query$

**æ¨ç†è§„åˆ™**ï¼š

- **ç®—æ³•æ­£ç¡®æ€§**ï¼š$\frac{Precondition \land Algorithm}{Postcondition}$
- **å¤æ‚åº¦åˆ†æ**ï¼š$\frac{Algorithm}{Time(n) = O(f(n))}$

**å®šç†**ï¼šç®—æ³•å®ç°åˆ†æ”¯çš„è¯­æ³•ç³»ç»Ÿå…·ä¸€è‡´æ€§ä¸å¯æ‰©å±•æ€§ã€‚

**è¯æ˜**ï¼šç”±ç®—æ³•å®šä¹‰ã€æ•°æ®ç»“æ„ä¸æ¨ç†è§„åˆ™é€’å½’å®šä¹‰ï¼Œä¿è¯ç³»ç»Ÿä¸€è‡´ä¸å¯æ‰©å±•ã€‚

---

## 12. å·¥å…·ä¸å®ç°

### 12.1. ç®—æ³•åº“

**æ ‡å‡†åº“**ï¼š

- C++ STLï¼šæ ‡å‡†æ¨¡æ¿åº“
- Java Collectionsï¼šé›†åˆæ¡†æ¶
- Pythonæ ‡å‡†åº“ï¼šå†…ç½®ç®—æ³•

**ç¬¬ä¸‰æ–¹åº“**ï¼š

- Boost C++ï¼šC++æ‰©å±•åº“
- Apache Commonsï¼šJavaå·¥å…·åº“
- NumPy/SciPyï¼šPythonç§‘å­¦è®¡ç®—

### 12.2. æ€§èƒ½åˆ†æå·¥å…·

**æ€§èƒ½åˆ†æ**ï¼š

- Valgrindï¼šå†…å­˜å’Œæ€§èƒ½åˆ†æ
- gprofï¼šæ€§èƒ½åˆ†æå·¥å…·
- perfï¼šLinuxæ€§èƒ½åˆ†æ

### 12.3. ç®—æ³•å¯è§†åŒ–

**å¯è§†åŒ–å·¥å…·**ï¼š

- VisuAlgoï¼šç®—æ³•å¯è§†åŒ–ç½‘ç«™
- Algorithm Visualizerï¼šç®—æ³•å¯è§†åŒ–å·¥å…·
- Python matplotlibï¼šç®—æ³•å¯è§†åŒ–

---

## 13. å­¦ä¹ ä¸ç ”ç©¶è·¯å¾„

### 13.1. åŸºç¡€çŸ¥è¯†

1. **æ•°æ®ç»“æ„**ï¼šæ•°ç»„ã€é“¾è¡¨ã€æ ‘ã€å›¾ã€å“ˆå¸Œè¡¨
2. **ç®—æ³•åŸºç¡€**ï¼šæ’åºã€æŸ¥æ‰¾ã€å›¾ç®—æ³•ã€åŠ¨æ€è§„åˆ’

### 13.2. æ ¸å¿ƒæŠ€èƒ½

1. **ç®—æ³•è®¾è®¡**ï¼šåˆ†æ²»æ³•ã€è´ªå¿ƒæ³•ã€åŠ¨æ€è§„åˆ’
2. **å¤æ‚åº¦åˆ†æ**ï¼šæ—¶é—´å¤æ‚åº¦ã€ç©ºé—´å¤æ‚åº¦
3. **ç®—æ³•ä¼˜åŒ–**ï¼šæ€§èƒ½ä¼˜åŒ–ã€ç©ºé—´ä¼˜åŒ–

### 13.3. å®è·µåº”ç”¨

1. **ç³»ç»Ÿå®ç°**ï¼šæ•°æ®åº“ç³»ç»Ÿã€åˆ†å¸ƒå¼ç³»ç»Ÿ
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šæŸ¥è¯¢ä¼˜åŒ–ã€å¹¶å‘ä¼˜åŒ–
3. **ç ”ç©¶å‰æ²¿**ï¼šå¹¶è¡Œç®—æ³•ã€é‡å­ç®—æ³•ã€è¿‘ä¼¼ç®—æ³•

---

## 14. æ€»ç»“

ç®—æ³•å®ç°æ˜¯æ•°æ®ç§‘å­¦å’Œè®¡ç®—æœºç§‘å­¦çš„æ ¸å¿ƒï¼Œä¸ºæ•°æ®å¤„ç†ã€æ•°æ®åº“ç³»ç»Ÿã€åˆ†å¸ƒå¼ç³»ç»Ÿç­‰æä¾›äº†é«˜æ•ˆçš„ç®—æ³•æ”¯æŒã€‚ä»åŸºç¡€æ’åºæŸ¥æ‰¾åˆ°é«˜çº§æŸ¥è¯¢ä¼˜åŒ–ï¼Œä»å•æœºç®—æ³•åˆ°åˆ†å¸ƒå¼ç®—æ³•ï¼Œç®—æ³•å®ç°æ¶µç›–äº†ç®—æ³•çš„å„ä¸ªæ–¹é¢ã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š

1. **é«˜æ•ˆæ€§**ï¼šè¿½æ±‚ç®—æ³•çš„é«˜æ•ˆæ€§
2. **æ­£ç¡®æ€§**ï¼šä¿è¯ç®—æ³•çš„æ­£ç¡®æ€§
3. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒå¤§è§„æ¨¡å’Œåˆ†å¸ƒå¼
4. **å®ç”¨æ€§**ï¼šé¢å‘å®é™…åº”ç”¨

**åº”ç”¨å‰æ™¯**ï¼š
éšç€å¤§æ•°æ®ã€åˆ†å¸ƒå¼ç³»ç»Ÿç­‰æŠ€æœ¯çš„å‘å±•ï¼Œç®—æ³•å®ç°å°†ç»§ç»­å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¶è¡Œç®—æ³•ã€åˆ†å¸ƒå¼ç®—æ³•ã€è¿‘ä¼¼ç®—æ³•ç­‰é¢†åŸŸï¼Œç®—æ³•å®ç°å°†æä¾›æ›´å¼ºå¤§çš„åŠŸèƒ½å’Œæ›´å¥½çš„æ€§èƒ½ã€‚

---

## æ·±å…¥å­¦ä¹ å»ºè®®

### ç†è®ºåŸºç¡€å¼ºåŒ–

**æ•°å­¦åŸºç¡€**ï¼š

- **ç¦»æ•£æ•°å­¦**ï¼šé›†åˆè®ºã€å›¾è®ºã€ç»„åˆæ•°å­¦
- **çº¿æ€§ä»£æ•°**ï¼šçŸ©é˜µè¿ç®—ã€å‘é‡ç©ºé—´
- **æ¦‚ç‡è®º**ï¼šæ¦‚ç‡åˆ†å¸ƒã€éšæœºè¿‡ç¨‹
- **ç®—æ³•åˆ†æ**ï¼šæ—¶é—´å¤æ‚åº¦ã€ç©ºé—´å¤æ‚åº¦

**è®¡ç®—æœºç§‘å­¦åŸºç¡€**ï¼š

- **æ•°æ®ç»“æ„**ï¼šæ ‘ã€å›¾ã€å“ˆå¸Œè¡¨ã€å †
- **ç®—æ³•è®¾è®¡**ï¼šè´ªå¿ƒã€åŠ¨æ€è§„åˆ’ã€åˆ†æ²»
- **è®¡ç®—ç†è®º**ï¼šå¯è®¡ç®—æ€§ã€å¤æ‚åº¦ç†è®º
- **å¹¶è¡Œè®¡ç®—**ï¼šå¹¶è¡Œç®—æ³•ã€åˆ†å¸ƒå¼ç®—æ³•

### å®è·µèƒ½åŠ›æå‡

**ç¼–ç¨‹èƒ½åŠ›**ï¼š

- **å¤šè¯­è¨€ç¼–ç¨‹**ï¼šPythonã€Javaã€C++ã€Rust
- **ç®—æ³•å®ç°**ï¼šç»å…¸ç®—æ³•å®ç°
- **æ€§èƒ½ä¼˜åŒ–**ï¼šç®—æ³•æ€§èƒ½ä¼˜åŒ–
- **æµ‹è¯•éªŒè¯**ï¼šç®—æ³•æµ‹è¯•å’ŒéªŒè¯

**å·¥å…·ä½¿ç”¨**ï¼š

- **ç®—æ³•åº“**ï¼šæ ‡å‡†ç®—æ³•åº“ä½¿ç”¨
- **æ€§èƒ½åˆ†æ**ï¼šæ€§èƒ½åˆ†æå·¥å…·
- **å¯è§†åŒ–å·¥å…·**ï¼šç®—æ³•å¯è§†åŒ–
- **æµ‹è¯•æ¡†æ¶**ï¼šå•å…ƒæµ‹è¯•æ¡†æ¶

### ç ”ç©¶èƒ½åŠ›åŸ¹å…»

**æ–‡çŒ®é˜…è¯»**ï¼š

- **ç»å…¸è®ºæ–‡**ï¼šç®—æ³•ç»å…¸è®ºæ–‡
- **å‰æ²¿ç ”ç©¶**ï¼šæœ€æ–°ç ”ç©¶è¿›å±•
- **åº”ç”¨è®ºæ–‡**ï¼šå®é™…åº”ç”¨æ¡ˆä¾‹
- **ä¼šè®®è®ºæ–‡**ï¼šé¡¶çº§ä¼šè®®è®ºæ–‡

---

## å­¦ä¹ èµ„æºæ±‡æ€»

### åœ¨çº¿è¯¾ç¨‹

- **Coursera**ï¼šç®—æ³•è¯¾ç¨‹
- **edX**ï¼šæ•°æ®ç»“æ„ä¸ç®—æ³•è¯¾ç¨‹
- **Udemy**ï¼šç®—æ³•å®ç°è¯¾ç¨‹
- **YouTube**ï¼šç®—æ³•æ•™ç¨‹

### ä¹¦ç±æ¨è

- **å…¥é—¨ä¹¦ç±**ï¼šã€Šç®—æ³•å¯¼è®ºã€‹
- **è¿›é˜¶ä¹¦ç±**ï¼šã€Šç®—æ³•è®¾è®¡æ‰‹å†Œã€‹
- **åº”ç”¨ä¹¦ç±**ï¼šã€Šç®—æ³•å®æˆ˜ã€‹
- **å·¥å…·ä¹¦ç±**ï¼šã€Šç®—æ³•å¯è§†åŒ–ã€‹

### å­¦æœ¯èµ„æº

- **æœŸåˆŠ**ï¼šACM Transactionsã€IEEE Transactions
- **ä¼šè®®**ï¼šSODAã€FOCSã€STOC
- **æ•°æ®åº“**ï¼šIEEE Xploreã€ACM Digital Library
- **é¢„å°æœ¬**ï¼šarXivã€ResearchGate

---

## å®è·µé¡¹ç›®å»ºè®®

### åŸºç¡€é¡¹ç›®

- **æ’åºç®—æ³•**ï¼šå®ç°å„ç§æ’åºç®—æ³•
- **æœç´¢ç®—æ³•**ï¼šå®ç°æœç´¢ç®—æ³•
- **å›¾ç®—æ³•**ï¼šå®ç°å›¾ç®—æ³•
- **åŠ¨æ€è§„åˆ’**ï¼šåŠ¨æ€è§„åˆ’é—®é¢˜æ±‚è§£

### è¿›é˜¶é¡¹ç›®

- **å¹¶è¡Œç®—æ³•**ï¼šå¹¶è¡Œç®—æ³•å®ç°
- **åˆ†å¸ƒå¼ç®—æ³•**ï¼šåˆ†å¸ƒå¼ç®—æ³•å®ç°
- **è¿‘ä¼¼ç®—æ³•**ï¼šè¿‘ä¼¼ç®—æ³•è®¾è®¡
- **åœ¨çº¿ç®—æ³•**ï¼šåœ¨çº¿ç®—æ³•å®ç°

### é«˜çº§é¡¹ç›®

- **ç®—æ³•ä¼˜åŒ–**ï¼šç®—æ³•æ€§èƒ½ä¼˜åŒ–
- **ç®—æ³•åº“å¼€å‘**ï¼šå¼€å‘ç®—æ³•åº“
- **ç®—æ³•å¯è§†åŒ–**ï¼šç®—æ³•å¯è§†åŒ–å·¥å…·
- **ç®—æ³•åº”ç”¨**ï¼šå®é™…åº”ç”¨ç³»ç»Ÿ

---

## èŒä¸šå‘å±•è·¯å¾„

### å­¦æœ¯ç ”ç©¶

- **ç ”ç©¶æ–¹å‘**ï¼šç®—æ³•è®¾è®¡ã€ç®—æ³•åˆ†æã€ç®—æ³•ä¼˜åŒ–
- **èŒä¸šè·¯å¾„**ï¼šåšå£«ç ”ç©¶ã€åšå£«åã€æ•™èŒã€ç ”ç©¶æœºæ„

### å·¥ä¸šåº”ç”¨

- **åº”ç”¨é¢†åŸŸ**ï¼šè½¯ä»¶å¼€å‘ã€ç³»ç»Ÿä¼˜åŒ–ã€ç®—æ³•å·¥ç¨‹
- **èŒä¸šè·¯å¾„**ï¼šç®—æ³•å·¥ç¨‹å¸ˆã€è½¯ä»¶å·¥ç¨‹å¸ˆã€ç³»ç»Ÿæ¶æ„å¸ˆ

---

## æŠ€æœ¯å‘å±•è¶‹åŠ¿

### ç®—æ³•ä¼˜åŒ–æŠ€æœ¯

**å¹¶è¡Œç®—æ³•**ï¼š

- **å¤šæ ¸å¹¶è¡Œ**ï¼šå¤šæ ¸å¤„ç†å™¨å¹¶è¡Œç®—æ³•
- **GPUå¹¶è¡Œ**ï¼šGPUåŠ é€Ÿç®—æ³•
- **åˆ†å¸ƒå¼å¹¶è¡Œ**ï¼šåˆ†å¸ƒå¼å¹¶è¡Œç®—æ³•
- **å¼‚æ­¥å¹¶è¡Œ**ï¼šå¼‚æ­¥å¹¶è¡Œç®—æ³•

**å¹¶è¡Œæ’åºç®—æ³•ç¤ºä¾‹**ï¼š

```python
from multiprocessing import Pool, cpu_count
import numpy as np
from typing import List

def parallel_merge_sort(data: List[int], num_processes: int = None) -> List[int]:
    """
    å¹¶è¡Œå½’å¹¶æ’åºå®ç°
    
    Args:
        data: å¾…æ’åºæ•°æ®
        num_processes: è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°
    
    Returns:
        æ’åºåçš„æ•°æ®
    """
    if num_processes is None:
        num_processes = cpu_count()
    
    if len(data) <= 1:
        return data
    
    # å¦‚æœæ•°æ®é‡å°æˆ–åªæœ‰ä¸€ä¸ªè¿›ç¨‹ï¼Œä½¿ç”¨ä¸²è¡Œæ’åº
    if len(data) < 1000 or num_processes == 1:
        return merge_sort_sequential(data)
    
    # åˆ†å‰²æ•°æ®
    chunk_size = len(data) // num_processes
    chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]
    
    # å¹¶è¡Œæ’åºå„ä¸ªå—
    with Pool(processes=num_processes) as pool:
        sorted_chunks = pool.map(merge_sort_sequential, chunks)
    
    # åˆå¹¶æ’åºåçš„å—
    result = sorted_chunks[0]
    for chunk in sorted_chunks[1:]:
        result = merge(result, chunk)
    
    return result

def merge_sort_sequential(data: List[int]) -> List[int]:
    """ä¸²è¡Œå½’å¹¶æ’åº"""
    if len(data) <= 1:
        return data
    
    mid = len(data) // 2
    left = merge_sort_sequential(data[:mid])
    right = merge_sort_sequential(data[mid:])
    return merge(left, right)

def merge(left: List[int], right: List[int]) -> List[int]:
    """åˆå¹¶ä¸¤ä¸ªæœ‰åºæ•°ç»„"""
    result = []
    i = j = 0
    
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    
    result.extend(left[i:])
    result.extend(right[j:])
    return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    data = np.random.randint(0, 1000000, size=1000000).tolist()
    
    # å¹¶è¡Œæ’åº
    import time
    start = time.time()
    sorted_data = parallel_merge_sort(data)
    parallel_time = time.time() - start
    
    # ä¸²è¡Œæ’åºå¯¹æ¯”
    start = time.time()
    sorted_data_seq = merge_sort_sequential(data.copy())
    sequential_time = time.time() - start
    
    print(f"å¹¶è¡Œæ’åºæ—¶é—´: {parallel_time:.4f}ç§’")
    print(f"ä¸²è¡Œæ’åºæ—¶é—´: {sequential_time:.4f}ç§’")
    print(f"åŠ é€Ÿæ¯”: {sequential_time / parallel_time:.2f}x")
```

**GPUåŠ é€Ÿç®—æ³•ç¤ºä¾‹**ï¼ˆä½¿ç”¨CuPyï¼‰ï¼š

```python
import cupy as cp
import numpy as np
import time

def gpu_matrix_multiplication(A, B):
    """
    GPUåŠ é€ŸçŸ©é˜µä¹˜æ³•
    
    Args:
        A: çŸ©é˜µA (numpyæ•°ç»„)
        B: çŸ©é˜µB (numpyæ•°ç»„)
    
    Returns:
        çŸ©é˜µä¹˜ç§¯ç»“æœ
    """
    # å°†æ•°æ®è½¬ç§»åˆ°GPU
    A_gpu = cp.asarray(A)
    B_gpu = cp.asarray(B)
    
    # GPUçŸ©é˜µä¹˜æ³•
    C_gpu = cp.dot(A_gpu, B_gpu)
    
    # å°†ç»“æœè½¬å›CPU
    C = cp.asnumpy(C_gpu)
    
    return C

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºå¤§çŸ©é˜µ
    size = 5000
    A = np.random.rand(size, size).astype(np.float32)
    B = np.random.rand(size, size).astype(np.float32)
    
    # CPUè®¡ç®—
    start = time.time()
    C_cpu = np.dot(A, B)
    cpu_time = time.time() - start
    
    # GPUè®¡ç®—
    start = time.time()
    C_gpu = gpu_matrix_multiplication(A, B)
    gpu_time = time.time() - start
    
    print(f"CPUæ—¶é—´: {cpu_time:.4f}ç§’")
    print(f"GPUæ—¶é—´: {gpu_time:.4f}ç§’")
    print(f"åŠ é€Ÿæ¯”: {cpu_time / gpu_time:.2f}x")
    print(f"ç»“æœå·®å¼‚: {np.max(np.abs(C_cpu - C_gpu)):.6f}")
```

**è¿‘ä¼¼ç®—æ³•**ï¼š

- **è¿‘ä¼¼æ¯”ä¼˜åŒ–**ï¼šæ”¹è¿›è¿‘ä¼¼æ¯”
- **åœ¨çº¿ç®—æ³•**ï¼šåœ¨çº¿ç®—æ³•è®¾è®¡
- **éšæœºç®—æ³•**ï¼šéšæœºç®—æ³•ä¼˜åŒ–
- **å¯å‘å¼ç®—æ³•**ï¼šå¯å‘å¼ç®—æ³•æ”¹è¿›

**è¿‘ä¼¼ç®—æ³•ç¤ºä¾‹**ï¼ˆé›†åˆè¦†ç›–é—®é¢˜ï¼‰ï¼š

```python
from typing import List, Set, Tuple
import random

def greedy_set_cover(universe: Set, subsets: List[Set]) -> List[Set]:
    """
    è´ªå¿ƒç®—æ³•æ±‚è§£é›†åˆè¦†ç›–é—®é¢˜ï¼ˆè¿‘ä¼¼ç®—æ³•ï¼‰
    
    è¿‘ä¼¼æ¯”ï¼šH(n)ï¼Œå…¶ä¸­næ˜¯æœ€å¤§å­é›†å¤§å°ï¼ŒH(n)æ˜¯ç¬¬nä¸ªè°ƒå’Œæ•°
    
    Args:
        universe: å…¨é›†
        subsets: å­é›†åˆ—è¡¨
    
    Returns:
        è¦†ç›–å…¨é›†çš„å­é›†åˆ—è¡¨
    """
    uncovered = set(universe)
    cover = []
    
    while uncovered:
        # é€‰æ‹©è¦†ç›–æœ€å¤šæœªè¦†ç›–å…ƒç´ çš„å­é›†
        best_subset = None
        best_covered = set()
        
        for subset in subsets:
            covered = subset & uncovered
            if len(covered) > len(best_covered):
                best_covered = covered
                best_subset = subset
        
        if best_subset:
            cover.append(best_subset)
            uncovered -= best_covered
        else:
            break  # æ— æ³•å®Œå…¨è¦†ç›–
    
    return cover

def randomized_set_cover(universe: Set, subsets: List[Set], 
                        num_iterations: int = 100) -> List[Set]:
    """
    éšæœºç®—æ³•æ±‚è§£é›†åˆè¦†ç›–é—®é¢˜
    
    Args:
        universe: å…¨é›†
        subsets: å­é›†åˆ—è¡¨
        num_iterations: è¿­ä»£æ¬¡æ•°
    
    Returns:
        è¦†ç›–å…¨é›†çš„å­é›†åˆ—è¡¨ï¼ˆå¯èƒ½ä¸æ˜¯æœ€ä¼˜è§£ï¼‰
    """
    best_cover = None
    best_size = float('inf')
    
    for _ in range(num_iterations):
        # éšæœºé€‰æ‹©å­é›†
        random.shuffle(subsets)
        cover = []
        covered = set()
        
        for subset in subsets:
            if not (subset <= covered):
                cover.append(subset)
                covered |= subset
                if covered >= universe:
                    break
        
        if covered >= universe and len(cover) < best_size:
            best_cover = cover
            best_size = len(cover)
    
    return best_cover

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    universe = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
    subsets = [
        {1, 2, 3, 4},
        {3, 4, 5, 6},
        {5, 6, 7, 8},
        {7, 8, 9, 10},
        {1, 5, 9},
        {2, 6, 10}
    ]
    
    greedy_cover = greedy_set_cover(universe, subsets)
    print(f"è´ªå¿ƒç®—æ³•è¦†ç›–: {len(greedy_cover)} ä¸ªå­é›†")
    print(f"å­é›†: {greedy_cover}")
    
    random_cover = randomized_set_cover(universe, subsets)
    if random_cover:
        print(f"\néšæœºç®—æ³•è¦†ç›–: {len(random_cover)} ä¸ªå­é›†")
        print(f"å­é›†: {random_cover}")
```

### ç®—æ³•åº”ç”¨æ‹“å±•

**AIç®—æ³•**ï¼š

- **æœºå™¨å­¦ä¹ ç®—æ³•**ï¼šMLç®—æ³•ä¼˜åŒ–
- **æ·±åº¦å­¦ä¹ ç®—æ³•**ï¼šæ·±åº¦å­¦ä¹ ç®—æ³•
- **å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•
- **ä¼˜åŒ–ç®—æ³•**ï¼šä¼˜åŒ–ç®—æ³•åº”ç”¨

**æœºå™¨å­¦ä¹ ç®—æ³•ä¼˜åŒ–ç¤ºä¾‹**ï¼š

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.datasets import make_classification
import numpy as np

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
X, y = make_classification(n_samples=10000, n_features=20, 
                          n_informative=15, n_redundant=5,
                          random_state=42)

# åŸºç¡€æ¨¡å‹
base_model = RandomForestClassifier(n_estimators=100, random_state=42)
base_scores = cross_val_score(base_model, X, y, cv=5, scoring='accuracy')
print(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_scores.mean():.4f} (+/- {base_scores.std() * 2:.4f})")

# è¶…å‚æ•°ä¼˜åŒ–
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X, y)
print(f"\næœ€ä¼˜å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä¼˜å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä¼˜æ¨¡å‹
best_model = grid_search.best_estimator_
best_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')
print(f"ä¼˜åŒ–åå‡†ç¡®ç‡: {best_scores.mean():.4f} (+/- {best_scores.std() * 2:.4f})")
```

**å¤§æ•°æ®ç®—æ³•**ï¼š

- **æµå¼ç®—æ³•**ï¼šæµå¼æ•°æ®å¤„ç†ç®—æ³•
- **åˆ†å¸ƒå¼ç®—æ³•**ï¼šå¤§æ•°æ®åˆ†å¸ƒå¼ç®—æ³•
- **è¿‘ä¼¼ç®—æ³•**ï¼šå¤§æ•°æ®è¿‘ä¼¼ç®—æ³•
- **é‡‡æ ·ç®—æ³•**ï¼šå¤§æ•°æ®é‡‡æ ·ç®—æ³•

**æµå¼ç®—æ³•ç¤ºä¾‹**ï¼ˆæµå¼K-meansï¼‰ï¼š

```python
import numpy as np
from collections import defaultdict
from typing import List, Tuple

class StreamingKMeans:
    """
    æµå¼K-meansèšç±»ç®—æ³•
    
    é€‚ç”¨äºå¤§æ•°æ®æµå¼å¤„ç†åœºæ™¯ï¼Œä¸éœ€è¦å­˜å‚¨æ‰€æœ‰æ•°æ®
    """
    
    def __init__(self, k: int, decay_factor: float = 0.9):
        """
        åˆå§‹åŒ–æµå¼K-means
        
        Args:
            k: èšç±»æ•°é‡
            decay_factor: è¡°å‡å› å­ï¼Œç”¨äºæ›´æ–°èšç±»ä¸­å¿ƒ
        """
        self.k = k
        self.decay_factor = decay_factor
        self.centroids = None
        self.counts = np.zeros(k)  # æ¯ä¸ªèšç±»çš„æ ·æœ¬è®¡æ•°
    
    def fit_partial(self, X: np.ndarray):
        """
        å¢é‡è®­ç»ƒï¼ˆå¤„ç†æ–°æ•°æ®ï¼‰
        
        Args:
            X: æ–°æ•°æ®ç‚¹ï¼Œå½¢çŠ¶ä¸º (n_samples, n_features)
        """
        if self.centroids is None:
            # åˆå§‹åŒ–èšç±»ä¸­å¿ƒï¼ˆä½¿ç”¨å‰kä¸ªç‚¹ï¼‰
            self.centroids = X[:self.k].copy()
            self.counts[:len(X[:self.k])] = 1
        
        for point in X:
            # æ‰¾åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
            distances = np.linalg.norm(self.centroids - point, axis=1)
            closest_cluster = np.argmin(distances)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒï¼ˆä½¿ç”¨è¡°å‡å› å­ï¼‰
            self.counts[closest_cluster] += 1
            learning_rate = 1.0 / self.counts[closest_cluster]
            
            self.centroids[closest_cluster] = (
                (1 - learning_rate) * self.centroids[closest_cluster] +
                learning_rate * point
            )
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹æ•°æ®ç‚¹æ‰€å±çš„èšç±»"""
        if self.centroids is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        labels = []
        for point in X:
            distances = np.linalg.norm(self.centroids - point, axis=1)
            labels.append(np.argmin(distances))
        
        return np.array(labels)
    
    def get_centroids(self) -> np.ndarray:
        """è·å–å½“å‰èšç±»ä¸­å¿ƒ"""
        return self.centroids.copy()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    true_centroids = np.array([[0, 0], [5, 5], [0, 5]])
    data = []
    for center in true_centroids:
        cluster_data = np.random.randn(1000, 2) + center
        data.append(cluster_data)
    all_data = np.vstack(data)
    
    # æµå¼å¤„ç†
    stream_kmeans = StreamingKMeans(k=3)
    
    # åˆ†æ‰¹å¤„ç†æ•°æ®ï¼ˆæ¨¡æ‹Ÿæµå¼æ•°æ®ï¼‰
    batch_size = 100
    for i in range(0, len(all_data), batch_size):
        batch = all_data[i:i + batch_size]
        stream_kmeans.fit_partial(batch)
    
    # é¢„æµ‹
    labels = stream_kmeans.predict(all_data)
    centroids = stream_kmeans.get_centroids()
    
    print(f"å­¦ä¹ åˆ°çš„èšç±»ä¸­å¿ƒ:\n{centroids}")
    print(f"\nçœŸå®èšç±»ä¸­å¿ƒ:\n{true_centroids}")
```

**åˆ†å¸ƒå¼ç®—æ³•ç¤ºä¾‹**ï¼ˆä½¿ç”¨Sparkï¼‰ï¼š

```python
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
import numpy as np

def distributed_kmeans_example():
    """åˆ†å¸ƒå¼K-meansèšç±»ç¤ºä¾‹"""
    
    # åˆ›å»ºSparkä¼šè¯
    spark = SparkSession.builder \
        .appName("DistributedKMeans") \
        .master("local[*]") \
        .getOrCreate()
    
    # ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®
    np.random.seed(42)
    n_samples = 1000000
    n_features = 10
    data = np.random.randn(n_samples, n_features)
    
    # è½¬æ¢ä¸ºSpark DataFrame
    from pyspark.sql.types import StructType, StructField, DoubleType
    from pyspark.sql import Row
    
    schema = StructType([
        StructField(f"feature_{i}", DoubleType(), True) 
        for i in range(n_features)
    ])
    
    rows = [Row(*row) for row in data]
    df = spark.createDataFrame(rows, schema)
    
    # ç‰¹å¾å‘é‡åŒ–
    feature_cols = [f"feature_{i}" for i in range(n_features)]
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features"
    )
    df_vectorized = assembler.transform(df).select("features")
    
    # åˆ†å¸ƒå¼K-meansèšç±»
    kmeans = KMeans(k=5, seed=42, maxIter=20)
    model = kmeans.fit(df_vectorized)
    
    # é¢„æµ‹
    predictions = model.transform(df_vectorized)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"èšç±»ä¸­å¿ƒæ•°é‡: {len(model.clusterCenters())}")
    print(f"æ€»æˆæœ¬: {model.summary.trainingCost}")
    
    # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„æ ·æœ¬æ•°
    predictions.groupBy("prediction").count().show()
    
    spark.stop()

# è¿è¡Œç¤ºä¾‹
# distributed_kmeans_example()
```

---

## åº”ç”¨å‰æ™¯

### ç³»ç»Ÿä¼˜åŒ–

- **æ€§èƒ½ä¼˜åŒ–**ï¼šç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–
- **èµ„æºä¼˜åŒ–**ï¼šèµ„æºä½¿ç”¨ä¼˜åŒ–
- **èƒ½è€—ä¼˜åŒ–**ï¼šèƒ½è€—ä¼˜åŒ–ç®—æ³•
- **æˆæœ¬ä¼˜åŒ–**ï¼šæˆæœ¬ä¼˜åŒ–ç®—æ³•

### æ™ºèƒ½ç³»ç»Ÿ

- **æ¨èç³»ç»Ÿ**ï¼šæ¨èç®—æ³•ä¼˜åŒ–
- **æœç´¢å¼•æ“**ï¼šæœç´¢ç®—æ³•ä¼˜åŒ–
- **å¹¿å‘Šç³»ç»Ÿ**ï¼šå¹¿å‘Šç®—æ³•ä¼˜åŒ–
- **æ™ºèƒ½è°ƒåº¦**ï¼šæ™ºèƒ½è°ƒåº¦ç®—æ³•

---

[è¿”å›æ•°æ®æ¨¡å‹ä¸ç®—æ³•æ€»å¯¼èˆª](../README.md)
