# 3.1-åŸºç¡€ç†è®º åˆ†æ”¯å¯¼èˆª

## ðŸ“‘ ç›®å½•

- [3.1-åŸºç¡€ç†è®º åˆ†æ”¯å¯¼èˆª](#31-åŸºç¡€ç†è®º-åˆ†æ”¯å¯¼èˆª)
  - [ðŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. ç›®å½•ç»“æž„ä¸Žæœ¬åœ°è·³è½¬](#2-ç›®å½•ç»“æž„ä¸Žæœ¬åœ°è·³è½¬)
  - [3. æ ¸å¿ƒæ¦‚å¿µ](#3-æ ¸å¿ƒæ¦‚å¿µ)
    - [3.1. æ•°æ®ç§‘å­¦å®šä¹‰](#31-æ•°æ®ç§‘å­¦å®šä¹‰)
    - [3.2. æ•°æ®ç§‘å­¦æ–¹æ³•è®º](#32-æ•°æ®ç§‘å­¦æ–¹æ³•è®º)
    - [3.3. æ•°æ®ç±»åž‹](#33-æ•°æ®ç±»åž‹)
  - [4. ç†è®ºåŸºç¡€](#4-ç†è®ºåŸºç¡€)
    - [4.1. ç»Ÿè®¡å­¦ä¹ ç†è®º](#41-ç»Ÿè®¡å­¦ä¹ ç†è®º)
    - [4.2. æ•°æ®ç”Ÿå‘½å‘¨æœŸ](#42-æ•°æ®ç”Ÿå‘½å‘¨æœŸ)
    - [4.3. æ•°æ®æ²»ç†](#43-æ•°æ®æ²»ç†)
  - [5. åº”ç”¨åœºæ™¯](#5-åº”ç”¨åœºæ™¯)
    - [5.1. æ•°æ®åˆ†æž](#51-æ•°æ®åˆ†æž)
    - [5.2. æœºå™¨å­¦ä¹ ](#52-æœºå™¨å­¦ä¹ )
    - [5.3. å•†ä¸šæ™ºèƒ½](#53-å•†ä¸šæ™ºèƒ½)
  - [6. è¡Œä¸šæ¡ˆä¾‹ä¸Žå¤šè¡¨å¾](#6-è¡Œä¸šæ¡ˆä¾‹ä¸Žå¤šè¡¨å¾)
    - [6.1. å…¸åž‹è¡Œä¸šæ¡ˆä¾‹](#61-å…¸åž‹è¡Œä¸šæ¡ˆä¾‹)
    - [6.2. å¤šè¡¨å¾ç¤ºä¾‹](#62-å¤šè¡¨å¾ç¤ºä¾‹)
  - [7. ä¸»é¢˜äº¤å‰å¼•ç”¨](#7-ä¸»é¢˜äº¤å‰å¼•ç”¨)
  - [8. å…¨é“¾è·¯çŸ¥è¯†æµ](#8-å…¨é“¾è·¯çŸ¥è¯†æµ)
  - [9. å¤šè¡¨å¾](#9-å¤šè¡¨å¾)
  - [10. å½¢å¼åŒ–è¯­ä¹‰](#10-å½¢å¼åŒ–è¯­ä¹‰)
  - [11. å½¢å¼åŒ–è¯­æ³•ä¸Žè¯æ˜Ž](#11-å½¢å¼åŒ–è¯­æ³•ä¸Žè¯æ˜Ž)
  - [12. å·¥å…·ä¸Žå®žçŽ°](#12-å·¥å…·ä¸Žå®žçŽ°)
    - [12.1. æ•°æ®å¤„ç†å·¥å…·](#121-æ•°æ®å¤„ç†å·¥å…·)
    - [12.2. æœºå™¨å­¦ä¹ å·¥å…·](#122-æœºå™¨å­¦ä¹ å·¥å…·)
    - [12.3. å¯è§†åŒ–å·¥å…·](#123-å¯è§†åŒ–å·¥å…·)
  - [13. å­¦ä¹ ä¸Žç ”ç©¶è·¯å¾„](#13-å­¦ä¹ ä¸Žç ”ç©¶è·¯å¾„)
    - [13.1. åŸºç¡€çŸ¥è¯†](#131-åŸºç¡€çŸ¥è¯†)
    - [13.2. æ ¸å¿ƒæŠ€èƒ½](#132-æ ¸å¿ƒæŠ€èƒ½)
    - [13.3. å®žè·µåº”ç”¨](#133-å®žè·µåº”ç”¨)
  - [14. æ€»ç»“](#14-æ€»ç»“)
  - [15. æ•°æ®ç§‘å­¦æ–¹æ³•è®ºè¯¦è§£](#15-æ•°æ®ç§‘å­¦æ–¹æ³•è®ºè¯¦è§£)
    - [15.1. CRISP-DMæ¨¡åž‹è¯¦è§£](#151-crisp-dmæ¨¡åž‹è¯¦è§£)
    - [15.2. æ•°æ®ç§‘å­¦å·¥ä½œæµ](#152-æ•°æ®ç§‘å­¦å·¥ä½œæµ)
    - [15.3. æ•°æ®ç§‘å­¦æœ€ä½³å®žè·µ](#153-æ•°æ®ç§‘å­¦æœ€ä½³å®žè·µ)
  - [16. æ•°æ®ç§‘å­¦å·¥å…·ç”Ÿæ€](#16-æ•°æ®ç§‘å­¦å·¥å…·ç”Ÿæ€)
    - [16.1. æ•°æ®å¤„ç†å·¥å…·](#161-æ•°æ®å¤„ç†å·¥å…·)
    - [16.2. æœºå™¨å­¦ä¹ æ¡†æž¶](#162-æœºå™¨å­¦ä¹ æ¡†æž¶)
    - [16.3. å¤§æ•°æ®å·¥å…·](#163-å¤§æ•°æ®å·¥å…·)
  - [17. æ•°æ®ç§‘å­¦ä¼¦ç†ä¸Žåˆè§„](#17-æ•°æ®ç§‘å­¦ä¼¦ç†ä¸Žåˆè§„)
    - [17.1. æ•°æ®éšç§](#171-æ•°æ®éšç§)
    - [17.2. ç®—æ³•å…¬å¹³æ€§](#172-ç®—æ³•å…¬å¹³æ€§)
    - [17.3. å¯è§£é‡Šæ€§](#173-å¯è§£é‡Šæ€§)
  - [æ·±å…¥å­¦ä¹ å»ºè®®](#æ·±å…¥å­¦ä¹ å»ºè®®)
    - [ç†è®ºåŸºç¡€å¼ºåŒ–](#ç†è®ºåŸºç¡€å¼ºåŒ–)
    - [å®žè·µèƒ½åŠ›æå‡](#å®žè·µèƒ½åŠ›æå‡)
  - [å­¦ä¹ èµ„æºæ±‡æ€»](#å­¦ä¹ èµ„æºæ±‡æ€»)
    - [åœ¨çº¿è¯¾ç¨‹](#åœ¨çº¿è¯¾ç¨‹)
    - [ä¹¦ç±æŽ¨è](#ä¹¦ç±æŽ¨è)
    - [å­¦æœ¯èµ„æº](#å­¦æœ¯èµ„æº)
  - [å®žè·µé¡¹ç›®å»ºè®®](#å®žè·µé¡¹ç›®å»ºè®®)
    - [åŸºç¡€é¡¹ç›®](#åŸºç¡€é¡¹ç›®)
    - [è¿›é˜¶é¡¹ç›®](#è¿›é˜¶é¡¹ç›®)
    - [é«˜çº§é¡¹ç›®](#é«˜çº§é¡¹ç›®)
  - [èŒä¸šå‘å±•è·¯å¾„](#èŒä¸šå‘å±•è·¯å¾„)
    - [å­¦æœ¯ç ”ç©¶](#å­¦æœ¯ç ”ç©¶)
    - [å·¥ä¸šåº”ç”¨](#å·¥ä¸šåº”ç”¨)

---

## 1. æ¦‚è¿°

æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ˜¯æ•°æ®ç§‘å­¦å­¦ç§‘çš„ç†è®ºåŸºç¡€ï¼Œæ¶µç›–äº†æ•°æ®å¤„ç†ã€åˆ†æžå’Œåº”ç”¨çš„æ ¸å¿ƒæ¦‚å¿µã€‚
å®ƒç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸçŸ¥è¯†ï¼Œä¸ºæ•°æ®ç§‘å­¦å®žè·µæä¾›ç†è®ºæŒ‡å¯¼å’Œæ–¹æ³•è®ºã€‚

**æ ¸å¿ƒç‰¹å¾**ï¼š

1. **è·¨å­¦ç§‘æ€§**ï¼šç»“åˆç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸçŸ¥è¯†
2. **æ–¹æ³•è®º**ï¼šæä¾›ç³»ç»ŸåŒ–çš„æ•°æ®ç§‘å­¦æ–¹æ³•è®º
3. **ç†è®ºåŸºç¡€**ï¼šä¸ºæ•°æ®ç§‘å­¦å®žè·µæä¾›ç†è®ºåŸºç¡€
4. **åº”ç”¨å¯¼å‘**ï¼šé¢å‘å®žé™…åº”ç”¨å’Œé—®é¢˜è§£å†³

**åº”ç”¨é¢†åŸŸ**ï¼š

- æ•°æ®åˆ†æžå’ŒæŒ–æŽ˜
- æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½
- å•†ä¸šæ™ºèƒ½å’Œå†³ç­–æ”¯æŒ
- ç§‘å­¦ç ”ç©¶å’Œå·¥ç¨‹åº”ç”¨

---

## 2. ç›®å½•ç»“æž„ä¸Žæœ¬åœ°è·³è½¬

- [3.1.1-æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ¡†æž¶](3.1.1-æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ¡†æž¶.md) - ç†è®ºåŸºç¡€æ–‡æ¡£

---

## 3. æ ¸å¿ƒæ¦‚å¿µ

### 3.1. æ•°æ®ç§‘å­¦å®šä¹‰

æ•°æ®ç§‘å­¦æ˜¯è·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸçŸ¥è¯†ï¼Œç”¨äºŽä»Žæ•°æ®ä¸­æå–çŸ¥è¯†å’Œæ´žå¯Ÿã€‚

**æ ¸å¿ƒå…¬å¼**ï¼š
$$\text{æ•°æ®ç§‘å­¦} = \text{ç»Ÿè®¡å­¦} + \text{è®¡ç®—æœºç§‘å­¦} + \text{é¢†åŸŸçŸ¥è¯†}$$

### 3.2. æ•°æ®ç§‘å­¦æ–¹æ³•è®º

**CRISP-DMæ¨¡åž‹**ï¼š

1. ä¸šåŠ¡ç†è§£
2. æ•°æ®ç†è§£
3. æ•°æ®å‡†å¤‡
4. å»ºæ¨¡
5. è¯„ä¼°
6. éƒ¨ç½²

### 3.3. æ•°æ®ç±»åž‹

**æ•°æ®åˆ†ç±»**ï¼š

- **ç»“æž„åŒ–æ•°æ®**ï¼šè¡¨æ ¼ã€æ•°æ®åº“
- **åŠç»“æž„åŒ–æ•°æ®**ï¼šJSONã€XML
- **éžç»“æž„åŒ–æ•°æ®**ï¼šæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘

**æ•°æ®è´¨é‡ç»´åº¦**ï¼š

- å®Œæ•´æ€§ã€ä¸€è‡´æ€§ã€å‡†ç¡®æ€§ã€åŠæ—¶æ€§

---

## 4. ç†è®ºåŸºç¡€

### 4.1. ç»Ÿè®¡å­¦ä¹ ç†è®º

ç»Ÿè®¡å­¦ä¹ ç†è®ºä¸ºæœºå™¨å­¦ä¹ æä¾›ç†è®ºåŸºç¡€ï¼ŒåŒ…æ‹¬VCç»´ã€PACå­¦ä¹ ç­‰æ¦‚å¿µã€‚

### 4.2. æ•°æ®ç”Ÿå‘½å‘¨æœŸ

æ•°æ®ç”Ÿå‘½å‘¨æœŸåŒ…æ‹¬æ•°æ®é‡‡é›†ã€å­˜å‚¨ã€å¤„ç†ã€åˆ†æžå’Œå½’æ¡£ç­‰é˜¶æ®µã€‚

### 4.3. æ•°æ®æ²»ç†

æ•°æ®æ²»ç†åŒ…æ‹¬æ•°æ®æ ‡å‡†ã€æ•°æ®è´¨é‡ã€æ•°æ®å®‰å…¨å’Œæ•°æ®åˆè§„ç­‰æ–¹é¢ã€‚

---

## 5. åº”ç”¨åœºæ™¯

### 5.1. æ•°æ®åˆ†æž

æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºåœ¨æ•°æ®åˆ†æžä¸­çš„åº”ç”¨åŒ…æ‹¬æè¿°æ€§åˆ†æžã€è¯Šæ–­æ€§åˆ†æžã€é¢„æµ‹æ€§åˆ†æžå’Œè§„èŒƒæ€§åˆ†æžã€‚

### 5.2. æœºå™¨å­¦ä¹ 

æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºä¸ºæœºå™¨å­¦ä¹ æä¾›ç†è®ºåŸºç¡€ï¼ŒåŒ…æ‹¬æ¨¡åž‹é€‰æ‹©ã€è¯„ä¼°å’Œä¼˜åŒ–ã€‚

### 5.3. å•†ä¸šæ™ºèƒ½

æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºåœ¨å•†ä¸šæ™ºèƒ½ä¸­çš„åº”ç”¨åŒ…æ‹¬æŠ¥è¡¨ç”Ÿæˆã€ä»ªè¡¨ç›˜å’Œå†³ç­–æ”¯æŒã€‚

---

## 6. è¡Œä¸šæ¡ˆä¾‹ä¸Žå¤šè¡¨å¾

### 6.1. å…¸åž‹è¡Œä¸šæ¡ˆä¾‹

- **æ•°å­¦åŸºç¡€ç†è®º**ï¼šæ•°æ®ç§‘å­¦ä¸­çš„æ•°å­¦åŸºç¡€ï¼ˆè¯¦è§[2.7-æ•°å­¦åŸºç¡€ç†è®º](../../../2-å½¢å¼ç§‘å­¦ç†è®º/2.7-æ•°å­¦åŸºç¡€ç†è®º/README.md)ï¼‰
- **å½¢å¼åŒ–æ¨¡åž‹**ï¼šæ•°æ®æ¨¡åž‹çš„å½¢å¼åŒ–ï¼ˆè¯¦è§[3.2-å½¢å¼åŒ–æ¨¡åž‹](../3.2-å½¢å¼åŒ–æ¨¡åž‹/README.md)ï¼‰
- **æœºå™¨å­¦ä¹ **ï¼šæ•°æ®ç§‘å­¦åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨ï¼ˆè¯¦è§[5.3-æœºå™¨å­¦ä¹ ](../../../5-è¡Œä¸šåº”ç”¨ä¸Žåœºæ™¯/5.3-æœºå™¨å­¦ä¹ /README.md)ï¼‰

### 6.2. å¤šè¡¨å¾ç¤ºä¾‹

- **ç¬¦å·è¡¨å¾**ï¼šæ•°æ®ç»“æž„ã€ç®—æ³•ã€æ¨¡åž‹ã€å…¬å¼
- **å›¾ç»“æž„**ï¼šæ•°æ®æµå›¾ã€æ¨¡åž‹ç»“æž„å›¾ã€ç®—æ³•æµç¨‹å›¾
- **å‘é‡/å¼ é‡**ï¼šç‰¹å¾å‘é‡ã€åµŒå…¥ã€å‚æ•°çŸ©é˜µ
- **è‡ªç„¶è¯­è¨€**ï¼šå®šä¹‰ã€æ³¨é‡Šã€æè¿°
- **å›¾åƒ/å¯è§†åŒ–**ï¼šç»“æž„å›¾ã€æµç¨‹å›¾ã€å¯è§†åŒ–ç»“æžœ

---

## 7. ä¸»é¢˜äº¤å‰å¼•ç”¨

| ä¸»é¢˜      | åŸºç¡€ç†è®º | å½¢å¼åŒ–æ¨¡åž‹ | åº”ç”¨åœºæ™¯ | ç®—æ³•å®žçŽ° | è¡Œä¸šæ¡ˆä¾‹ | å¤šè¡¨å¾ |
|-----------|----------|------------|----------|----------|----------|--------|
| æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ¡†æž¶| âœ… | âœ…       | âœ…     | âœ…     | âœ…     | âœ…   |

**äº¤å‰å¼•ç”¨**ï¼š

- [2.7-æ•°å­¦åŸºç¡€ç†è®º](../../../2-å½¢å¼ç§‘å­¦ç†è®º/2.7-æ•°å­¦åŸºç¡€ç†è®º/README.md)ï¼šæ•°æ®ç§‘å­¦ä¸­çš„æ•°å­¦åŸºç¡€
- [3.2-å½¢å¼åŒ–æ¨¡åž‹](../3.2-å½¢å¼åŒ–æ¨¡åž‹/README.md)ï¼šæ•°æ®æ¨¡åž‹çš„å½¢å¼åŒ–
- [5.3-æœºå™¨å­¦ä¹ ](../../../5-è¡Œä¸šåº”ç”¨ä¸Žåœºæ™¯/5.3-æœºå™¨å­¦ä¹ /README.md)ï¼šæ•°æ®ç§‘å­¦åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨

---

## 8. å…¨é“¾è·¯çŸ¥è¯†æµ

```mermaid
flowchart TD
  A[æ•°æ®ç§‘å­¦åŸºç¡€ç†è®º] --> B[åŸºç¡€ç†è®ºæ¡†æž¶]
  B --> C[æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ¡†æž¶]
  C --> D[æ•°æ®å®šä¹‰]
  C --> E[æ•°æ®åˆ†ç±»]
  C --> F[æ•°æ®è´¨é‡]
  C --> G[æ•°æ®ç”Ÿå‘½å‘¨æœŸ]
  C --> H[æ•°æ®æ²»ç†]
  D --> I[ç»“æž„åŒ–æ•°æ®]
  E --> J[åŠç»“æž„åŒ–æ•°æ®]
  F --> K[æ•°æ®å®Œæ•´æ€§]
  G --> L[é‡‡é›†/å­˜å‚¨/å¤„ç†/åˆ†æž]
  H --> M[æ•°æ®æ ‡å‡†]
  I & J & K & L & M --> N[æ•°æ®æ¨¡åž‹]
  N --> O[ç®—æ³•è®¾è®¡]
  O --> P[æœºå™¨å­¦ä¹ ]
  P --> Q[è¡Œä¸šåº”ç”¨]
  Q --> R[å¤šè¡¨å¾ä½“ç³»]
```

---

## 9. å¤šè¡¨å¾

åŸºç¡€ç†è®ºåˆ†æ”¯æ”¯æŒå¤šç§è¡¨å¾æ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š

- **ç¬¦å·è¡¨å¾**ï¼šæ•°æ®ç»“æž„ã€ç®—æ³•ã€æ¨¡åž‹ã€å…¬å¼ã€ç»Ÿè®¡æŒ‡æ ‡
- **å›¾ç»“æž„**ï¼šæ•°æ®æµå›¾ã€æ¨¡åž‹ç»“æž„å›¾ã€ç®—æ³•æµç¨‹å›¾ã€çŸ¥è¯†å›¾è°±
- **å‘é‡/å¼ é‡**ï¼šç‰¹å¾å‘é‡ã€åµŒå…¥ã€å‚æ•°çŸ©é˜µã€å¼ é‡æ•°æ®
- **è‡ªç„¶è¯­è¨€**ï¼šå®šä¹‰ã€æ³¨é‡Šã€æè¿°ã€æ–¹æ³•è®ºæ–‡æ¡£
- **å›¾åƒ/å¯è§†åŒ–**ï¼šç»“æž„å›¾ã€æµç¨‹å›¾ã€å¯è§†åŒ–ç»“æžœã€ç»Ÿè®¡å›¾è¡¨

è¿™äº›è¡¨å¾å¯äº’æ˜ ï¼Œæå‡ç†è®ºä¸Žç®—æ³•è¡¨è¾¾åŠ›ã€‚

---

## 10. å½¢å¼åŒ–è¯­ä¹‰

**è¯­ä¹‰åŸŸ**ï¼š$D$ï¼ŒåŒ…æ‹¬ï¼š

- æ•°æ®å¯¹è±¡é›†ï¼š$\mathcal{D} = \{d_1, d_2, \ldots\}$
- æ¨¡åž‹ç©ºé—´ï¼š$\mathcal{M}$
- ç®—æ³•æ“ä½œç©ºé—´ï¼š$\mathcal{A}$
- ç»Ÿè®¡ç©ºé—´ï¼š$\mathcal{S}$

**è§£é‡Šå‡½æ•°**ï¼š$I: \mathcal{L} \to D$ï¼Œå°†æ•°æ®ç§‘å­¦è§„èŒƒæ˜ å°„åˆ°è¯­ä¹‰å¯¹è±¡ï¼š

- $I(\text{data}) \in \mathcal{D}$ï¼šæ•°æ®çš„è§£é‡Š
- $I(\text{model}) \in \mathcal{M}$ï¼šæ¨¡åž‹çš„è§£é‡Š
- $I(\text{algorithm}) \in \mathcal{A}$ï¼šç®—æ³•çš„è§£é‡Š
- $I(\text{statistic}) \in \mathcal{S}$ï¼šç»Ÿè®¡é‡çš„è§£é‡Š

**è¯­ä¹‰ä¸€è‡´æ€§**ï¼šæ¯ä¸ªæ•°æ®ç»“æž„/ç®—æ³•/æ¨¡åž‹åœ¨$D$ä¸­æœ‰æ˜Žç¡®å®šä¹‰ï¼Œæ»¡è¶³æ•°æ®è´¨é‡è¦æ±‚ã€æ¨¡åž‹æœ‰æ•ˆæ€§ã€ç®—æ³•æ­£ç¡®æ€§ã€‚

---

## 11. å½¢å¼åŒ–è¯­æ³•ä¸Žè¯æ˜Ž

**è¯­æ³•è§„åˆ™**ï¼š

- **æ•°æ®ç»“æž„è¯­æ³•**ï¼š$DS ::= Array \mid List \mid Tree \mid Graph$
- **ç®—æ³•è¯­æ³•**ï¼š$Algorithm ::= Sort \mid Search \mid Cluster \mid Classify$
- **æ¨¡åž‹è¯­æ³•**ï¼š$Model ::= Linear \mid NonLinear \mid Ensemble$

**æŽ¨ç†è§„åˆ™**ï¼š

- **æ•°æ®è´¨é‡**ï¼š$\frac{Quality(d) \geq threshold}{d \text{ is valid}}$
- **æ¨¡åž‹æœ‰æ•ˆæ€§**ï¼š$\frac{Accuracy(m) \geq threshold}{m \text{ is valid}}$

**å®šç†**ï¼šåŸºç¡€ç†è®ºåˆ†æ”¯çš„è¯­æ³•ç³»ç»Ÿå…·ä¸€è‡´æ€§ä¸Žå¯æ‰©å±•æ€§ã€‚

**è¯æ˜Ž**ï¼šç”±æ•°æ®ç»“æž„ã€ç®—æ³•å®šä¹‰ä¸ŽæŽ¨ç†è§„åˆ™é€’å½’å®šä¹‰ï¼Œä¿è¯ç³»ç»Ÿä¸€è‡´ä¸Žå¯æ‰©å±•ã€‚

---

## 12. å·¥å…·ä¸Žå®žçŽ°

### 12.1. æ•°æ®å¤„ç†å·¥å…·

- **Python**ï¼šPandasã€NumPyã€SciPy
- **Rè¯­è¨€**ï¼šç»Ÿè®¡åˆ†æž
- **SQL**ï¼šæ•°æ®æŸ¥è¯¢

### 12.2. æœºå™¨å­¦ä¹ å·¥å…·

- **Scikit-learn**ï¼šPythonæœºå™¨å­¦ä¹ åº“
- **TensorFlow**ï¼šæ·±åº¦å­¦ä¹ æ¡†æž¶
- **PyTorch**ï¼šæ·±åº¦å­¦ä¹ æ¡†æž¶

### 12.3. å¯è§†åŒ–å·¥å…·

- **Matplotlib**ï¼šPythonç»˜å›¾
- **Seaborn**ï¼šç»Ÿè®¡å¯è§†åŒ–
- **Plotly**ï¼šäº¤äº’å¼å¯è§†åŒ–

---

## 13. å­¦ä¹ ä¸Žç ”ç©¶è·¯å¾„

### 13.1. åŸºç¡€çŸ¥è¯†

1. **æ•°å­¦åŸºç¡€**ï¼šçº¿æ€§ä»£æ•°ã€æ¦‚çŽ‡è®ºã€ç»Ÿè®¡å­¦
2. **ç¼–ç¨‹åŸºç¡€**ï¼šPythonã€Rã€SQL

### 13.2. æ ¸å¿ƒæŠ€èƒ½

1. **æ•°æ®å¤„ç†**ï¼šæ•°æ®æ¸…æ´—ã€è½¬æ¢ã€éªŒè¯
2. **ç»Ÿè®¡åˆ†æž**ï¼šæè¿°ç»Ÿè®¡ã€æŽ¨æ–­ç»Ÿè®¡
3. **æœºå™¨å­¦ä¹ **ï¼šç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ 

### 13.3. å®žè·µåº”ç”¨

1. **é¡¹ç›®å®žè·µ**ï¼šæ•°æ®åˆ†æžé¡¹ç›®ã€æœºå™¨å­¦ä¹ é¡¹ç›®
2. **å·¥å…·æŽŒæ¡**ï¼šæ•°æ®å¤„ç†å·¥å…·ã€æœºå™¨å­¦ä¹ å·¥å…·
3. **ç ”ç©¶å‰æ²¿**ï¼šæ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ 

---

## 14. æ€»ç»“

æ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ˜¯æ•°æ®ç§‘å­¦å­¦ç§‘çš„ç†è®ºåŸºç¡€ï¼Œä¸ºæ•°æ®ç§‘å­¦å®žè·µæä¾›ç†è®ºæŒ‡å¯¼å’Œæ–¹æ³•è®ºã€‚ä»Žæ•°æ®å®šä¹‰åˆ°æ•°æ®æ²»ç†ï¼Œä»Žç»Ÿè®¡å­¦ä¹ åˆ°æœºå™¨å­¦ä¹ ï¼Œæ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºæ¶µç›–äº†æ•°æ®ç§‘å­¦çš„å„ä¸ªæ–¹é¢ã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š

1. **ç†è®ºåŸºç¡€**ï¼šä¸ºæ•°æ®ç§‘å­¦å®žè·µæä¾›ç†è®ºåŸºç¡€
2. **æ–¹æ³•è®º**ï¼šæä¾›ç³»ç»ŸåŒ–çš„æ•°æ®ç§‘å­¦æ–¹æ³•è®º
3. **å·¥å…·æ”¯æŒ**ï¼šæä¾›å¼ºå¤§çš„å·¥å…·å’Œæ¡†æž¶
4. **åº”ç”¨å¯¼å‘**ï¼šé¢å‘å®žé™…åº”ç”¨å’Œé—®é¢˜è§£å†³

**åº”ç”¨å‰æ™¯**ï¼š
éšç€å¤§æ•°æ®ã€äººå·¥æ™ºèƒ½ç­‰æŠ€æœ¯çš„å‘å±•ï¼Œæ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºå°†ç»§ç»­å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ã€å¯è§£é‡ŠAIã€æ•°æ®æ²»ç†ç­‰é¢†åŸŸï¼Œæ•°æ®ç§‘å­¦åŸºç¡€ç†è®ºå°†æä¾›æ›´å¼ºå¤§çš„ç†è®ºæ”¯æ’‘ã€‚

---

## 15. æ•°æ®ç§‘å­¦æ–¹æ³•è®ºè¯¦è§£

### 15.1. CRISP-DMæ¨¡åž‹è¯¦è§£

**ä¸šåŠ¡ç†è§£é˜¶æ®µ**ï¼š

- ç¡®å®šä¸šåŠ¡ç›®æ ‡
- è¯„ä¼°å½“å‰æƒ…å†µ
- åˆ¶å®šæ•°æ®æŒ–æŽ˜ç›®æ ‡
- åˆ¶å®šé¡¹ç›®è®¡åˆ’

**æ•°æ®ç†è§£é˜¶æ®µ**ï¼š

- æ”¶é›†åˆå§‹æ•°æ®
- æè¿°æ•°æ®
- æŽ¢ç´¢æ•°æ®
- éªŒè¯æ•°æ®è´¨é‡

**æ•°æ®å‡†å¤‡é˜¶æ®µ**ï¼š

- é€‰æ‹©æ•°æ®
- æ¸…æ´—æ•°æ®
- æž„é€ æ•°æ®
- é›†æˆæ•°æ®
- æ ¼å¼åŒ–æ•°æ®

**å»ºæ¨¡é˜¶æ®µ**ï¼š

- é€‰æ‹©å»ºæ¨¡æŠ€æœ¯
- ç”Ÿæˆæµ‹è¯•è®¾è®¡
- å»ºç«‹æ¨¡åž‹
- è¯„ä¼°æ¨¡åž‹

**è¯„ä¼°é˜¶æ®µ**ï¼š

- è¯„ä¼°ç»“æžœ
- å®¡æŸ¥è¿‡ç¨‹
- ç¡®å®šä¸‹ä¸€æ­¥

**éƒ¨ç½²é˜¶æ®µ**ï¼š

- åˆ¶å®šéƒ¨ç½²è®¡åˆ’
- ç›‘æŽ§å’Œç»´æŠ¤
- ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
- å®¡æŸ¥é¡¹ç›®

### 15.2. æ•°æ®ç§‘å­¦å·¥ä½œæµ

**å…¸åž‹å·¥ä½œæµ**ï¼š

1. é—®é¢˜å®šä¹‰
2. æ•°æ®æ”¶é›†
3. æ•°æ®æŽ¢ç´¢
4. ç‰¹å¾å·¥ç¨‹
5. æ¨¡åž‹è®­ç»ƒ
6. æ¨¡åž‹è¯„ä¼°
7. æ¨¡åž‹éƒ¨ç½²
8. æ¨¡åž‹ç›‘æŽ§

### 15.3. æ•°æ®ç§‘å­¦æœ€ä½³å®žè·µ

**æ•°æ®ç®¡ç†**ï¼š

- æ•°æ®ç‰ˆæœ¬æŽ§åˆ¶
- æ•°æ®æ–‡æ¡£åŒ–
- æ•°æ®è´¨é‡ä¿è¯
- æ•°æ®å®‰å…¨

**æ¨¡åž‹ç®¡ç†**ï¼š

- æ¨¡åž‹ç‰ˆæœ¬æŽ§åˆ¶
- æ¨¡åž‹æ–‡æ¡£åŒ–
- æ¨¡åž‹ç›‘æŽ§
- æ¨¡åž‹æ›´æ–°

---

## 16. æ•°æ®ç§‘å­¦å·¥å…·ç”Ÿæ€

### 16.1. æ•°æ®å¤„ç†å·¥å…·

**Pythonç”Ÿæ€ç³»ç»Ÿ**ï¼š

- Pandasï¼šæ•°æ®åˆ†æžå’Œå¤„ç†
- NumPyï¼šæ•°å€¼è®¡ç®—
- SciPyï¼šç§‘å­¦è®¡ç®—
- Daskï¼šå¹¶è¡Œè®¡ç®—

**Rè¯­è¨€**ï¼š

- dplyrï¼šæ•°æ®æ“ä½œ
- ggplot2ï¼šæ•°æ®å¯è§†åŒ–
- caretï¼šæœºå™¨å­¦ä¹ 

### 16.2. æœºå™¨å­¦ä¹ æ¡†æž¶

**ä¼ ç»Ÿæœºå™¨å­¦ä¹ **ï¼š

- Scikit-learnï¼šPythonæœºå™¨å­¦ä¹ åº“
- XGBoostï¼šæ¢¯åº¦æå‡æ¡†æž¶
- LightGBMï¼šè½»é‡çº§æ¢¯åº¦æå‡

**æ·±åº¦å­¦ä¹ **ï¼š

- TensorFlowï¼šGoogleæ·±åº¦å­¦ä¹ æ¡†æž¶
- PyTorchï¼šFacebookæ·±åº¦å­¦ä¹ æ¡†æž¶
- Kerasï¼šé«˜çº§ç¥žç»ç½‘ç»œAPI

### 16.3. å¤§æ•°æ®å·¥å…·

**åˆ†å¸ƒå¼è®¡ç®—**ï¼š

- Apache Sparkï¼šå¤§æ•°æ®å¤„ç†
- Apache Hadoopï¼šåˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—
- Apache Flinkï¼šæµå¤„ç†

**æ•°æ®å­˜å‚¨**ï¼š

- HDFSï¼šåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
- HBaseï¼šNoSQLæ•°æ®åº“
- Cassandraï¼šåˆ†å¸ƒå¼æ•°æ®åº“

---

## 17. æ•°æ®ç§‘å­¦ä¼¦ç†ä¸Žåˆè§„

### 17.1. æ•°æ®éšç§

**éšç§ä¿æŠ¤æŠ€æœ¯**ï¼š

- æ•°æ®è„±æ•
- å·®åˆ†éšç§
- åŒæ€åŠ å¯†
- è”é‚¦å­¦ä¹ 

### 17.2. ç®—æ³•å…¬å¹³æ€§

**å…¬å¹³æ€§æŒ‡æ ‡**ï¼š

- ç»Ÿè®¡å‡ç­‰
- æœºä¼šå‡ç­‰
- ä¸ªä½“å…¬å¹³

### 17.3. å¯è§£é‡Šæ€§

**å¯è§£é‡Šæ€§æ–¹æ³•**ï¼š

- LIMEï¼šå±€éƒ¨å¯è§£é‡Šæ¨¡åž‹
- SHAPï¼šSHapley Additive exPlanations
- ç‰¹å¾é‡è¦æ€§
- æ¨¡åž‹å¯è§†åŒ–

**LIMEå¯è§£é‡Šæ€§ç¤ºä¾‹**ï¼š

```python
from lime import lime_tabular
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# åŠ è½½æ•°æ®
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# è®­ç»ƒæ¨¡åž‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# åˆ›å»ºLIMEè§£é‡Šå™¨
explainer = lime_tabular.LimeTabularExplainer(
    X_train,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    mode='classification'
)

# è§£é‡Šå•ä¸ªé¢„æµ‹
instance_idx = 0
explanation = explainer.explain_instance(
    X_test[instance_idx],
    model.predict_proba,
    num_features=4
)

# æ˜¾ç¤ºè§£é‡Šç»“æžœ
print(f"çœŸå®žæ ‡ç­¾: {iris.target_names[y_test[instance_idx]]}")
print(f"é¢„æµ‹æ ‡ç­¾: {iris.target_names[model.predict([X_test[instance_idx]])[0]]}")
explanation.show_in_notebook(show_table=True)
```

**SHAPå¯è§£é‡Šæ€§ç¤ºä¾‹**ï¼š

```python
import shap
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
import pandas as pd

# åŠ è½½æ•°æ®
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# è®­ç»ƒæ¨¡åž‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# åˆ›å»ºSHAPè§£é‡Šå™¨
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# å¯è§†åŒ–SHAPå€¼
shap.summary_plot(shap_values, X, plot_type="bar")
shap.summary_plot(shap_values[0], X)  # å¯¹äºŽç±»åˆ«0

# å•ä¸ªæ ·æœ¬çš„è§£é‡Š
shap.force_plot(
    explainer.expected_value[0],
    shap_values[0][0],
    X.iloc[0],
    matplotlib=True
)
```

---

## æ·±å…¥å­¦ä¹ å»ºè®®

### ç†è®ºåŸºç¡€å¼ºåŒ–

**æ•°å­¦åŸºç¡€**ï¼š

- **çº¿æ€§ä»£æ•°**ï¼šçŸ©é˜µè¿ç®—ã€å‘é‡ç©ºé—´
- **æ¦‚çŽ‡è®º**ï¼šæ¦‚çŽ‡åˆ†å¸ƒã€ç»Ÿè®¡æŽ¨æ–­
- **å¾®ç§¯åˆ†**ï¼šä¼˜åŒ–ç†è®ºã€æ¢¯åº¦ä¸‹é™
- **ç»Ÿè®¡å­¦**ï¼šå‡è®¾æ£€éªŒã€å›žå½’åˆ†æž

**è®¡ç®—æœºç§‘å­¦åŸºç¡€**ï¼š

- **æ•°æ®ç»“æž„**ï¼šæ ‘ã€å›¾ã€å“ˆå¸Œè¡¨
- **ç®—æ³•è®¾è®¡**ï¼šæŽ’åºã€æœç´¢ã€å›¾ç®—æ³•
- **æ•°æ®åº“ç†è®º**ï¼šå…³ç³»æ•°æ®åº“ã€NoSQL
- **åˆ†å¸ƒå¼ç³»ç»Ÿ**ï¼šåˆ†å¸ƒå¼è®¡ç®—ã€å¹¶è¡Œå¤„ç†

### å®žè·µèƒ½åŠ›æå‡

**ç¼–ç¨‹èƒ½åŠ›**ï¼š

- **Python**ï¼šNumPyã€Pandasã€Scikit-learn
- **R**ï¼šç»Ÿè®¡åˆ†æžã€æ•°æ®å¯è§†åŒ–
- **SQL**ï¼šæ•°æ®æŸ¥è¯¢å’Œå¤„ç†
- **å¤§æ•°æ®å·¥å…·**ï¼šSparkã€Hadoop

**Pythonæ•°æ®ç§‘å­¦å®Œæ•´ç¤ºä¾‹**ï¼š

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class DataSciencePipeline:
    """
    æ•°æ®ç§‘å­¦å®Œæ•´æµç¨‹ç¤ºä¾‹

    åŒ…å«ï¼šæ•°æ®åŠ è½½ã€æŽ¢ç´¢æ€§åˆ†æžã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡åž‹è®­ç»ƒã€è¯„ä¼°
    """

    def __init__(self, data_path):
        """åˆå§‹åŒ–æ•°æ®ç§‘å­¦æµç¨‹"""
        self.data_path = data_path
        self.data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.model = None

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        self.data = pd.read_csv(self.data_path)
        print(f"æ•°æ®å½¢çŠ¶: {self.data.shape}")
        print(f"ç¼ºå¤±å€¼:\n{self.data.isnull().sum()}")
        return self.data

    def exploratory_analysis(self):
        """æŽ¢ç´¢æ€§æ•°æ®åˆ†æž"""
        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        print("åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
        print(self.data.describe())

        # æ•°æ®åˆ†å¸ƒå¯è§†åŒ–
        if self.data.select_dtypes(include=[np.number]).shape[1] > 0:
            numeric_cols = self.data.select_dtypes(include=[np.number]).columns
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            for idx, col in enumerate(numeric_cols[:4]):
                ax = axes[idx // 2, idx % 2]
                self.data[col].hist(bins=30, ax=ax)
                ax.set_title(f'{col} åˆ†å¸ƒ')
            plt.tight_layout()
            plt.show()

        # ç›¸å…³æ€§åˆ†æž
        if self.data.select_dtypes(include=[np.number]).shape[1] > 1:
            corr_matrix = self.data.select_dtypes(include=[np.number]).corr()
            plt.figure(figsize=(10, 8))
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
            plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
            plt.show()

    def preprocess_data(self, target_column, test_size=0.2):
        """æ•°æ®é¢„å¤„ç†"""
        # å¤„ç†ç¼ºå¤±å€¼
        self.data = self.data.fillna(self.data.mean())

        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        X = self.data.drop(columns=[target_column])
        y = self.data[target_column]

        # ç¼–ç åˆ†ç±»å˜é‡
        X = pd.get_dummies(X, drop_first=True)

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # ç‰¹å¾ç¼©æ”¾
        self.X_train = self.scaler.fit_transform(self.X_train)
        self.X_test = self.scaler.transform(self.X_test)

        return self.X_train, self.X_test, self.y_train, self.y_test

    def train_model(self, model_type='random_forest', **kwargs):
        """è®­ç»ƒæ¨¡åž‹"""
        if model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=kwargs.get('n_estimators', 100),
                max_depth=kwargs.get('max_depth', None),
                random_state=42
            )

        self.model.fit(self.X_train, self.y_train)
        return self.model

    def evaluate_model(self):
        """è¯„ä¼°æ¨¡åž‹"""
        # è®­ç»ƒé›†é¢„æµ‹
        y_train_pred = self.model.predict(self.X_train)
        train_accuracy = (y_train_pred == self.y_train).mean()

        # æµ‹è¯•é›†é¢„æµ‹
        y_test_pred = self.model.predict(self.X_test)
        test_accuracy = (y_test_pred == self.y_test).mean()

        print(f"è®­ç»ƒé›†å‡†ç¡®çŽ‡: {train_accuracy:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®çŽ‡: {test_accuracy:.4f}")

        # åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(self.y_test, y_test_pred))

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_test_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®žæ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.show()

        return {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'confusion_matrix': cm
        }

    def feature_importance(self):
        """ç‰¹å¾é‡è¦æ€§åˆ†æž"""
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            feature_names = pd.get_dummies(
                self.data.drop(columns=[self.data.columns[-1]]),
                drop_first=True
            ).columns

            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            feature_importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)

            # å¯è§†åŒ–
            plt.figure(figsize=(10, 6))
            sns.barplot(data=feature_importance_df.head(10),
                       x='importance', y='feature')
            plt.title('Top 10 ç‰¹å¾é‡è¦æ€§')
            plt.xlabel('é‡è¦æ€§')
            plt.tight_layout()
            plt.show()

            return feature_importance_df

# ä½¿ç”¨ç¤ºä¾‹
# pipeline = DataSciencePipeline('data.csv')
# pipeline.load_data()
# pipeline.exploratory_analysis()
# pipeline.preprocess_data('target_column')
# pipeline.train_model()
# pipeline.evaluate_model()
# pipeline.feature_importance()
```

**å·¥å…·ä½¿ç”¨**ï¼š

- **æ•°æ®å·¥å…·**ï¼šJupyterã€Tableau
- **æœºå™¨å­¦ä¹ **ï¼šTensorFlowã€PyTorch
- **æ•°æ®å­˜å‚¨**ï¼šæ•°æ®åº“ã€æ•°æ®ä»“åº“
- **å¯è§†åŒ–å·¥å…·**ï¼šMatplotlibã€D3.js

**Jupyter Notebookæ•°æ®åˆ†æžç¤ºä¾‹**ï¼š

```python
# å®Œæ•´çš„æ•°æ®åˆ†æžå·¥ä½œæµ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingRegressor

# 1. æ•°æ®åŠ è½½å’Œåˆæ­¥æŽ¢ç´¢
df = pd.read_csv('sales_data.csv')
print("æ•°æ®æ¦‚è§ˆ:")
print(df.head())
print(f"\næ•°æ®å½¢çŠ¶: {df.shape}")
print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{df.isnull().sum()}")

# 2. æ•°æ®æ¸…æ´—
# å¤„ç†ç¼ºå¤±å€¼
df['price'] = df['price'].fillna(df['price'].median())
df['category'] = df['category'].fillna('Unknown')

# å¤„ç†å¼‚å¸¸å€¼
Q1 = df['sales'].quantile(0.25)
Q3 = df['sales'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['sales'] >= Q1 - 1.5*IQR) & (df['sales'] <= Q3 + 1.5*IQR)]

# 3. ç‰¹å¾å·¥ç¨‹
# ç¼–ç åˆ†ç±»å˜é‡
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['category'])

# åˆ›å»ºæ—¶é—´ç‰¹å¾
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek

# 4. æ•°æ®å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# é”€å”®é¢æ—¶é—´åºåˆ—
axes[0, 0].plot(df.groupby('date')['sales'].sum())
axes[0, 0].set_title('é”€å”®é¢æ—¶é—´åºåˆ—')
axes[0, 0].set_xlabel('æ—¥æœŸ')
axes[0, 0].set_ylabel('é”€å”®é¢')

# ç±»åˆ«åˆ†å¸ƒ
df['category'].value_counts().plot(kind='bar', ax=axes[0, 1])
axes[0, 1].set_title('ç±»åˆ«åˆ†å¸ƒ')
axes[0, 1].set_xlabel('ç±»åˆ«')
axes[0, 1].set_ylabel('æ•°é‡')

# ä»·æ ¼ä¸Žé”€å”®é¢å…³ç³»
axes[1, 0].scatter(df['price'], df['sales'], alpha=0.5)
axes[1, 0].set_title('ä»·æ ¼ vs é”€å”®é¢')
axes[1, 0].set_xlabel('ä»·æ ¼')
axes[1, 0].set_ylabel('é”€å”®é¢')

# é”€å”®é¢åˆ†å¸ƒ
axes[1, 1].hist(df['sales'], bins=50)
axes[1, 1].set_title('é”€å”®é¢åˆ†å¸ƒ')
axes[1, 1].set_xlabel('é”€å”®é¢')
axes[1, 1].set_ylabel('é¢‘æ•°')

plt.tight_layout()
plt.show()

# 5. æ¨¡åž‹è®­ç»ƒå’Œè¯„ä¼°
features = ['price', 'category_encoded', 'year', 'month', 'day_of_week']
X = df[features]
y = df['sales']

model = GradientBoostingRegressor(n_estimators=100, random_state=42)
scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"\næ¨¡åž‹RÂ²å¾—åˆ†: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

model.fit(X, y)
print(f"ç‰¹å¾é‡è¦æ€§:\n{pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)}")
```

---

## å­¦ä¹ èµ„æºæ±‡æ€»

### åœ¨çº¿è¯¾ç¨‹

- **Coursera**ï¼šæ•°æ®ç§‘å­¦è¯¾ç¨‹
- **edX**ï¼šæ•°æ®åˆ†æžè¯¾ç¨‹
- **Udemy**ï¼šæ•°æ®ç§‘å­¦å®žè·µè¯¾ç¨‹
- **YouTube**ï¼šæ•°æ®ç§‘å­¦æ•™ç¨‹

### ä¹¦ç±æŽ¨è

- **å…¥é—¨ä¹¦ç±**ï¼šã€Šæ•°æ®ç§‘å­¦å¯¼è®ºã€‹
- **è¿›é˜¶ä¹¦ç±**ï¼šã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹
- **åº”ç”¨ä¹¦ç±**ï¼šã€Šæ•°æ®ç§‘å­¦å®žæˆ˜ã€‹
- **å·¥å…·ä¹¦ç±**ï¼šã€ŠPythonæ•°æ®ç§‘å­¦æ‰‹å†Œã€‹

### å­¦æœ¯èµ„æº

- **æœŸåˆŠ**ï¼šJournal of Data Scienceã€IEEE TKDE
- **ä¼šè®®**ï¼šKDDã€ICDMã€SIGMOD
- **æ•°æ®åº“**ï¼šIEEE Xploreã€ACM Digital Library
- **é¢„å°æœ¬**ï¼šarXivã€ResearchGate

---

## å®žè·µé¡¹ç›®å»ºè®®

### åŸºç¡€é¡¹ç›®

- **æ•°æ®åˆ†æž**ï¼šæ•°æ®åˆ†æžé¡¹ç›®
- **æ•°æ®å¯è§†åŒ–**ï¼šæ•°æ®å¯è§†åŒ–é¡¹ç›®
- **æœºå™¨å­¦ä¹ **ï¼šæœºå™¨å­¦ä¹ é¡¹ç›®
- **æ•°æ®æŒ–æŽ˜**ï¼šæ•°æ®æŒ–æŽ˜é¡¹ç›®

### è¿›é˜¶é¡¹ç›®

- **å¤§æ•°æ®å¤„ç†**ï¼šå¤§æ•°æ®å¤„ç†é¡¹ç›®
- **æ·±åº¦å­¦ä¹ **ï¼šæ·±åº¦å­¦ä¹ é¡¹ç›®
- **æŽ¨èç³»ç»Ÿ**ï¼šæŽ¨èç³»ç»Ÿé¡¹ç›®
- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šNLPé¡¹ç›®

### é«˜çº§é¡¹ç›®

- **ç«¯åˆ°ç«¯ç³»ç»Ÿ**ï¼šå®Œæ•´çš„æ•°æ®ç§‘å­¦ç³»ç»Ÿ
- **å®žæ—¶ç³»ç»Ÿ**ï¼šå®žæ—¶æ•°æ®å¤„ç†ç³»ç»Ÿ
- **åˆ†å¸ƒå¼ç³»ç»Ÿ**ï¼šåˆ†å¸ƒå¼æ•°æ®å¤„ç†
- **AIåº”ç”¨**ï¼šAIåº”ç”¨ç³»ç»Ÿ

---

## èŒä¸šå‘å±•è·¯å¾„

### å­¦æœ¯ç ”ç©¶

- **ç ”ç©¶æ–¹å‘**ï¼šæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€æ•°æ®æŒ–æŽ˜
- **èŒä¸šè·¯å¾„**ï¼šåšå£«ç ”ç©¶ã€åšå£«åŽã€æ•™èŒã€ç ”ç©¶æœºæž„

### å·¥ä¸šåº”ç”¨

- **åº”ç”¨é¢†åŸŸ**ï¼šæ•°æ®åˆ†æžã€æœºå™¨å­¦ä¹ ã€æ•°æ®å·¥ç¨‹
- **èŒä¸šè·¯å¾„**ï¼šæ•°æ®ç§‘å­¦å®¶ã€æ•°æ®åˆ†æžå¸ˆã€æ•°æ®å·¥ç¨‹å¸ˆ

---

[è¿”å›žæ•°æ®æ¨¡åž‹ä¸Žç®—æ³•æ€»å¯¼èˆª](../README.md)
