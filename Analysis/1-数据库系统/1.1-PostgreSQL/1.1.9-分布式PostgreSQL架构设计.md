# 1.1.9 åˆ†å¸ƒå¼PostgreSQLæ¶æ„è®¾è®¡

## ğŸ“‹ æ¦‚è¿°

åˆ†å¸ƒå¼PostgreSQLæ¶æ„æ˜¯ç°ä»£å¤§è§„æ¨¡æ•°æ®åº”ç”¨çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œé€šè¿‡æ°´å¹³æ‰©å±•ã€é«˜å¯ç”¨æ€§ã€å®¹é”™æœºåˆ¶ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œä¸ºæµ·é‡æ•°æ®å¤„ç†æä¾›å¼ºå¤§çš„æ”¯æ’‘ã€‚æœ¬æ–‡æ¡£ç³»ç»Ÿæ€§åœ°é˜è¿°åˆ†å¸ƒå¼PostgreSQLçš„æ¶æ„æ¨¡å¼ã€è®¾è®¡åŸåˆ™ã€å®ç°æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ ç†è®ºåŸºç¡€

### 1. åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€æ¦‚å¿µ

#### 1.1 åˆ†å¸ƒå¼ç³»ç»Ÿç‰¹å¾

åˆ†å¸ƒå¼PostgreSQLç³»ç»Ÿå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒç‰¹å¾ï¼š

```mermaid
graph TD
    A[åˆ†å¸ƒå¼ç³»ç»Ÿç‰¹å¾] --> B[æ°´å¹³æ‰©å±•æ€§]
    A --> C[é«˜å¯ç”¨æ€§]
    A --> D[å®¹é”™æ€§]
    A --> E[ä¸€è‡´æ€§]
    A --> F[åˆ†åŒºå®¹å¿æ€§]
    
    B --> G[çº¿æ€§æ‰©å±•]
    B --> H[åŠ¨æ€æ‰©å®¹]
    
    C --> I[æ•…éšœæ¢å¤]
    C --> J[æœåŠ¡è¿ç»­æ€§]
    
    D --> K[æ•…éšœéš”ç¦»]
    D --> L[è‡ªåŠ¨æ¢å¤]
    
    E --> M[å¼ºä¸€è‡´æ€§]
    E --> N[æœ€ç»ˆä¸€è‡´æ€§]
    
    F --> P[ç½‘ç»œåˆ†åŒº]
    F --> Q[éƒ¨åˆ†å¯ç”¨]
```

#### 1.2 CAPå®šç†ä¸åˆ†å¸ƒå¼ä¸€è‡´æ€§

**å®šç† 1.1 (CAPå®šç†)**
åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œæœ€å¤šåªèƒ½åŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸‰ä¸ªç‰¹æ€§ä¸­çš„ä¸¤ä¸ªï¼š

- **ä¸€è‡´æ€§(Consistency)**ï¼šæ‰€æœ‰èŠ‚ç‚¹çœ‹åˆ°çš„æ•°æ®æ˜¯ä¸€è‡´çš„
- **å¯ç”¨æ€§(Availability)**ï¼šæ¯ä¸ªè¯·æ±‚éƒ½èƒ½æ”¶åˆ°å“åº”
- **åˆ†åŒºå®¹å¿æ€§(Partition Tolerance)**ï¼šç³»ç»Ÿåœ¨ç½‘ç»œåˆ†åŒºæ—¶ä»èƒ½æ­£å¸¸å·¥ä½œ

**å®šä¹‰ 1.1 (åˆ†å¸ƒå¼ä¸€è‡´æ€§æ¨¡å‹)**
å¯¹äºåˆ†å¸ƒå¼PostgreSQLç³»ç»Ÿï¼Œä¸€è‡´æ€§æ¨¡å‹å®šä¹‰ä¸ºï¼š

1. **å¼ºä¸€è‡´æ€§**ï¼š$\forall i,j \in N, \forall t \in T, v_i(t) = v_j(t)$
2. **æœ€ç»ˆä¸€è‡´æ€§**ï¼š$\lim_{t \to \infty} v_i(t) = \lim_{t \to \infty} v_j(t)$
3. **å› æœä¸€è‡´æ€§**ï¼š$e_1 \rightarrow e_2 \Rightarrow v(e_1) \leq v(e_2)$

å…¶ä¸­ï¼š

- $N$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $T$ æ˜¯æ—¶é—´åŸŸ
- $v_i(t)$ æ˜¯èŠ‚ç‚¹ $i$ åœ¨æ—¶é—´ $t$ çš„æ•°æ®ç‰ˆæœ¬
- $e_1 \rightarrow e_2$ è¡¨ç¤ºäº‹ä»¶ $e_1$ å› æœå…ˆäºäº‹ä»¶ $e_2$

#### 1.3 åˆ†å¸ƒå¼äº‹åŠ¡ç†è®º

**å®šä¹‰ 1.2 (åˆ†å¸ƒå¼äº‹åŠ¡)**
åˆ†å¸ƒå¼äº‹åŠ¡æ˜¯ä¸€ä¸ªå››å…ƒç»„ $T = (O, S, C, A)$ï¼Œå…¶ä¸­ï¼š

- $O$ æ˜¯æ“ä½œé›†åˆ
- $S$ æ˜¯çŠ¶æ€é›†åˆ
- $C$ æ˜¯æäº¤æ¡ä»¶
- $A$ æ˜¯ä¸­æ­¢æ¡ä»¶

**ä¸¤é˜¶æ®µæäº¤åè®®(2PC)**:

```rust
// Rustå®ç°çš„ä¸¤é˜¶æ®µæäº¤åè®®
use std::collections::HashMap;
use tokio::sync::mpsc;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionState {
    Initial,
    Prepared,
    Committed,
    Aborted,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Message {
    Prepare { transaction_id: String },
    Prepared { transaction_id: String, participant_id: String },
    Commit { transaction_id: String },
    Abort { transaction_id: String },
}

pub struct TwoPhaseCommit {
    transaction_id: String,
    participants: Vec<String>,
    state: TransactionState,
    prepared_participants: HashMap<String, bool>,
}

impl TwoPhaseCommit {
    pub fn new(transaction_id: String, participants: Vec<String>) -> Self {
        Self {
            transaction_id,
            participants,
            state: TransactionState::Initial,
            prepared_participants: HashMap::new(),
        }
    }
    
    pub async fn execute(&mut self, tx: mpsc::Sender<Message>) -> Result<bool, String> {
        // ç¬¬ä¸€é˜¶æ®µï¼šå‡†å¤‡é˜¶æ®µ
        let prepare_result = self.prepare_phase(&tx).await?;
        
        if !prepare_result {
            self.abort_phase(&tx).await?;
            return Ok(false);
        }
        
        // ç¬¬äºŒé˜¶æ®µï¼šæäº¤é˜¶æ®µ
        let commit_result = self.commit_phase(&tx).await?;
        Ok(commit_result)
    }
    
    async fn prepare_phase(&mut self, tx: &mpsc::Sender<Message>) -> Result<bool, String> {
        // å‘é€å‡†å¤‡æ¶ˆæ¯ç»™æ‰€æœ‰å‚ä¸è€…
        for participant in &self.participants {
            let msg = Message::Prepare {
                transaction_id: self.transaction_id.clone(),
            };
            tx.send(msg).await.map_err(|e| e.to_string())?;
        }
        
        // ç­‰å¾…æ‰€æœ‰å‚ä¸è€…å“åº”
        let mut prepared_count = 0;
        for _ in 0..self.participants.len() {
            // è¿™é‡Œåº”è¯¥æ¥æ”¶å“åº”æ¶ˆæ¯
            prepared_count += 1;
        }
        
        Ok(prepared_count == self.participants.len())
    }
    
    async fn commit_phase(&mut self, tx: &mpsc::Sender<Message>) -> Result<bool, String> {
        // å‘é€æäº¤æ¶ˆæ¯ç»™æ‰€æœ‰å‚ä¸è€…
        for participant in &self.participants {
            let msg = Message::Commit {
                transaction_id: self.transaction_id.clone(),
            };
            tx.send(msg).await.map_err(|e| e.to_string())?;
        }
        
        self.state = TransactionState::Committed;
        Ok(true)
    }
    
    async fn abort_phase(&mut self, tx: &mpsc::Sender<Message>) -> Result<(), String> {
        // å‘é€ä¸­æ­¢æ¶ˆæ¯ç»™æ‰€æœ‰å‚ä¸è€…
        for participant in &self.participants {
            let msg = Message::Abort {
                transaction_id: self.transaction_id.clone(),
            };
            tx.send(msg).await.map_err(|e| e.to_string())?;
        }
        
        self.state = TransactionState::Aborted;
        Ok(())
    }
}
```

### 2. åˆ†å¸ƒå¼æ¶æ„æ¨¡å¼

#### 2.1 ä¸»ä»å¤åˆ¶æ¶æ„

**å®šä¹‰ 2.1 (ä¸»ä»å¤åˆ¶)**
ä¸»ä»å¤åˆ¶æ˜¯ä¸€ç§åˆ†å¸ƒå¼æ¶æ„æ¨¡å¼ï¼Œå…¶ä¸­ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºä¸»èŠ‚ç‚¹å¤„ç†å†™æ“ä½œï¼Œå¤šä¸ªä»èŠ‚ç‚¹å¤åˆ¶ä¸»èŠ‚ç‚¹çš„æ•°æ®å¹¶æä¾›è¯»æœåŠ¡ã€‚

**ä¸»ä»å¤åˆ¶ç‰¹å¾**ï¼š

```mermaid
graph TD
    A[ä¸»èŠ‚ç‚¹] --> B[ä»èŠ‚ç‚¹1]
    A --> C[ä»èŠ‚ç‚¹2]
    A --> D[ä»èŠ‚ç‚¹3]
    
    E[å†™è¯·æ±‚] --> A
    F[è¯»è¯·æ±‚] --> B
    G[è¯»è¯·æ±‚] --> C
    H[è¯»è¯·æ±‚] --> D
    
    A --> I[WALæ—¥å¿—]
    I --> B
    I --> C
    I --> D
```

**å¤åˆ¶å»¶è¿Ÿè®¡ç®—**ï¼š

$$
\text{å¤åˆ¶å»¶è¿Ÿ} = \text{ç½‘ç»œå»¶è¿Ÿ} + \text{WALä¼ è¾“æ—¶é—´} + \text{åº”ç”¨æ—¶é—´}
$$

**ä¸€è‡´æ€§çº§åˆ«**ï¼š

1. **åŒæ­¥å¤åˆ¶**ï¼š$\text{å»¶è¿Ÿ} = \max(\text{æ‰€æœ‰ä»èŠ‚ç‚¹å»¶è¿Ÿ})$
2. **å¼‚æ­¥å¤åˆ¶**ï¼š$\text{å»¶è¿Ÿ} = \min(\text{ä»èŠ‚ç‚¹å»¶è¿Ÿ})$
3. **åŠåŒæ­¥å¤åˆ¶**ï¼š$\text{å»¶è¿Ÿ} = \text{ç¬¬kä¸ªä»èŠ‚ç‚¹å»¶è¿Ÿ}$

#### 2.2 åˆ†ç‰‡é›†ç¾¤æ¶æ„

**å®šä¹‰ 2.2 (æ•°æ®åˆ†ç‰‡)**
æ•°æ®åˆ†ç‰‡æ˜¯å°†æ•°æ®é›†æŒ‰ç…§æŸç§è§„åˆ™åˆ†å‰²åˆ°å¤šä¸ªèŠ‚ç‚¹ä¸Šçš„æŠ€æœ¯ï¼Œæ¯ä¸ªåˆ†ç‰‡åŒ…å«æ•°æ®çš„ä¸€ä¸ªå­é›†ã€‚

**åˆ†ç‰‡ç­–ç•¥**ï¼š

1. **èŒƒå›´åˆ†ç‰‡**ï¼š$S_i = \{d \in D | k_i \leq \text{hash}(d) < k_{i+1}\}$
2. **å“ˆå¸Œåˆ†ç‰‡**ï¼š$S_i = \{d \in D | \text{hash}(d) \bmod n = i\}$
3. **åˆ—è¡¨åˆ†ç‰‡**ï¼š$S_i = \{d \in D | \text{partition_key}(d) \in L_i\}$

**åˆ†ç‰‡é›†ç¾¤æ¶æ„**ï¼š

```mermaid
graph TD
    A[åè°ƒèŠ‚ç‚¹] --> B[åˆ†ç‰‡1]
    A --> C[åˆ†ç‰‡2]
    A --> D[åˆ†ç‰‡3]
    A --> E[åˆ†ç‰‡4]
    
    F[å®¢æˆ·ç«¯] --> A
    
    B --> G[å‰¯æœ¬1]
    B --> H[å‰¯æœ¬2]
    
    C --> I[å‰¯æœ¬1]
    C --> J[å‰¯æœ¬2]
    
    D --> K[å‰¯æœ¬1]
    D --> L[å‰¯æœ¬2]
    
    E --> M[å‰¯æœ¬1]
    E --> N[å‰¯æœ¬2]
```

#### 2.3 å¤šä¸»å¤åˆ¶æ¶æ„

**å®šä¹‰ 2.3 (å¤šä¸»å¤åˆ¶)**
å¤šä¸»å¤åˆ¶å…è®¸å¤šä¸ªèŠ‚ç‚¹åŒæ—¶å¤„ç†å†™æ“ä½œï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥ä½œä¸ºä¸»èŠ‚ç‚¹ã€‚

**å†²çªè§£å†³ç­–ç•¥**ï¼š

1. **æœ€åå†™å…¥èƒœåˆ©(LWW)**ï¼š$v_{final} = \arg\max_{v \in V} \text{timestamp}(v)$
2. **å‘é‡æ—¶é’Ÿ**ï¼š$VC_i[j] = \max(VC_i[j], VC_k[j]) + \delta_{i,k}$
3. **CRDT(æ— å†²çªå¤åˆ¶æ•°æ®ç±»å‹)**ï¼šåŸºäºæ•°å­¦æ€§è´¨ä¿è¯æ”¶æ•›

## ğŸ”§ å®ç°æ–¹æ¡ˆ

### 1. Citusåˆ†å¸ƒå¼æ‰©å±•

#### 1.1 Citusæ¶æ„è®¾è®¡

Citusæ˜¯PostgreSQLçš„åˆ†å¸ƒå¼æ‰©å±•ï¼Œæä¾›æ°´å¹³æ‰©å±•èƒ½åŠ›ï¼š

```sql
-- å®‰è£…Citusæ‰©å±•
CREATE EXTENSION citus;

-- é…ç½®åè°ƒèŠ‚ç‚¹
SELECT citus_set_coordinator_host('coordinator', 5432);

-- æ·»åŠ å·¥ä½œèŠ‚ç‚¹
SELECT citus_add_node('worker1', 5432);
SELECT citus_add_node('worker2', 5432);
SELECT citus_add_node('worker3', 5432);
```

#### 1.2 åˆ†å¸ƒå¼è¡¨è®¾è®¡

```sql
-- åˆ›å»ºåˆ†å¸ƒå¼è¡¨
CREATE TABLE distributed_users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- æŒ‰user_idåˆ†ç‰‡
SELECT create_distributed_table('distributed_users', 'user_id');

-- åˆ›å»ºåˆ†å¸ƒå¼è¡¨çš„åˆ†ç‰‡æ•°
SELECT create_distributed_table('distributed_users', 'user_id', 
                               shard_count := 32);

-- æŸ¥çœ‹åˆ†ç‰‡åˆ†å¸ƒ
SELECT * FROM citus_shards WHERE table_name = 'distributed_users';
```

#### 1.3 åˆ†å¸ƒå¼æŸ¥è¯¢ä¼˜åŒ–

```sql
-- åˆ†å¸ƒå¼JOINæŸ¥è¯¢
SELECT u.username, o.order_id, o.amount
FROM distributed_users u
JOIN distributed_orders o ON u.user_id = o.user_id
WHERE u.created_at >= '2024-01-01';

-- åˆ†å¸ƒå¼èšåˆæŸ¥è¯¢
SELECT 
    date_trunc('day', created_at) as day,
    COUNT(*) as user_count,
    SUM(amount) as total_amount
FROM distributed_orders
WHERE created_at >= '2024-01-01'
GROUP BY day
ORDER BY day;

-- åˆ†å¸ƒå¼çª—å£å‡½æ•°
SELECT 
    user_id,
    order_id,
    amount,
    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY created_at) as order_rank
FROM distributed_orders
WHERE created_at >= '2024-01-01';
```

#### 1.4 Citusé…ç½®ä¼˜åŒ–

```yaml
# citus.conf é…ç½®
citus:
  # åˆ†ç‰‡é…ç½®
  shard_count: 32
  replication_factor: 2
  
  # æŸ¥è¯¢ä¼˜åŒ–
  enable_repartition_joins: true
  enable_cross_shard_parallel_queries: true
  
  # è¿æ¥æ± é…ç½®
  max_connections: 100
  shared_preload_libraries: 'citus'
  
  # ç›‘æ§é…ç½®
  enable_statistics_collection: true
  log_statement: 'all'
```

### 2. Greenplumæ•°æ®ä»“åº“

#### 2.1 Greenplumæ¶æ„

Greenplumæ˜¯åŸºäºPostgreSQLçš„MPP(å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†)æ•°æ®ä»“åº“ï¼š

```sql
-- Greenplumé›†ç¾¤é…ç½®
-- ä¸»èŠ‚ç‚¹é…ç½®
gpconfig -c max_connections -v 200
gpconfig -c shared_buffers -v 256MB
gpconfig -c work_mem -v 64MB

-- æ®µèŠ‚ç‚¹é…ç½®
gpconfig -c max_connections -v 1000 -m 200
gpconfig -c shared_buffers -v 1GB -m 256MB
gpconfig -c work_mem -v 256MB -m 64MB
```

#### 2.2 æ•°æ®åˆ†å¸ƒç­–ç•¥

```sql
-- åˆ›å»ºåˆ†å¸ƒè¡¨
CREATE TABLE sales_data (
    sale_id SERIAL,
    product_id INTEGER,
    customer_id INTEGER,
    sale_amount DECIMAL(10,2),
    sale_date DATE
) DISTRIBUTED BY (customer_id);

-- éšæœºåˆ†å¸ƒ
CREATE TABLE temp_data (
    id SERIAL,
    data TEXT
) DISTRIBUTED RANDOMLY;

-- å“ˆå¸Œåˆ†å¸ƒ
CREATE TABLE user_sessions (
    session_id VARCHAR(50),
    user_id INTEGER,
    start_time TIMESTAMP,
    end_time TIMESTAMP
) DISTRIBUTED BY (user_id);

-- æŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
SELECT gp_segment_id, COUNT(*) 
FROM sales_data 
GROUP BY gp_segment_id 
ORDER BY gp_segment_id;
```

#### 2.3 å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–

```sql
-- å¹¶è¡Œæ‰«æä¼˜åŒ–
SET enable_seqscan = off;
SET enable_indexscan = on;
SET enable_bitmapscan = on;

-- å¹¶è¡ŒJOINä¼˜åŒ–
SET enable_hashjoin = on;
SET enable_mergejoin = on;
SET enable_nestloop = off;

-- å¹¶è¡Œèšåˆä¼˜åŒ–
SET enable_hashagg = on;
SET enable_groupagg = on;

-- æŸ¥çœ‹æŸ¥è¯¢è®¡åˆ’
EXPLAIN (ANALYZE, VERBOSE, BUFFERS)
SELECT 
    customer_id,
    SUM(sale_amount) as total_sales,
    COUNT(*) as transaction_count
FROM sales_data
WHERE sale_date >= '2024-01-01'
GROUP BY customer_id
HAVING SUM(sale_amount) > 1000;
```

### 3. PostgreSQL-XLé›†ç¾¤

#### 3.1 PostgreSQL-XLæ¶æ„

PostgreSQL-XLæ˜¯ä¸€ä¸ªå¤šä¸»PostgreSQLé›†ç¾¤è§£å†³æ–¹æ¡ˆï¼š

```sql
-- é…ç½®GTM (Global Transaction Manager)
-- gtm.conf
nodename = 'gtm'
listen_addresses = '*'
port = 6666
log_file = 'gtm.log'

-- é…ç½®åè°ƒèŠ‚ç‚¹
-- postgresql.conf
listen_addresses = '*'
port = 5432
gtm_host = 'gtm'
gtm_port = 6666
max_connections = 200

-- é…ç½®æ•°æ®èŠ‚ç‚¹
-- postgresql.conf
listen_addresses = '*'
port = 5433
gtm_host = 'gtm'
gtm_port = 6666
max_connections = 1000
```

#### 3.2 åˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†

```sql
-- åˆ›å»ºåˆ†å¸ƒå¼è¡¨
CREATE TABLE distributed_orders (
    order_id SERIAL,
    customer_id INTEGER,
    product_id INTEGER,
    quantity INTEGER,
    price DECIMAL(10,2),
    order_date TIMESTAMP DEFAULT NOW()
) DISTRIBUTE BY HASH(customer_id);

-- åˆ†å¸ƒå¼äº‹åŠ¡ç¤ºä¾‹
BEGIN;
INSERT INTO distributed_orders (customer_id, product_id, quantity, price)
VALUES (1001, 2001, 2, 99.99);
UPDATE customer_balance SET balance = balance - 199.98 WHERE customer_id = 1001;
COMMIT;

-- æŸ¥çœ‹åˆ†å¸ƒå¼äº‹åŠ¡çŠ¶æ€
SELECT * FROM pg_stat_activity WHERE application_name = 'coordinator';
```

## ğŸ­ è¡Œä¸šåº”ç”¨æ¡ˆä¾‹

### 1. äº’è”ç½‘è¡Œä¸šï¼šCitusåˆ†ç‰‡é›†ç¾¤

#### 1.1 ç”µå•†å¹³å°æ¶æ„

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[è´Ÿè½½å‡è¡¡å™¨]
    B --> C[APIç½‘å…³]
    C --> D[Citusåè°ƒèŠ‚ç‚¹]
    D --> E[ç”¨æˆ·åˆ†ç‰‡]
    D --> F[è®¢å•åˆ†ç‰‡]
    D --> G[å•†å“åˆ†ç‰‡]
    D --> H[æ”¯ä»˜åˆ†ç‰‡]
    
    E --> I[ç”¨æˆ·å‰¯æœ¬]
    F --> J[è®¢å•å‰¯æœ¬]
    G --> K[å•†å“å‰¯æœ¬]
    H --> L[æ”¯ä»˜å‰¯æœ¬]
```

#### 1.2 åˆ†ç‰‡ç­–ç•¥è®¾è®¡

```python
# Pythonåˆ†ç‰‡ç­–ç•¥å®ç°
import hashlib
from typing import List, Dict, Any
import psycopg2

class ShardingStrategy:
    def __init__(self, shard_count: int):
        self.shard_count = shard_count
        self.shard_connections = {}
        self.setup_connections()
    
    def setup_connections(self):
        """å»ºç«‹åˆ°å„ä¸ªåˆ†ç‰‡çš„è¿æ¥"""
        for i in range(self.shard_count):
            self.shard_connections[i] = psycopg2.connect(
                host=f'shard-{i}',
                database='ecommerce',
                user='postgres',
                password='password'
            )
    
    def get_shard_id(self, key: str) -> int:
        """æ ¹æ®é”®å€¼è®¡ç®—åˆ†ç‰‡ID"""
        hash_value = hashlib.md5(key.encode()).hexdigest()
        return int(hash_value, 16) % self.shard_count
    
    def get_connection(self, key: str):
        """è·å–å¯¹åº”åˆ†ç‰‡çš„è¿æ¥"""
        shard_id = self.get_shard_id(key)
        return self.shard_connections[shard_id]
    
    def execute_query(self, key: str, query: str, params: tuple = None):
        """åœ¨æŒ‡å®šåˆ†ç‰‡æ‰§è¡ŒæŸ¥è¯¢"""
        conn = self.get_connection(key)
        cursor = conn.cursor()
        
        if params:
            cursor.execute(query, params)
        else:
            cursor.execute(query)
        
        return cursor.fetchall()
    
    def execute_distributed_query(self, query: str, params: tuple = None):
        """æ‰§è¡Œåˆ†å¸ƒå¼æŸ¥è¯¢"""
        results = []
        for shard_id, conn in self.shard_connections.items():
            cursor = conn.cursor()
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            results.extend(cursor.fetchall())
        return results

# ä½¿ç”¨ç¤ºä¾‹
class EcommerceSharding:
    def __init__(self):
        self.sharding = ShardingStrategy(shard_count=32)
    
    def create_user(self, user_id: str, username: str, email: str):
        """åˆ›å»ºç”¨æˆ·"""
        query = """
            INSERT INTO users (user_id, username, email, created_at)
            VALUES (%s, %s, %s, NOW())
        """
        return self.sharding.execute_query(user_id, query, (user_id, username, email))
    
    def create_order(self, user_id: str, order_id: str, amount: float):
        """åˆ›å»ºè®¢å•"""
        query = """
            INSERT INTO orders (order_id, user_id, amount, created_at)
            VALUES (%s, %s, %s, NOW())
        """
        return self.sharding.execute_query(user_id, query, (order_id, user_id, amount))
    
    def get_user_orders(self, user_id: str):
        """è·å–ç”¨æˆ·è®¢å•"""
        query = """
            SELECT o.order_id, o.amount, o.created_at
            FROM orders o
            WHERE o.user_id = %s
            ORDER BY o.created_at DESC
        """
        return self.sharding.execute_query(user_id, query, (user_id,))
    
    def get_all_orders(self):
        """è·å–æ‰€æœ‰è®¢å•ï¼ˆåˆ†å¸ƒå¼æŸ¥è¯¢ï¼‰"""
        query = """
            SELECT user_id, COUNT(*) as order_count, SUM(amount) as total_amount
            FROM orders
            GROUP BY user_id
        """
        return self.sharding.execute_distributed_query(query)

# æµ‹è¯•åˆ†ç‰‡ç³»ç»Ÿ
def test_sharding_system():
    ecommerce = EcommerceSharding()
    
    # åˆ›å»ºç”¨æˆ·
    ecommerce.create_user("user_001", "alice", "alice@example.com")
    ecommerce.create_user("user_002", "bob", "bob@example.com")
    
    # åˆ›å»ºè®¢å•
    ecommerce.create_order("user_001", "order_001", 99.99)
    ecommerce.create_order("user_001", "order_002", 149.99)
    ecommerce.create_order("user_002", "order_003", 199.99)
    
    # æŸ¥è¯¢ç”¨æˆ·è®¢å•
    user_orders = ecommerce.get_user_orders("user_001")
    print(f"User orders: {user_orders}")
    
    # æŸ¥è¯¢æ‰€æœ‰è®¢å•ç»Ÿè®¡
    all_orders = ecommerce.get_all_orders()
    print(f"All orders stats: {all_orders}")
```

#### 1.3 æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

```sql
-- åˆ†ç‰‡æ€§èƒ½ç›‘æ§
SELECT 
    shard_id,
    table_name,
    shard_size,
    shard_count,
    avg_query_time,
    max_query_time
FROM citus_shard_statistics
ORDER BY avg_query_time DESC;

-- åˆ†ç‰‡è´Ÿè½½å‡è¡¡
SELECT 
    nodename,
    COUNT(*) as shard_count,
    SUM(shard_size) as total_size
FROM citus_shards
GROUP BY nodename
ORDER BY total_size DESC;

-- æŸ¥è¯¢æ€§èƒ½åˆ†æ
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements
WHERE query LIKE '%distributed%'
ORDER BY total_time DESC
LIMIT 10;
```

### 2. é‡‘èè¡Œä¸šï¼šGreenplumæ•°æ®ä»“åº“

#### 2.1 é‡‘èæ•°æ®ä»“åº“æ¶æ„

```mermaid
graph TD
    A[äº¤æ˜“æ•°æ®] --> B[ETLå¤„ç†]
    B --> C[Greenplumä¸»èŠ‚ç‚¹]
    C --> D[æ®µèŠ‚ç‚¹1]
    C --> E[æ®µèŠ‚ç‚¹2]
    C --> F[æ®µèŠ‚ç‚¹3]
    C --> G[æ®µèŠ‚ç‚¹4]
    
    H[é£é™©åˆ†æ] --> C
    I[åˆè§„æŠ¥å‘Š] --> C
    J[ä¸šåŠ¡åˆ†æ] --> C
    
    D --> K[æ•°æ®å¤‡ä»½]
    E --> L[æ•°æ®å¤‡ä»½]
    F --> M[æ•°æ®å¤‡ä»½]
    G --> N[æ•°æ®å¤‡ä»½]
```

#### 2.2 é‡‘èæ•°æ®åˆ†æ

```sql
-- åˆ›å»ºé‡‘èæ•°æ®è¡¨
CREATE TABLE financial_transactions (
    transaction_id SERIAL,
    account_id INTEGER,
    transaction_type VARCHAR(20),
    amount DECIMAL(15,2),
    currency VARCHAR(3),
    transaction_date TIMESTAMP,
    counterparty_id INTEGER,
    risk_score FLOAT
) DISTRIBUTED BY (account_id);

-- é£é™©åˆ†ææŸ¥è¯¢
WITH risk_analysis AS (
    SELECT 
        account_id,
        COUNT(*) as transaction_count,
        SUM(amount) as total_amount,
        AVG(risk_score) as avg_risk_score,
        MAX(amount) as max_amount,
        STDDEV(amount) as amount_volatility
    FROM financial_transactions
    WHERE transaction_date >= CURRENT_DATE - INTERVAL '30 days'
    GROUP BY account_id
),
risk_categories AS (
    SELECT 
        account_id,
        transaction_count,
        total_amount,
        avg_risk_score,
        CASE 
            WHEN avg_risk_score > 0.8 THEN 'HIGH'
            WHEN avg_risk_score > 0.5 THEN 'MEDIUM'
            ELSE 'LOW'
        END as risk_category
    FROM risk_analysis
)
SELECT 
    risk_category,
    COUNT(*) as account_count,
    AVG(total_amount) as avg_amount,
    SUM(total_amount) as total_volume
FROM risk_categories
GROUP BY risk_category
ORDER BY risk_category;

-- å®æ—¶é£é™©ç›‘æ§
CREATE OR REPLACE VIEW real_time_risk_monitor AS
SELECT 
    account_id,
    COUNT(*) as recent_transactions,
    SUM(amount) as recent_amount,
    AVG(risk_score) as recent_risk_score,
    MAX(transaction_date) as last_transaction
FROM financial_transactions
WHERE transaction_date >= NOW() - INTERVAL '1 hour'
GROUP BY account_id
HAVING AVG(risk_score) > 0.7 OR SUM(amount) > 100000;
```

#### 2.3 åˆè§„æŠ¥å‘Šç”Ÿæˆ

```python
# Pythonåˆè§„æŠ¥å‘Šç”Ÿæˆå™¨
import pandas as pd
import psycopg2
from datetime import datetime, timedelta
import plotly.graph_objects as go
import plotly.express as px

class ComplianceReportGenerator:
    def __init__(self, db_config):
        self.db_config = db_config
        self.connection = psycopg2.connect(**db_config)
    
    def generate_suspicious_activity_report(self, start_date: datetime, end_date: datetime):
        """ç”Ÿæˆå¯ç–‘æ´»åŠ¨æŠ¥å‘Š"""
        query = """
            SELECT 
                account_id,
                COUNT(*) as transaction_count,
                SUM(amount) as total_amount,
                AVG(risk_score) as avg_risk_score,
                MIN(transaction_date) as first_transaction,
                MAX(transaction_date) as last_transaction
            FROM financial_transactions
            WHERE transaction_date BETWEEN %s AND %s
            AND risk_score > 0.8
            GROUP BY account_id
            ORDER BY total_amount DESC
        """
        
        df = pd.read_sql(query, self.connection, params=(start_date, end_date))
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'period': f"{start_date.date()} to {end_date.date()}",
            'total_suspicious_accounts': len(df),
            'total_suspicious_amount': df['total_amount'].sum(),
            'accounts': df.to_dict('records')
        }
        
        return report
    
    def generate_volume_analysis_report(self, start_date: datetime, end_date: datetime):
        """ç”Ÿæˆäº¤æ˜“é‡åˆ†ææŠ¥å‘Š"""
        query = """
            SELECT 
                DATE(transaction_date) as transaction_date,
                COUNT(*) as transaction_count,
                SUM(amount) as total_amount,
                AVG(amount) as avg_amount
            FROM financial_transactions
            WHERE transaction_date BETWEEN %s AND %s
            GROUP BY DATE(transaction_date)
            ORDER BY transaction_date
        """
        
        df = pd.read_sql(query, self.connection, params=(start_date, end_date))
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=df['transaction_date'],
            y=df['total_amount'],
            mode='lines+markers',
            name='äº¤æ˜“é‡‘é¢'
        ))
        
        fig.add_trace(go.Bar(
            x=df['transaction_date'],
            y=df['transaction_count'],
            name='äº¤æ˜“æ•°é‡',
            yaxis='y2'
        ))
        
        fig.update_layout(
            title='äº¤æ˜“é‡åˆ†ææŠ¥å‘Š',
            xaxis_title='æ—¥æœŸ',
            yaxis_title='äº¤æ˜“é‡‘é¢',
            yaxis2=dict(
                title='äº¤æ˜“æ•°é‡',
                overlaying='y',
                side='right'
            )
        )
        
        return fig
    
    def generate_risk_distribution_report(self):
        """ç”Ÿæˆé£é™©åˆ†å¸ƒæŠ¥å‘Š"""
        query = """
            SELECT 
                CASE 
                    WHEN risk_score >= 0.8 THEN 'é«˜é£é™©'
                    WHEN risk_score >= 0.5 THEN 'ä¸­é£é™©'
                    ELSE 'ä½é£é™©'
                END as risk_level,
                COUNT(*) as transaction_count,
                SUM(amount) as total_amount
            FROM financial_transactions
            WHERE transaction_date >= CURRENT_DATE - INTERVAL '30 days'
            GROUP BY risk_level
            ORDER BY risk_level
        """
        
        df = pd.read_sql(query, self.connection)
        
        # åˆ›å»ºé¥¼å›¾
        fig = px.pie(
            df, 
            values='total_amount', 
            names='risk_level',
            title='é£é™©åˆ†å¸ƒåˆ†æ'
        )
        
        return fig
    
    def export_compliance_report(self, output_path: str):
        """å¯¼å‡ºåˆè§„æŠ¥å‘Š"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        # ç”Ÿæˆå„ç§æŠ¥å‘Š
        suspicious_report = self.generate_suspicious_activity_report(start_date, end_date)
        volume_fig = self.generate_volume_analysis_report(start_date, end_date)
        risk_fig = self.generate_risk_distribution_report()
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f"{output_path}/suspicious_activity_report.json", 'w') as f:
            json.dump(suspicious_report, f, indent=2, default=str)
        
        volume_fig.write_html(f"{output_path}/volume_analysis.html")
        risk_fig.write_html(f"{output_path}/risk_distribution.html")
        
        print(f"åˆè§„æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db_config = {
        'host': 'greenplum-master',
        'database': 'financial_dw',
        'user': 'postgres',
        'password': 'password'
    }
    
    generator = ComplianceReportGenerator(db_config)
    generator.export_compliance_report('./reports')
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### 1. åˆ†å¸ƒå¼æŸ¥è¯¢ä¼˜åŒ–

#### 1.1 æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–

```sql
-- å¯ç”¨æŸ¥è¯¢è®¡åˆ’ç¼“å­˜
SET plan_cache_mode = 'auto';

-- è®¾ç½®å¹¶è¡Œåº¦
SET max_parallel_workers_per_gather = 4;
SET max_parallel_workers = 16;

-- ä¼˜åŒ–å™¨è®¾ç½®
SET enable_hashjoin = on;
SET enable_mergejoin = on;
SET enable_nestloop = off;

-- å†…å­˜è®¾ç½®
SET work_mem = '256MB';
SET maintenance_work_mem = '1GB';
SET shared_buffers = '4GB';
```

#### 1.2 ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

```sql
-- åˆ›å»ºåˆ†å¸ƒå¼ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_transactions_user_date 
ON distributed_transactions(user_id, transaction_date);

-- åˆ›å»ºéƒ¨åˆ†ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_high_value_transactions 
ON distributed_transactions(amount, transaction_date) 
WHERE amount > 10000;

-- åˆ›å»ºè¦†ç›–ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_transactions_covering 
ON distributed_transactions(user_id, transaction_date, amount, status) 
INCLUDE (transaction_id);

-- åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

### 2. ç›‘æ§ä¸è¯Šæ–­

#### 2.1 æ€§èƒ½ç›‘æ§è§†å›¾

```sql
-- åˆ†å¸ƒå¼æŸ¥è¯¢æ€§èƒ½ç›‘æ§
CREATE OR REPLACE VIEW distributed_performance AS
WITH query_stats AS (
    SELECT 
        query,
        calls,
        total_time,
        mean_time,
        rows,
        shared_blks_hit,
        shared_blks_read
    FROM pg_stat_statements
    WHERE query LIKE '%distributed%'
),
node_stats AS (
    SELECT 
        nodename,
        COUNT(*) as shard_count,
        SUM(shard_size) as total_size
    FROM citus_shards
    GROUP BY nodename
)
SELECT 
    qs.*,
    ns.nodename,
    ns.shard_count,
    ns.total_size
FROM query_stats qs
CROSS JOIN node_stats ns
ORDER BY qs.total_time DESC;

-- åˆ†ç‰‡è´Ÿè½½ç›‘æ§
SELECT 
    nodename,
    COUNT(*) as shard_count,
    SUM(shard_size) as total_size,
    AVG(shard_size) as avg_size,
    MAX(shard_size) as max_size,
    MIN(shard_size) as min_size
FROM citus_shards
GROUP BY nodename
ORDER BY total_size DESC;
```

#### 2.2 è‡ªåŠ¨åŒ–ç›‘æ§è„šæœ¬

```python
# Pythonåˆ†å¸ƒå¼ç›‘æ§è„šæœ¬
import psycopg2
import time
import json
from datetime import datetime
from typing import Dict, List, Any

class DistributedMonitor:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.connection = psycopg2.connect(**db_config)
    
    def get_cluster_health(self) -> Dict[str, Any]:
        """è·å–é›†ç¾¤å¥åº·çŠ¶æ€"""
        cursor = self.connection.cursor()
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        cursor.execute("""
            SELECT nodename, nodeport, isactive, noderole
            FROM pg_dist_node
            ORDER BY nodename
        """)
        
        nodes = cursor.fetchall()
        
        # æ£€æŸ¥åˆ†ç‰‡åˆ†å¸ƒ
        cursor.execute("""
            SELECT nodename, COUNT(*) as shard_count, SUM(shard_size) as total_size
            FROM citus_shards
            GROUP BY nodename
            ORDER BY total_size DESC
        """)
        
        shards = cursor.fetchall()
        
        # æ£€æŸ¥æŸ¥è¯¢æ€§èƒ½
        cursor.execute("""
            SELECT 
                COUNT(*) as total_queries,
                AVG(total_time) as avg_query_time,
                MAX(total_time) as max_query_time
            FROM pg_stat_statements
            WHERE query LIKE '%distributed%'
        """)
        
        performance = cursor.fetchone()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'nodes': [{'name': n[0], 'port': n[1], 'active': n[2], 'role': n[3]} for n in nodes],
            'shards': [{'node': s[0], 'count': s[1], 'size': s[2]} for s in shards],
            'performance': {
                'total_queries': performance[0],
                'avg_query_time': performance[1],
                'max_query_time': performance[2]
            }
        }
    
    def check_replication_lag(self) -> Dict[str, Any]:
        """æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ"""
        cursor = self.connection.cursor()
        
        cursor.execute("""
            SELECT 
                application_name,
                client_addr,
                state,
                sent_lsn,
                write_lsn,
                flush_lsn,
                replay_lsn
            FROM pg_stat_replication
        """)
        
        replication_stats = cursor.fetchall()
        
        lag_info = []
        for stat in replication_stats:
            lag_info.append({
                'application': stat[0],
                'client': stat[1],
                'state': stat[2],
                'sent_lag': stat[3],
                'write_lag': stat[4],
                'flush_lag': stat[5],
                'replay_lag': stat[6]
            })
        
        return {
            'timestamp': datetime.now().isoformat(),
            'replication_lag': lag_info
        }
    
    def get_slow_queries(self, threshold_ms: int = 1000) -> List[Dict[str, Any]]:
        """è·å–æ…¢æŸ¥è¯¢"""
        cursor = self.connection.cursor()
        
        cursor.execute("""
            SELECT 
                query,
                calls,
                total_time,
                mean_time,
                rows
            FROM pg_stat_statements
            WHERE mean_time > %s
            ORDER BY mean_time DESC
            LIMIT 10
        """, (threshold_ms,))
        
        slow_queries = cursor.fetchall()
        
        return [
            {
                'query': q[0][:200] + '...' if len(q[0]) > 200 else q[0],
                'calls': q[1],
                'total_time': q[2],
                'mean_time': q[3],
                'rows': q[4]
            }
            for q in slow_queries
        ]
    
    def generate_health_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¥åº·æŠ¥å‘Š"""
        health = self.get_cluster_health()
        replication = self.check_replication_lag()
        slow_queries = self.get_slow_queries()
        
        # è®¡ç®—å¥åº·è¯„åˆ†
        active_nodes = sum(1 for node in health['nodes'] if node['active'])
        total_nodes = len(health['nodes'])
        health_score = (active_nodes / total_nodes) * 100 if total_nodes > 0 else 0
        
        return {
            'timestamp': datetime.now().isoformat(),
            'health_score': health_score,
            'cluster_health': health,
            'replication_status': replication,
            'slow_queries': slow_queries,
            'alerts': self.generate_alerts(health, replication, slow_queries)
        }
    
    def generate_alerts(self, health: Dict, replication: Dict, slow_queries: List) -> List[str]:
        """ç”Ÿæˆå‘Šè­¦"""
        alerts = []
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        inactive_nodes = [node for node in health['nodes'] if not node['active']]
        if inactive_nodes:
            alerts.append(f"å‘ç° {len(inactive_nodes)} ä¸ªéæ´»è·ƒèŠ‚ç‚¹")
        
        # æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ
        for rep in replication['replication_lag']:
            if rep['replay_lag'] and rep['replay_lag'] > 1000000:  # 1MBå»¶è¿Ÿ
                alerts.append(f"å¤åˆ¶å»¶è¿Ÿè¿‡é«˜: {rep['application']}")
        
        # æ£€æŸ¥æ…¢æŸ¥è¯¢
        if slow_queries:
            alerts.append(f"å‘ç° {len(slow_queries)} ä¸ªæ…¢æŸ¥è¯¢")
        
        return alerts
    
    def monitor_continuously(self, interval_seconds: int = 60):
        """æŒç»­ç›‘æ§"""
        print("å¼€å§‹åˆ†å¸ƒå¼é›†ç¾¤ç›‘æ§...")
        
        while True:
            try:
                report = self.generate_health_report()
                
                print(f"\n=== ç›‘æ§æŠ¥å‘Š {report['timestamp']} ===")
                print(f"å¥åº·è¯„åˆ†: {report['health_score']:.1f}%")
                print(f"æ´»è·ƒèŠ‚ç‚¹: {sum(1 for node in report['cluster_health']['nodes'] if node['active'])}/{len(report['cluster_health']['nodes'])}")
                
                if report['alerts']:
                    print("å‘Šè­¦:")
                    for alert in report['alerts']:
                        print(f"  - {alert}")
                
                time.sleep(interval_seconds)
                
            except KeyboardInterrupt:
                print("\nç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                time.sleep(interval_seconds)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db_config = {
        'host': 'citus-coordinator',
        'database': 'distributed_db',
        'user': 'postgres',
        'password': 'password'
    }
    
    monitor = DistributedMonitor(db_config)
    monitor.monitor_continuously()
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [1.1.5-åˆ†å¸ƒå¼ä¸é«˜å¯ç”¨](1.1.5-åˆ†å¸ƒå¼ä¸é«˜å¯ç”¨.md) - é«˜å¯ç”¨æ€§è®¾è®¡
- [1.1.14-å®æ—¶æµå¤„ç†ä¸CEP](1.1.14-å®æ—¶æµå¤„ç†ä¸CEP.md) - å®æ—¶æ•°æ®å¤„ç†
- [1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½²](1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½².md) - äº‘åŸç”Ÿéƒ¨ç½²
- [4.3.1-å¾®æœåŠ¡æ¶æ„åŸºç¡€ç†è®º](../../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.3-å¾®æœåŠ¡æ¶æ„/4.3.1-å¾®æœåŠ¡æ¶æ„åŸºç¡€ç†è®º.md) - å¾®æœåŠ¡æ¶æ„
- [3.5.1-æ•°æ®åˆ†æåŸºç¡€ç†è®º](../../../3-æ•°æ®æ¨¡å‹ä¸ç®—æ³•/3.5-æ•°æ®åˆ†æä¸ETL/3.5.1-æ•°æ®åˆ†æåŸºç¡€ç†è®º.md) - æ•°æ®åˆ†æç†è®º

[è¿”å›PostgreSQLå¯¼èˆª](README.md)
