---
title: 1.1.5-åˆ†å¸ƒå¼ä¸é«˜å¯ç”¨
slug: 1.1.5-åˆ†å¸ƒå¼ä¸é«˜å¯ç”¨
tags: []
pg_version: 16
status: draft
last_review: 2025-09-12
owner: TBD
---

# 1.1.5 åˆ†å¸ƒå¼ä¸é«˜å¯ç”¨

## ğŸ“‹ æ¦‚è¿°

åˆ†å¸ƒå¼ä¸é«˜å¯ç”¨æ˜¯PostgreSQLä¼ä¸šçº§åº”ç”¨çš„æ ¸å¿ƒç‰¹æ€§ï¼Œé€šè¿‡ä¸»ä»å¤åˆ¶ã€è¯»å†™åˆ†ç¦»ã€æ•…éšœè½¬ç§»ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œç¡®ä¿æ•°æ®åº“ç³»ç»Ÿçš„é«˜å¯ç”¨æ€§ã€æ•°æ®å®‰å…¨æ€§å’Œä¸šåŠ¡è¿ç»­æ€§ã€‚æœ¬æ–‡æ¡£ç³»ç»Ÿæ€§åœ°é˜è¿°PostgreSQLåˆ†å¸ƒå¼æ¶æ„çš„è®¾è®¡åŸç†ã€å®ç°æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ ç†è®ºåŸºç¡€

### 1. é«˜å¯ç”¨æ€§åŸºç¡€æ¦‚å¿µ

#### 1.1 å¯ç”¨æ€§å®šä¹‰ä¸è®¡ç®—

**å®šä¹‰ 1.1 (ç³»ç»Ÿå¯ç”¨æ€§)**
ç³»ç»Ÿå¯ç”¨æ€§æ˜¯æŒ‡åœ¨æŒ‡å®šæ—¶é—´æ®µå†…ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ­£å¸¸æä¾›æœåŠ¡çš„æ—¶é—´æ¯”ä¾‹ï¼š

$$Availability = \frac{MTBF}{MTBF + MTTR}$$

å…¶ä¸­ï¼š

- $MTBF$ (Mean Time Between Failures) æ˜¯å¹³å‡æ•…éšœé—´éš”æ—¶é—´
- $MTTR$ (Mean Time To Repair) æ˜¯å¹³å‡ä¿®å¤æ—¶é—´

**å¯ç”¨æ€§ç­‰çº§**ï¼š

| å¯ç”¨æ€§ç­‰çº§ | å¯ç”¨æ€§ç™¾åˆ†æ¯” | å¹´åœæœºæ—¶é—´ | é€‚ç”¨åœºæ™¯ |
|-----------|-------------|-----------|----------|
| 99% | 99% | 87.6å°æ—¶ | ä¸€èˆ¬ä¸šåŠ¡ |
| 99.9% | 99.9% | 8.76å°æ—¶ | é‡è¦ä¸šåŠ¡ |
| 99.99% | 99.99% | 52.56åˆ†é’Ÿ | å…³é”®ä¸šåŠ¡ |
| 99.999% | 99.999% | 5.26åˆ†é’Ÿ | æ ¸å¿ƒä¸šåŠ¡ |

#### 1.2 æ•…éšœæ¨¡å¼ä¸å½±å“åˆ†æ

**å®šä¹‰ 1.2 (æ•…éšœæ¨¡å¼)**
PostgreSQLåˆ†å¸ƒå¼ç³»ç»Ÿçš„æ•…éšœæ¨¡å¼åŒ…æ‹¬ï¼š

1. **èŠ‚ç‚¹æ•…éšœ**ï¼šå•ä¸ªæ•°æ®åº“èŠ‚ç‚¹ä¸å¯ç”¨
2. **ç½‘ç»œæ•…éšœ**ï¼šèŠ‚ç‚¹é—´ç½‘ç»œè¿æ¥ä¸­æ–­
3. **å­˜å‚¨æ•…éšœ**ï¼šç£ç›˜æˆ–å­˜å‚¨ç³»ç»Ÿæ•…éšœ
4. **è½¯ä»¶æ•…éšœ**ï¼šæ•°æ®åº“è½¯ä»¶æˆ–åº”ç”¨è½¯ä»¶æ•…éšœ

**æ•…éšœå½±å“åˆ†æ**ï¼š

```mermaid
graph TD
    A[æ•…éšœç±»å‹] --> B[èŠ‚ç‚¹æ•…éšœ]
    A --> C[ç½‘ç»œæ•…éšœ]
    A --> D[å­˜å‚¨æ•…éšœ]
    A --> E[è½¯ä»¶æ•…éšœ]
    
    B --> F[ä¸»ä»åˆ‡æ¢]
    B --> G[è´Ÿè½½é‡åˆ†å¸ƒ]
    
    C --> H[ç½‘ç»œéš”ç¦»]
    C --> I[åˆ†åŒºå¤„ç†]
    
    D --> J[æ•°æ®ä¸¢å¤±]
    D --> K[å¤‡ä»½æ¢å¤]
    
    E --> L[æœåŠ¡ä¸­æ–­]
    E --> M[ç‰ˆæœ¬å›æ»š]
```

### 2. å¤åˆ¶ç†è®ºä¸ä¸€è‡´æ€§æ¨¡å‹

#### 2.1 å¤åˆ¶ä¸€è‡´æ€§ç†è®º

**å®šä¹‰ 2.1 (å¤åˆ¶ä¸€è‡´æ€§)**
åœ¨åˆ†å¸ƒå¼PostgreSQLç³»ç»Ÿä¸­ï¼Œå¤åˆ¶ä¸€è‡´æ€§å®šä¹‰ä¸ºï¼š

1. **å¼ºä¸€è‡´æ€§**ï¼š$\forall i,j \in N, \forall t \in T, v_i(t) = v_j(t)$
2. **æœ€ç»ˆä¸€è‡´æ€§**ï¼š$\lim_{t \to \infty} v_i(t) = \lim_{t \to \infty} v_j(t)$
3. **ä¼šè¯ä¸€è‡´æ€§**ï¼š$\forall s \in S, \forall t_1, t_2 \in T, v_s(t_1) \leq v_s(t_2)$

å…¶ä¸­ï¼š

- $N$ æ˜¯èŠ‚ç‚¹é›†åˆ
- $T$ æ˜¯æ—¶é—´åŸŸ
- $v_i(t)$ æ˜¯èŠ‚ç‚¹ $i$ åœ¨æ—¶é—´ $t$ çš„æ•°æ®ç‰ˆæœ¬
- $S$ æ˜¯ä¼šè¯é›†åˆ

#### 2.2 å¤åˆ¶å»¶è¿Ÿåˆ†æ

**å¤åˆ¶å»¶è¿Ÿæ¨¡å‹**ï¼š

$$T_{replication} = T_{network} + T_{transfer} + T_{apply}$$

å…¶ä¸­ï¼š

- $T_{network}$ æ˜¯ç½‘ç»œä¼ è¾“å»¶è¿Ÿ
- $T_{transfer}$ æ˜¯WALæ—¥å¿—ä¼ è¾“æ—¶é—´
- $T_{apply}$ æ˜¯æ—¥å¿—åº”ç”¨æ—¶é—´

**å»¶è¿Ÿä¼˜åŒ–ç­–ç•¥**ï¼š

1. **ç½‘ç»œä¼˜åŒ–**ï¼š$T_{network} = \frac{data\_size}{bandwidth} + propagation\_delay$
2. **æ‰¹é‡ä¼ è¾“**ï¼š$T_{transfer} = \frac{wal\_size}{batch\_size} \times transfer\_time$
3. **å¹¶è¡Œåº”ç”¨**ï¼š$T_{apply} = \frac{wal\_records}{parallel\_workers} \times apply\_time$

### 3. åˆ†å¸ƒå¼äº‹åŠ¡ç†è®º

#### 3.1 åˆ†å¸ƒå¼äº‹åŠ¡æ¨¡å‹

**å®šä¹‰ 3.1 (åˆ†å¸ƒå¼äº‹åŠ¡)**
åˆ†å¸ƒå¼äº‹åŠ¡æ˜¯ä¸€ä¸ªäº”å…ƒç»„ $T = (O, S, C, A, T)$ï¼Œå…¶ä¸­ï¼š

- $O$ æ˜¯æ“ä½œé›†åˆ
- $S$ æ˜¯çŠ¶æ€é›†åˆ
- $C$ æ˜¯æäº¤æ¡ä»¶
- $A$ æ˜¯ä¸­æ­¢æ¡ä»¶
- $T$ æ˜¯æ—¶é—´çº¦æŸ

**äº‹åŠ¡ä¸€è‡´æ€§çº§åˆ«**ï¼š

1. **è¯»æœªæäº¤**ï¼š$RC_0$ - å…è®¸è„è¯»
2. **è¯»å·²æäº¤**ï¼š$RC_1$ - é˜²æ­¢è„è¯»
3. **å¯é‡å¤è¯»**ï¼š$RC_2$ - é˜²æ­¢ä¸å¯é‡å¤è¯»
4. **ä¸²è¡ŒåŒ–**ï¼š$RC_3$ - æœ€é«˜éš”ç¦»çº§åˆ«

#### 3.2 ä¸¤é˜¶æ®µæäº¤åè®®

**2PCåè®®çŠ¶æ€æœº**ï¼š

```rust
// Rustå®ç°çš„2PCåè®®
use std::collections::HashMap;
use tokio::sync::mpsc;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionState {
    Initial,
    Prepared,
    Committed,
    Aborted,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Message {
    Prepare { transaction_id: String },
    Prepared { transaction_id: String, participant_id: String },
    Commit { transaction_id: String },
    Abort { transaction_id: String },
}

pub struct TwoPhaseCommit {
    transaction_id: String,
    participants: Vec<String>,
    state: TransactionState,
    prepared_participants: HashMap<String, bool>,
}

impl TwoPhaseCommit {
    pub fn new(transaction_id: String, participants: Vec<String>) -> Self {
        Self {
            transaction_id,
            participants,
            state: TransactionState::Initial,
            prepared_participants: HashMap::new(),
        }
    }
    
    pub async fn execute(&mut self, tx: mpsc::Sender<Message>) -> Result<bool, String> {
        // ç¬¬ä¸€é˜¶æ®µï¼šå‡†å¤‡é˜¶æ®µ
        let prepare_result = self.prepare_phase(&tx).await?;
        
        if !prepare_result {
            self.abort_phase(&tx).await?;
            return Ok(false);
        }
        
        // ç¬¬äºŒé˜¶æ®µï¼šæäº¤é˜¶æ®µ
        let commit_result = self.commit_phase(&tx).await?;
        Ok(commit_result)
    }
    
    async fn prepare_phase(&mut self, tx: &mpsc::Sender<Message>) -> Result<bool, String> {
        // å‘é€å‡†å¤‡æ¶ˆæ¯ç»™æ‰€æœ‰å‚ä¸è€…
        for participant in &self.participants {
            let msg = Message::Prepare {
                transaction_id: self.transaction_id.clone(),
            };
            tx.send(msg).await.map_err(|e| e.to_string())?;
        }
        
        // ç­‰å¾…æ‰€æœ‰å‚ä¸è€…å“åº”
        let mut prepared_count = 0;
        for _ in 0..self.participants.len() {
            // è¿™é‡Œåº”è¯¥æ¥æ”¶å“åº”æ¶ˆæ¯
            prepared_count += 1;
        }
        
        Ok(prepared_count == self.participants.len())
    }
    
    async fn commit_phase(&mut self, tx: &mpsc::Sender<Message>) -> Result<bool, String> {
        // å‘é€æäº¤æ¶ˆæ¯ç»™æ‰€æœ‰å‚ä¸è€…
        for participant in &self.participants {
            let msg = Message::Commit {
                transaction_id: self.transaction_id.clone(),
            };
            tx.send(msg).await.map_err(|e| e.to_string())?;
        }
        
        self.state = TransactionState::Committed;
        Ok(true)
    }
    
    async fn abort_phase(&mut self, tx: &mpsc::Sender<Message>) -> Result<(), String> {
        // å‘é€ä¸­æ­¢æ¶ˆæ¯ç»™æ‰€æœ‰å‚ä¸è€…
        for participant in &self.participants {
            let msg = Message::Abort {
                transaction_id: self.transaction_id.clone(),
            };
            tx.send(msg).await.map_err(|e| e.to_string())?;
        }
        
        self.state = TransactionState::Aborted;
        Ok(())
    }
}
```

## ğŸ”§ ä¸»ä»å¤åˆ¶å®ç°

### 1. ç‰©ç†å¤åˆ¶æ¶æ„

#### 1.1 WALæ—¥å¿—æœºåˆ¶

**WALæ—¥å¿—ç»“æ„**ï¼š

```sql
-- æŸ¥çœ‹WALæ—¥å¿—é…ç½®
SHOW wal_level;
SHOW max_wal_senders;
SHOW wal_keep_segments;

-- æŸ¥çœ‹WALæ—¥å¿—çŠ¶æ€
SELECT * FROM pg_stat_wal_receiver;
SELECT * FROM pg_stat_replication;
```

**WALæ—¥å¿—é…ç½®ä¼˜åŒ–**ï¼š

```sql
-- ä¸»åº“WALé…ç½®
ALTER SYSTEM SET wal_level = replica;
ALTER SYSTEM SET max_wal_senders = 10;
ALTER SYSTEM SET wal_keep_segments = 64;
ALTER SYSTEM SET synchronous_commit = on;

-- ä»åº“WALé…ç½®
ALTER SYSTEM SET hot_standby = on;
ALTER SYSTEM SET primary_conninfo = 'host=master port=5432 user=repl password=repl';
ALTER SYSTEM SET recovery_target_timeline = 'latest';
```

#### 1.2 å¤åˆ¶é…ç½®å®ç°

**ä¸»åº“é…ç½®**ï¼š

```sql
-- åˆ›å»ºå¤åˆ¶ç”¨æˆ·
CREATE USER repl REPLICATION LOGIN PASSWORD 'repl_password';

-- é…ç½®pg_hba.conf
-- host replication repl 192.168.1.0/24 md5

-- é…ç½®postgresql.conf
wal_level = replica
max_wal_senders = 10
wal_keep_segments = 64
synchronous_commit = on
synchronous_standby_names = 'standby1,standby2'
```

**ä»åº“é…ç½®**ï¼š

```sql
-- é…ç½®postgresql.conf
hot_standby = on
primary_conninfo = 'host=master port=5432 user=repl password=repl_password'
recovery_target_timeline = 'latest'
max_standby_archive_delay = 30s
max_standby_streaming_delay = 30s
```

#### 1.3 å¤åˆ¶ç›‘æ§ä¸ç®¡ç†

```sql
-- å¤åˆ¶çŠ¶æ€ç›‘æ§
SELECT 
    pid,
    application_name,
    client_addr,
    state,
    sent_lsn,
    write_lsn,
    flush_lsn,
    replay_lsn,
    sync_priority,
    sync_state
FROM pg_stat_replication;

-- å¤åˆ¶å»¶è¿Ÿè®¡ç®—
SELECT 
    application_name,
    client_addr,
    (pg_current_wal_lsn() - sent_lsn) as sent_lag_bytes,
    (pg_current_wal_lsn() - write_lsn) as write_lag_bytes,
    (pg_current_wal_lsn() - flush_lsn) as flush_lag_bytes,
    (pg_current_wal_lsn() - replay_lsn) as replay_lag_bytes
FROM pg_stat_replication;

-- å¤åˆ¶è¿æ¥çŠ¶æ€
SELECT 
    application_name,
    state,
    COUNT(*) as connection_count
FROM pg_stat_replication
GROUP BY application_name, state;
```

### 2. é€»è¾‘å¤åˆ¶å®ç°

#### 2.1 é€»è¾‘å¤åˆ¶æ¶æ„

**é€»è¾‘å¤åˆ¶é…ç½®**ï¼š

```sql
-- å‘å¸ƒç«¯é…ç½®
CREATE PUBLICATION sales_pub FOR TABLE sales, customers;

-- è®¢é˜…ç«¯é…ç½®
CREATE SUBSCRIPTION sales_sub 
CONNECTION 'host=master port=5432 dbname=sales user=repl password=repl'
PUBLICATION sales_pub;

-- æŸ¥çœ‹å‘å¸ƒçŠ¶æ€
SELECT * FROM pg_publication;
SELECT * FROM pg_publication_tables;

-- æŸ¥çœ‹è®¢é˜…çŠ¶æ€
SELECT * FROM pg_subscription;
SELECT * FROM pg_stat_subscription;
```

#### 2.2 é€»è¾‘å¤åˆ¶ç›‘æ§

```sql
-- é€»è¾‘å¤åˆ¶å»¶è¿Ÿç›‘æ§
SELECT 
    subname,
    pid,
    received_lsn,
    latest_end_lsn,
    latest_end_time,
    (pg_current_wal_lsn() - received_lsn) as lag_bytes
FROM pg_stat_subscription;

-- é€»è¾‘å¤åˆ¶é”™è¯¯ç›‘æ§
SELECT 
    subname,
    pid,
    relname,
    received_lsn,
    latest_end_lsn,
    latest_end_time
FROM pg_stat_subscription_stats;
```

## ğŸ­ è¯»å†™åˆ†ç¦»æ¶æ„

### 1. è¯»å†™åˆ†ç¦»è®¾è®¡

#### 1.1 è´Ÿè½½å‡è¡¡ç­–ç•¥

**è¯»å†™åˆ†ç¦»æ¶æ„**ï¼š

```mermaid
graph TD
    A[åº”ç”¨å±‚] --> B[è¿æ¥æ± ]
    B --> C[ä¸»åº“]
    B --> D[ä»åº“1]
    B --> E[ä»åº“2]
    B --> F[ä»åº“3]
    
    G[å†™è¯·æ±‚] --> C
    H[è¯»è¯·æ±‚] --> D
    I[è¯»è¯·æ±‚] --> E
    J[è¯»è¯·æ±‚] --> F
```

**è´Ÿè½½å‡è¡¡ç®—æ³•**ï¼š

```python
# Pythonè´Ÿè½½å‡è¡¡å™¨å®ç°
import random
import time
from typing import List, Dict, Any
import psycopg2

class LoadBalancer:
    def __init__(self, master_config: Dict[str, Any], slave_configs: List[Dict[str, Any]]):
        self.master_config = master_config
        self.slave_configs = slave_configs
        self.current_slave_index = 0
        self.slave_weights = [1] * len(slave_configs)
        self.slave_health = [True] * len(slave_configs)
    
    def get_master_connection(self):
        """è·å–ä¸»åº“è¿æ¥"""
        return psycopg2.connect(**self.master_config)
    
    def get_slave_connection(self, strategy: str = 'round_robin'):
        """è·å–ä»åº“è¿æ¥"""
        if strategy == 'round_robin':
            return self._round_robin()
        elif strategy == 'random':
            return self._random()
        elif strategy == 'weighted':
            return self._weighted()
        else:
            return self._round_robin()
    
    def _round_robin(self):
        """è½®è¯¢ç­–ç•¥"""
        for i in range(len(self.slave_configs)):
            self.current_slave_index = (self.current_slave_index + 1) % len(self.slave_configs)
            if self.slave_health[self.current_slave_index]:
                return psycopg2.connect(**self.slave_configs[self.current_slave_index])
        raise Exception("No healthy slave available")
    
    def _random(self):
        """éšæœºç­–ç•¥"""
        healthy_slaves = [i for i, healthy in enumerate(self.slave_health) if healthy]
        if not healthy_slaves:
            raise Exception("No healthy slave available")
        
        slave_index = random.choice(healthy_slaves)
        return psycopg2.connect(**self.slave_configs[slave_index])
    
    def _weighted(self):
        """åŠ æƒç­–ç•¥"""
        total_weight = sum(self.slave_weights[i] for i, healthy in enumerate(self.slave_health) if healthy)
        if total_weight == 0:
            raise Exception("No healthy slave available")
        
        rand = random.uniform(0, total_weight)
        current_weight = 0
        
        for i, healthy in enumerate(self.slave_health):
            if healthy:
                current_weight += self.slave_weights[i]
                if rand <= current_weight:
                    return psycopg2.connect(**self.slave_configs[i])
        
        raise Exception("No healthy slave available")
    
    def check_slave_health(self):
        """æ£€æŸ¥ä»åº“å¥åº·çŠ¶æ€"""
        for i, config in enumerate(self.slave_configs):
            try:
                conn = psycopg2.connect(**config)
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.fetchone()
                conn.close()
                self.slave_health[i] = True
            except Exception as e:
                print(f"Slave {i} is unhealthy: {e}")
                self.slave_health[i] = False
    
    def update_slave_weight(self, slave_index: int, weight: int):
        """æ›´æ–°ä»åº“æƒé‡"""
        if 0 <= slave_index < len(self.slave_weights):
            self.slave_weights[slave_index] = weight

# ä½¿ç”¨ç¤ºä¾‹
class DatabaseManager:
    def __init__(self):
        master_config = {
            'host': 'master',
            'database': 'app_db',
            'user': 'app_user',
            'password': 'password'
        }
        
        slave_configs = [
            {'host': 'slave1', 'database': 'app_db', 'user': 'app_user', 'password': 'password'},
            {'host': 'slave2', 'database': 'app_db', 'user': 'app_user', 'password': 'password'},
            {'host': 'slave3', 'database': 'app_db', 'user': 'app_user', 'password': 'password'},
        ]
        
        self.load_balancer = LoadBalancer(master_config, slave_configs)
    
    def execute_write(self, query: str, params: tuple = None):
        """æ‰§è¡Œå†™æ“ä½œ"""
        conn = self.load_balancer.get_master_connection()
        cursor = conn.cursor()
        
        try:
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            conn.commit()
            return cursor.fetchall()
        finally:
            conn.close()
    
    def execute_read(self, query: str, params: tuple = None, strategy: str = 'round_robin'):
        """æ‰§è¡Œè¯»æ“ä½œ"""
        conn = self.load_balancer.get_slave_connection(strategy)
        cursor = conn.cursor()
        
        try:
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            return cursor.fetchall()
        finally:
            conn.close()
```

#### 1.2 è¿æ¥æ± é…ç½®

```sql
-- PgBounceré…ç½®ç¤ºä¾‹
[databases]
app_db = host=master port=5432 dbname=app_db

[pgbouncer]
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 20
max_db_connections = 100
max_user_connections = 100

# è¯»å†™åˆ†ç¦»é…ç½®
[databases]
app_db_master = host=master port=5432 dbname=app_db
app_db_slave1 = host=slave1 port=5432 dbname=app_db
app_db_slave2 = host=slave2 port=5432 dbname=app_db
```

### 2. æ•…éšœè½¬ç§»æœºåˆ¶

#### 2.1 è‡ªåŠ¨æ•…éšœè½¬ç§»

```python
# Pythonè‡ªåŠ¨æ•…éšœè½¬ç§»å®ç°
import psycopg2
import time
import threading
from typing import Dict, List, Any

class FailoverManager:
    def __init__(self, master_config: Dict[str, Any], slave_configs: List[Dict[str, Any]]):
        self.master_config = master_config
        self.slave_configs = slave_configs
        self.current_master = master_config
        self.available_slaves = slave_configs.copy()
        self.failover_lock = threading.Lock()
        self.monitoring = False
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        monitor_thread = threading.Thread(target=self._monitor_master)
        monitor_thread.daemon = True
        monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
    
    def _monitor_master(self):
        """ç›‘æ§ä¸»åº“çŠ¶æ€"""
        while self.monitoring:
            try:
                conn = psycopg2.connect(**self.current_master)
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.fetchone()
                conn.close()
                time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                print(f"Master is down: {e}")
                self._trigger_failover()
                break
    
    def _trigger_failover(self):
        """è§¦å‘æ•…éšœè½¬ç§»"""
        with self.failover_lock:
            print("Starting failover process...")
            
            # é€‰æ‹©æ–°çš„ä¸»åº“
            new_master = self._select_new_master()
            if new_master:
                self._promote_slave_to_master(new_master)
                print(f"Failover completed. New master: {new_master['host']}")
            else:
                print("No suitable slave for failover")
    
    def _select_new_master(self) -> Dict[str, Any]:
        """é€‰æ‹©æ–°çš„ä¸»åº“"""
        for slave in self.available_slaves:
            try:
                conn = psycopg2.connect(**slave)
                cursor = conn.cursor()
                
                # æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ
                cursor.execute("""
                    SELECT 
                        CASE 
                            WHEN pg_is_in_recovery() THEN 
                                (pg_current_wal_lsn() - received_lsn)
                            ELSE 0
                        END as lag_bytes
                    FROM pg_stat_wal_receiver
                """)
                
                result = cursor.fetchone()
                lag_bytes = result[0] if result else 0
                
                conn.close()
                
                # é€‰æ‹©å»¶è¿Ÿæœ€å°çš„ä»åº“
                if lag_bytes < 1024 * 1024:  # å°äº1MBå»¶è¿Ÿ
                    return slave
                    
            except Exception as e:
                print(f"Slave {slave['host']} is not suitable: {e}")
                continue
        
        return None
    
    def _promote_slave_to_master(self, new_master: Dict[str, Any]):
        """å°†ä»åº“æå‡ä¸ºä¸»åº“"""
        try:
            # åœæ­¢å¤åˆ¶
            conn = psycopg2.connect(**new_master)
            cursor = conn.cursor()
            cursor.execute("SELECT pg_promote_node()")
            conn.close()
            
            # æ›´æ–°å½“å‰ä¸»åº“
            self.current_master = new_master
            
            # ä»å¯ç”¨ä»åº“åˆ—è¡¨ä¸­ç§»é™¤
            self.available_slaves = [s for s in self.available_slaves if s != new_master]
            
        except Exception as e:
            print(f"Failed to promote slave: {e}")
    
    def get_current_master(self) -> Dict[str, Any]:
        """è·å–å½“å‰ä¸»åº“é…ç½®"""
        return self.current_master
    
    def get_available_slaves(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨ä»åº“åˆ—è¡¨"""
        return self.available_slaves.copy()

# ä½¿ç”¨ç¤ºä¾‹
def test_failover():
    master_config = {
        'host': 'master',
        'database': 'app_db',
        'user': 'app_user',
        'password': 'password'
    }
    
    slave_configs = [
        {'host': 'slave1', 'database': 'app_db', 'user': 'app_user', 'password': 'password'},
        {'host': 'slave2', 'database': 'app_db', 'user': 'app_user', 'password': 'password'},
    ]
    
    failover_manager = FailoverManager(master_config, slave_configs)
    failover_manager.start_monitoring()
    
    try:
        while True:
            current_master = failover_manager.get_current_master()
            print(f"Current master: {current_master['host']}")
            time.sleep(10)
    except KeyboardInterrupt:
        failover_manager.stop_monitoring()
        print("Monitoring stopped")
```

## ğŸ“Š ç›‘æ§ä¸ç»´æŠ¤

### 1. é«˜å¯ç”¨ç›‘æ§

#### 1.1 å¤åˆ¶çŠ¶æ€ç›‘æ§

```sql
-- åˆ›å»ºå¤åˆ¶ç›‘æ§è§†å›¾
CREATE OR REPLACE VIEW replication_monitor AS
WITH replication_stats AS (
    SELECT 
        application_name,
        client_addr,
        state,
        sent_lsn,
        write_lsn,
        flush_lsn,
        replay_lsn,
        sync_priority,
        sync_state,
        EXTRACT(EPOCH FROM (now() - backend_start)) as uptime_seconds
    FROM pg_stat_replication
),
lag_calculation AS (
    SELECT 
        *,
        (pg_current_wal_lsn() - sent_lsn) as sent_lag_bytes,
        (pg_current_wal_lsn() - write_lsn) as write_lag_bytes,
        (pg_current_wal_lsn() - flush_lsn) as flush_lag_bytes,
        (pg_current_wal_lsn() - replay_lsn) as replay_lag_bytes
    FROM replication_stats
)
SELECT 
    application_name,
    client_addr,
    state,
    sync_state,
    sent_lag_bytes,
    write_lag_bytes,
    flush_lag_bytes,
    replay_lag_bytes,
    uptime_seconds,
    CASE 
        WHEN replay_lag_bytes > 1024*1024*100 THEN 'HIGH'
        WHEN replay_lag_bytes > 1024*1024*10 THEN 'MEDIUM'
        ELSE 'LOW'
    END as lag_level
FROM lag_calculation
ORDER BY replay_lag_bytes DESC;

-- æŸ¥è¯¢å¤åˆ¶ç›‘æ§
SELECT * FROM replication_monitor;
```

#### 1.2 æ€§èƒ½ç›‘æ§

```sql
-- åˆ›å»ºæ€§èƒ½ç›‘æ§è§†å›¾
CREATE OR REPLACE VIEW performance_monitor AS
SELECT 
    -- è¿æ¥ç»Ÿè®¡
    (SELECT COUNT(*) FROM pg_stat_activity) as active_connections,
    (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active') as active_queries,
    
    -- äº‹åŠ¡ç»Ÿè®¡
    (SELECT xact_commit FROM pg_stat_database WHERE datname = current_database()) as commits,
    (SELECT xact_rollback FROM pg_stat_database WHERE datname = current_database()) as rollbacks,
    
    -- ç¼“å­˜ç»Ÿè®¡
    (SELECT blks_hit FROM pg_stat_database WHERE datname = current_database()) as cache_hits,
    (SELECT blks_read FROM pg_stat_database WHERE datname = current_database()) as disk_reads,
    
    -- å¤åˆ¶ç»Ÿè®¡
    (SELECT COUNT(*) FROM pg_stat_replication) as replication_connections,
    (SELECT COUNT(*) FROM pg_stat_replication WHERE sync_state = 'sync') as sync_replicas,
    
    -- ç³»ç»Ÿèµ„æº
    (SELECT pg_database_size(current_database())) as database_size,
    (SELECT pg_size_pretty(pg_database_size(current_database()))) as database_size_pretty;

-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§
SELECT * FROM performance_monitor;
```

### 2. è‡ªåŠ¨åŒ–ç»´æŠ¤

#### 2.1 è‡ªåŠ¨å¤‡ä»½ç­–ç•¥

```python
# Pythonè‡ªåŠ¨å¤‡ä»½è„šæœ¬
import psycopg2
import subprocess
import os
import time
from datetime import datetime
from typing import Dict, Any

class BackupManager:
    def __init__(self, db_config: Dict[str, Any], backup_dir: str):
        self.db_config = db_config
        self.backup_dir = backup_dir
        self.retention_days = 7
    
    def create_full_backup(self) -> str:
        """åˆ›å»ºå…¨é‡å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"{self.backup_dir}/full_backup_{timestamp}.sql"
        
        # ä½¿ç”¨pg_dumpåˆ›å»ºå¤‡ä»½
        cmd = [
            'pg_dump',
            '-h', self.db_config['host'],
            '-p', str(self.db_config['port']),
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-f', backup_file,
            '--verbose'
        ]
        
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_config['password']
        
        try:
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"Full backup created: {backup_file}")
                return backup_file
            else:
                print(f"Backup failed: {result.stderr}")
                return None
        except Exception as e:
            print(f"Backup error: {e}")
            return None
    
    def create_incremental_backup(self) -> str:
        """åˆ›å»ºå¢é‡å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"{self.backup_dir}/incremental_backup_{timestamp}.sql"
        
        # ä½¿ç”¨pg_dumpåˆ›å»ºå¢é‡å¤‡ä»½
        cmd = [
            'pg_dump',
            '-h', self.db_config['host'],
            '-p', str(self.db_config['port']),
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-f', backup_file,
            '--verbose',
            '--data-only',
            '--exclude-table=*'
        ]
        
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_config['password']
        
        try:
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"Incremental backup created: {backup_file}")
                return backup_file
            else:
                print(f"Incremental backup failed: {result.stderr}")
                return None
        except Exception as e:
            print(f"Incremental backup error: {e}")
            return None
    
    def cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        current_time = time.time()
        
        for filename in os.listdir(self.backup_dir):
            filepath = os.path.join(self.backup_dir, filename)
            if os.path.isfile(filepath):
                file_age = current_time - os.path.getmtime(filepath)
                if file_age > (self.retention_days * 24 * 3600):
                    os.remove(filepath)
                    print(f"Removed old backup: {filename}")
    
    def restore_backup(self, backup_file: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        if not os.path.exists(backup_file):
            print(f"Backup file not found: {backup_file}")
            return False
        
        cmd = [
            'psql',
            '-h', self.db_config['host'],
            '-p', str(self.db_config['port']),
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-f', backup_file
        ]
        
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_config['password']
        
        try:
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"Backup restored: {backup_file}")
                return True
            else:
                print(f"Restore failed: {result.stderr}")
                return False
        except Exception as e:
            print(f"Restore error: {e}")
            return False

# ä½¿ç”¨ç¤ºä¾‹
def schedule_backups():
    db_config = {
        'host': 'master',
        'port': 5432,
        'database': 'app_db',
        'user': 'backup_user',
        'password': 'backup_password'
    }
    
    backup_manager = BackupManager(db_config, '/backups')
    
    while True:
        try:
            # æ¯å¤©å‡Œæ™¨2ç‚¹åˆ›å»ºå…¨é‡å¤‡ä»½
            current_hour = datetime.now().hour
            if current_hour == 2:
                backup_manager.create_full_backup()
            
            # æ¯å°æ—¶åˆ›å»ºå¢é‡å¤‡ä»½
            backup_manager.create_incremental_backup()
            
            # æ¸…ç†æ—§å¤‡ä»½
            backup_manager.cleanup_old_backups()
            
            # ç­‰å¾…1å°æ—¶
            time.sleep(3600)
            
        except Exception as e:
            print(f"Backup schedule error: {e}")
            time.sleep(3600)
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [1.1.9-åˆ†å¸ƒå¼PostgreSQLæ¶æ„è®¾è®¡](1.1.9-åˆ†å¸ƒå¼PostgreSQLæ¶æ„è®¾è®¡.md) - åˆ†å¸ƒå¼æ¶æ„è®¾è®¡
- [1.1.14-å®æ—¶æµå¤„ç†ä¸CEP](1.1.14-å®æ—¶æµå¤„ç†ä¸CEP.md) - å®æ—¶æ•°æ®å¤„ç†
- [1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½²](1.1.15-äº‘åŸç”Ÿä¸å®¹å™¨åŒ–éƒ¨ç½².md) - äº‘åŸç”Ÿéƒ¨ç½²
- [4.3.1-å¾®æœåŠ¡æ¶æ„åŸºç¡€ç†è®º](../../../4-è½¯ä»¶æ¶æ„ä¸å·¥ç¨‹/4.3-å¾®æœåŠ¡æ¶æ„/4.3.1-å¾®æœåŠ¡æ¶æ„åŸºç¡€ç†è®º.md) - å¾®æœåŠ¡æ¶æ„

[è¿”å›PostgreSQLå¯¼èˆª](README.md)
