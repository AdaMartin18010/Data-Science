# 高性能计算深化

## 1. 并行计算框架

### 多进程并行计算

```python
# 多进程并行计算框架
import multiprocessing as mp
from multiprocessing import Pool, Process, Queue, Manager
import numpy as np
import time
from functools import partial

class ParallelComputingFramework:
    def __init__(self, num_processes: int = None):
        self.num_processes = num_processes or mp.cpu_count()
        self.pool = None
        self.manager = Manager()
    
    def initialize_pool(self):
        """初始化进程池"""
        self.pool = Pool(processes=self.num_processes)
    
    def close_pool(self):
        """关闭进程池"""
        if self.pool:
            self.pool.close()
            self.pool.join()
    
    def parallel_map(self, func, data, chunksize=None):
        """并行映射函数"""
        if not self.pool:
            self.initialize_pool()
        
        return self.pool.map(func, data, chunksize=chunksize)
    
    def parallel_starmap(self, func, data, chunksize=None):
        """并行星映射函数"""
        if not self.pool:
            self.initialize_pool()
        
        return self.pool.starmap(func, data, chunksize=chunksize)
    
    def parallel_apply_async(self, func, data, callback=None):
        """异步并行执行"""
        if not self.pool:
            self.initialize_pool()
        
        results = []
        for item in data:
            result = self.pool.apply_async(func, args=(item,), callback=callback)
            results.append(result)
        
        return [result.get() for result in results]

# 使用示例
def matrix_multiply_worker(args):
    """矩阵乘法工作函数"""
    matrix_a, matrix_b = args
    return np.dot(matrix_a, matrix_b)

def monte_carlo_simulation(iterations):
    """蒙特卡洛模拟"""
    inside_circle = 0
    for _ in range(iterations):
        x = np.random.uniform(-1, 1)
        y = np.random.uniform(-1, 1)
        if x**2 + y**2 <= 1:
            inside_circle += 1
    return inside_circle

# 并行计算示例
def parallel_matrix_operations():
    """并行矩阵运算"""
    framework = ParallelComputingFramework(num_processes=4)
    
    # 创建测试矩阵
    matrices_a = [np.random.rand(100, 100) for _ in range(10)]
    matrices_b = [np.random.rand(100, 100) for _ in range(10)]
    
    # 并行矩阵乘法
    start_time = time.time()
    results = framework.parallel_map(matrix_multiply_worker, 
                                   list(zip(matrices_a, matrices_b)))
    end_time = time.time()
    
    print(f"并行矩阵乘法耗时: {end_time - start_time:.4f}秒")
    return results

def parallel_monte_carlo():
    """并行蒙特卡洛模拟"""
    framework = ParallelComputingFramework(num_processes=8)
    
    # 分配迭代次数
    total_iterations = 1000000
    iterations_per_process = total_iterations // 8
    
    # 并行执行
    start_time = time.time()
    results = framework.parallel_map(monte_carlo_simulation, 
                                   [iterations_per_process] * 8)
    end_time = time.time()
    
    # 计算π值
    total_inside = sum(results)
    pi_estimate = 4 * total_inside / total_iterations
    
    print(f"π估计值: {pi_estimate:.6f}")
    print(f"并行蒙特卡洛耗时: {end_time - start_time:.4f}秒")
    
    return pi_estimate
```

### 多线程并行计算

```python
# 多线程并行计算框架
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import time

class ThreadingComputingFramework:
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.executor = None
        self.results_queue = queue.Queue()
    
    def initialize_executor(self):
        """初始化线程池执行器"""
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
    
    def close_executor(self):
        """关闭执行器"""
        if self.executor:
            self.executor.shutdown(wait=True)
    
    def parallel_submit(self, func, *args, **kwargs):
        """提交任务到线程池"""
        if not self.executor:
            self.initialize_executor()
        
        return self.executor.submit(func, *args, **kwargs)
    
    def parallel_map(self, func, iterable):
        """并行映射函数"""
        if not self.executor:
            self.initialize_executor()
        
        return list(self.executor.map(func, iterable))
    
    def parallel_submit_with_callback(self, func, iterable, callback=None):
        """带回调的并行提交"""
        if not self.executor:
            self.initialize_executor()
        
        futures = []
        for item in iterable:
            future = self.executor.submit(func, item)
            if callback:
                future.add_done_callback(callback)
            futures.append(future)
        
        return futures

# 使用示例
def io_bound_task(filename):
    """IO密集型任务"""
    time.sleep(0.1)  # 模拟IO操作
    return f"Processed {filename}"

def parallel_io_operations():
    """并行IO操作"""
    framework = ThreadingComputingFramework(max_workers=10)
    
    filenames = [f"file_{i}.txt" for i in range(100)]
    
    start_time = time.time()
    results = framework.parallel_map(io_bound_task, filenames)
    end_time = time.time()
    
    print(f"并行IO操作耗时: {end_time - start_time:.4f}秒")
    return results
```

## 2. 分布式计算

### 分布式任务调度

```python
# 分布式任务调度系统
import redis
import json
import pickle
from typing import Any, Dict, List
import time

class DistributedTaskScheduler:
    def __init__(self, redis_host: str = 'localhost', redis_port: int = 6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port)
        self.task_queue = 'task_queue'
        self.result_queue = 'result_queue'
        self.worker_status = 'worker_status'
    
    def submit_task(self, task_id: str, task_func: str, task_args: tuple, 
                   task_kwargs: dict = None):
        """提交任务"""
        task_data = {
            'task_id': task_id,
            'func': task_func,
            'args': task_args,
            'kwargs': task_kwargs or {},
            'timestamp': time.time()
        }
        
        self.redis_client.lpush(self.task_queue, pickle.dumps(task_data))
        print(f"任务 {task_id} 已提交到队列")
    
    def get_task(self):
        """获取任务"""
        task_data = self.redis_client.brpop(self.task_queue, timeout=1)
        if task_data:
            return pickle.loads(task_data[1])
        return None
    
    def submit_result(self, task_id: str, result: Any):
        """提交结果"""
        result_data = {
            'task_id': task_id,
            'result': result,
            'timestamp': time.time()
        }
        
        self.redis_client.lpush(self.result_queue, pickle.dumps(result_data))
        print(f"任务 {task_id} 结果已提交")
    
    def get_result(self, task_id: str, timeout: int = 60):
        """获取结果"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            result_data = self.redis_client.brpop(self.result_queue, timeout=1)
            if result_data:
                data = pickle.loads(result_data[1])
                if data['task_id'] == task_id:
                    return data['result']
        
        return None
    
    def register_worker(self, worker_id: str, status: str = 'idle'):
        """注册工作节点"""
        worker_info = {
            'worker_id': worker_id,
            'status': status,
            'last_heartbeat': time.time()
        }
        
        self.redis_client.hset(self.worker_status, worker_id, 
                              json.dumps(worker_info))
    
    def update_worker_status(self, worker_id: str, status: str):
        """更新工作节点状态"""
        worker_info = self.redis_client.hget(self.worker_status, worker_id)
        if worker_info:
            info = json.loads(worker_info)
            info['status'] = status
            info['last_heartbeat'] = time.time()
            self.redis_client.hset(self.worker_status, worker_id, 
                                  json.dumps(info))

# 分布式工作节点
class DistributedWorker:
    def __init__(self, worker_id: str, scheduler: DistributedTaskScheduler):
        self.worker_id = worker_id
        self.scheduler = scheduler
        self.running = False
        
        # 注册可用的函数
        self.available_functions = {
            'matrix_multiply': self.matrix_multiply,
            'monte_carlo': self.monte_carlo_simulation,
            'data_processing': self.data_processing
        }
    
    def start(self):
        """启动工作节点"""
        self.running = True
        self.scheduler.register_worker(self.worker_id)
        
        print(f"工作节点 {self.worker_id} 已启动")
        
        while self.running:
            try:
                # 获取任务
                task = self.scheduler.get_task()
                if task:
                    self.scheduler.update_worker_status(self.worker_id, 'busy')
                    
                    # 执行任务
                    result = self.execute_task(task)
                    
                    # 提交结果
                    self.scheduler.submit_result(task['task_id'], result)
                    
                    self.scheduler.update_worker_status(self.worker_id, 'idle')
                else:
                    time.sleep(1)
            
            except Exception as e:
                print(f"工作节点 {self.worker_id} 错误: {e}")
                self.scheduler.update_worker_status(self.worker_id, 'error')
    
    def execute_task(self, task: Dict) -> Any:
        """执行任务"""
        func_name = task['func']
        args = task['args']
        kwargs = task['kwargs']
        
        if func_name in self.available_functions:
            return self.available_functions[func_name](*args, **kwargs)
        else:
            raise ValueError(f"未知函数: {func_name}")
    
    def matrix_multiply(self, matrix_a, matrix_b):
        """矩阵乘法"""
        return np.dot(matrix_a, matrix_b)
    
    def monte_carlo_simulation(self, iterations):
        """蒙特卡洛模拟"""
        inside_circle = 0
        for _ in range(iterations):
            x = np.random.uniform(-1, 1)
            y = np.random.uniform(-1, 1)
            if x**2 + y**2 <= 1:
                inside_circle += 1
        return inside_circle
    
    def data_processing(self, data):
        """数据处理"""
        return np.mean(data), np.std(data), len(data)
    
    def stop(self):
        """停止工作节点"""
        self.running = False
```

### 分布式数据处理

```python
# 分布式数据处理框架
import pandas as pd
from typing import List, Dict, Any
import numpy as np

class DistributedDataProcessor:
    def __init__(self, scheduler: DistributedTaskScheduler):
        self.scheduler = scheduler
    
    def parallel_data_processing(self, data_chunks: List[pd.DataFrame], 
                               operation: str, **kwargs):
        """并行数据处理"""
        task_ids = []
        
        # 提交任务
        for i, chunk in enumerate(data_chunks):
            task_id = f"data_processing_{i}"
            self.scheduler.submit_task(
                task_id=task_id,
                task_func='data_processing',
                task_args=(chunk.to_dict(), operation),
                task_kwargs=kwargs
            )
            task_ids.append(task_id)
        
        # 收集结果
        results = []
        for task_id in task_ids:
            result = self.scheduler.get_result(task_id)
            if result:
                results.append(result)
        
        return results
    
    def distributed_aggregation(self, data_chunks: List[pd.DataFrame], 
                              aggregation_func: str):
        """分布式聚合"""
        # 第一阶段：局部聚合
        local_results = self.parallel_data_processing(
            data_chunks, 'local_aggregation', func=aggregation_func
        )
        
        # 第二阶段：全局聚合
        global_result = self.scheduler.submit_task(
            task_id='global_aggregation',
            task_func='global_aggregation',
            task_args=(local_results, aggregation_func)
        )
        
        return self.scheduler.get_result('global_aggregation')
    
    def distributed_join(self, left_chunks: List[pd.DataFrame], 
                        right_chunks: List[pd.DataFrame], 
                        on_column: str):
        """分布式连接"""
        # 提交连接任务
        task_ids = []
        for i, (left_chunk, right_chunk) in enumerate(zip(left_chunks, right_chunks)):
            task_id = f"join_{i}"
            self.scheduler.submit_task(
                task_id=task_id,
                task_func='dataframe_join',
                task_args=(left_chunk.to_dict(), right_chunk.to_dict(), on_column)
            )
            task_ids.append(task_id)
        
        # 收集结果
        results = []
        for task_id in task_ids:
            result = self.scheduler.get_result(task_id)
            if result:
                results.append(pd.DataFrame(result))
        
        return pd.concat(results, ignore_index=True)

# 扩展工作节点功能
class DataProcessingWorker(DistributedWorker):
    def __init__(self, worker_id: str, scheduler: DistributedTaskScheduler):
        super().__init__(worker_id, scheduler)
        
        # 添加数据处理函数
        self.available_functions.update({
            'local_aggregation': self.local_aggregation,
            'global_aggregation': self.global_aggregation,
            'dataframe_join': self.dataframe_join
        })
    
    def local_aggregation(self, data_dict, func):
        """局部聚合"""
        df = pd.DataFrame(data_dict)
        
        if func == 'sum':
            return df.sum().to_dict()
        elif func == 'mean':
            return df.mean().to_dict()
        elif func == 'count':
            return df.count().to_dict()
        else:
            return df.agg(func).to_dict()
    
    def global_aggregation(self, local_results, func):
        """全局聚合"""
        # 合并局部结果
        combined_data = {}
        for result in local_results:
            for key, value in result.items():
                if key not in combined_data:
                    combined_data[key] = []
                combined_data[key].append(value)
        
        # 执行全局聚合
        global_result = {}
        for key, values in combined_data.items():
            if func == 'sum':
                global_result[key] = sum(values)
            elif func == 'mean':
                global_result[key] = np.mean(values)
            elif func == 'count':
                global_result[key] = sum(values)
        
        return global_result
    
    def dataframe_join(self, left_dict, right_dict, on_column):
        """DataFrame连接"""
        left_df = pd.DataFrame(left_dict)
        right_df = pd.DataFrame(right_dict)
        
        joined_df = pd.merge(left_df, right_df, on=on_column, how='inner')
        return joined_df.to_dict()
```

## 3. GPU加速计算

### CUDA并行计算

```python
# GPU加速计算框架
import numba
from numba import cuda, jit
import numpy as np
import time

class GPUComputingFramework:
    def __init__(self):
        self.device_count = cuda.detect()
        print(f"检测到 {self.device_count} 个CUDA设备")
    
    @cuda.jit
    def matrix_multiply_gpu(self, a, b, c):
        """GPU矩阵乘法"""
        row, col = cuda.grid(2)
        if row < c.shape[0] and col < c.shape[1]:
            tmp = 0.0
            for k in range(a.shape[1]):
                tmp += a[row, k] * b[k, col]
            c[row, col] = tmp
    
    def gpu_matrix_multiply(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """GPU矩阵乘法"""
        # 确保数据类型
        a = np.ascontiguousarray(a, dtype=np.float32)
        b = np.ascontiguousarray(b, dtype=np.float32)
        
        # 分配GPU内存
        a_gpu = cuda.to_device(a)
        b_gpu = cuda.to_device(b)
        c_gpu = cuda.device_array((a.shape[0], b.shape[1]), dtype=np.float32)
        
        # 设置线程块
        threadsperblock = (16, 16)
        blockspergrid_x = (a.shape[0] + threadsperblock[0] - 1) // threadsperblock[0]
        blockspergrid_y = (b.shape[1] + threadsperblock[1] - 1) // threadsperblock[1]
        blockspergrid = (blockspergrid_x, blockspergrid_y)
        
        # 启动内核
        self.matrix_multiply_gpu[blockspergrid, threadsperblock](a_gpu, b_gpu, c_gpu)
        
        # 复制结果回CPU
        return c_gpu.copy_to_host()
    
    @cuda.jit
    def vector_add_gpu(self, a, b, c):
        """GPU向量加法"""
        idx = cuda.grid(1)
        if idx < c.shape[0]:
            c[idx] = a[idx] + b[idx]
    
    def gpu_vector_add(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """GPU向量加法"""
        a = np.ascontiguousarray(a, dtype=np.float32)
        b = np.ascontiguousarray(b, dtype=np.float32)
        
        # 分配GPU内存
        a_gpu = cuda.to_device(a)
        b_gpu = cuda.to_device(b)
        c_gpu = cuda.device_array_like(a)
        
        # 设置线程块
        threadsperblock = 256
        blockspergrid = (a.size + threadsperblock - 1) // threadsperblock
        
        # 启动内核
        self.vector_add_gpu[blockspergrid, threadsperblock](a_gpu, b_gpu, c_gpu)
        
        return c_gpu.copy_to_host()
    
    @cuda.jit
    def monte_carlo_gpu(self, random_numbers, results):
        """GPU蒙特卡洛模拟"""
        idx = cuda.grid(1)
        if idx < results.shape[0]:
            x = random_numbers[idx * 2]
            y = random_numbers[idx * 2 + 1]
            if x * x + y * y <= 1.0:
                results[idx] = 1
            else:
                results[idx] = 0
    
    def gpu_monte_carlo(self, iterations: int) -> float:
        """GPU蒙特卡洛模拟"""
        # 生成随机数
        random_numbers = np.random.uniform(-1, 1, iterations * 2).astype(np.float32)
        results = np.zeros(iterations, dtype=np.int32)
        
        # 分配GPU内存
        random_gpu = cuda.to_device(random_numbers)
        results_gpu = cuda.to_device(results)
        
        # 设置线程块
        threadsperblock = 256
        blockspergrid = (iterations + threadsperblock - 1) // threadsperblock
        
        # 启动内核
        self.monte_carlo_gpu[blockspergrid, threadsperblock](random_gpu, results_gpu)
        
        # 计算结果
        results = results_gpu.copy_to_host()
        inside_circle = np.sum(results)
        
        return 4.0 * inside_circle / iterations

# 使用示例
def gpu_computing_example():
    """GPU计算示例"""
    framework = GPUComputingFramework()
    
    # GPU矩阵乘法
    size = 1000
    a = np.random.rand(size, size).astype(np.float32)
    b = np.random.rand(size, size).astype(np.float32)
    
    start_time = time.time()
    c_gpu = framework.gpu_matrix_multiply(a, b)
    gpu_time = time.time() - start_time
    
    start_time = time.time()
    c_cpu = np.dot(a, b)
    cpu_time = time.time() - start_time
    
    print(f"GPU矩阵乘法耗时: {gpu_time:.4f}秒")
    print(f"CPU矩阵乘法耗时: {cpu_time:.4f}秒")
    print(f"加速比: {cpu_time/gpu_time:.2f}x")
    
    # GPU蒙特卡洛模拟
    iterations = 10000000
    
    start_time = time.time()
    pi_gpu = framework.gpu_monte_carlo(iterations)
    gpu_time = time.time() - start_time
    
    print(f"GPU蒙特卡洛π估计: {pi_gpu:.6f}")
    print(f"GPU蒙特卡洛耗时: {gpu_time:.4f}秒")
```

### PyTorch GPU加速

```python
# PyTorch GPU加速框架
import torch
import torch.nn as nn
import torch.optim as optim
import time

class PyTorchGPUFramework:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"使用设备: {self.device}")
    
    def gpu_tensor_operations(self, size: int = 10000):
        """GPU张量运算"""
        # 创建张量
        a = torch.randn(size, size, device=self.device)
        b = torch.randn(size, size, device=self.device)
        
        # GPU矩阵乘法
        start_time = time.time()
        c = torch.mm(a, b)
        torch.cuda.synchronize()  # 确保GPU操作完成
        gpu_time = time.time() - start_time
        
        print(f"GPU张量运算耗时: {gpu_time:.4f}秒")
        return c
    
    def gpu_neural_network(self, input_size: int = 1000, hidden_size: int = 500, 
                          output_size: int = 100, batch_size: int = 64):
        """GPU神经网络训练"""
        # 创建模型
        model = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        ).to(self.device)
        
        # 创建数据
        x = torch.randn(batch_size, input_size, device=self.device)
        y = torch.randn(batch_size, output_size, device=self.device)
        
        # 定义损失函数和优化器
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        # 训练
        start_time = time.time()
        for epoch in range(100):
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
        
        torch.cuda.synchronize()
        training_time = time.time() - start_time
        
        print(f"GPU神经网络训练耗时: {training_time:.4f}秒")
        return model
    
    def gpu_batch_processing(self, data_size: int = 1000000, batch_size: int = 1000):
        """GPU批处理"""
        # 创建数据
        data = torch.randn(data_size, device=self.device)
        
        # 批处理
        start_time = time.time()
        results = []
        
        for i in range(0, data_size, batch_size):
            batch = data[i:i+batch_size]
            # 执行批处理操作
            batch_result = torch.sin(batch) + torch.cos(batch)
            results.append(batch_result)
        
        torch.cuda.synchronize()
        processing_time = time.time() - start_time
        
        print(f"GPU批处理耗时: {processing_time:.4f}秒")
        return torch.cat(results)
```

## 4. 云计算平台

### AWS并行计算

```python
# AWS并行计算框架
import boto3
import json
import time
from typing import List, Dict, Any

class AWSComputingFramework:
    def __init__(self, region_name: str = 'us-east-1'):
        self.ec2 = boto3.client('ec2', region_name=region_name)
        self.s3 = boto3.client('s3', region_name=region_name)
        self.lambda_client = boto3.client('lambda', region_name=region_name)
        self.batch = boto3.client('batch', region_name=region_name)
    
    def create_compute_cluster(self, instance_type: str = 't3.micro', 
                              count: int = 4):
        """创建计算集群"""
        # 启动EC2实例
        response = self.ec2.run_instances(
            ImageId='ami-0c55b159cbfafe1f0',  # Amazon Linux 2
            MinCount=count,
            MaxCount=count,
            InstanceType=instance_type,
            KeyName='your-key-pair',
            SecurityGroupIds=['sg-xxxxxxxxx'],
            TagSpecifications=[
                {
                    'ResourceType': 'instance',
                    'Tags': [
                        {
                            'Key': 'Name',
                            'Value': 'compute-cluster'
                        }
                    ]
                }
            ]
        )
        
        instance_ids = [instance['InstanceId'] for instance in response['Instances']]
        print(f"创建了 {len(instance_ids)} 个计算实例")
        
        return instance_ids
    
    def submit_batch_job(self, job_name: str, job_queue: str, 
                         job_definition: str, command: List[str]):
        """提交批处理作业"""
        response = self.batch.submit_job(
            jobName=job_name,
            jobQueue=job_queue,
            jobDefinition=job_definition,
            containerOverrides={
                'command': command
            }
        )
        
        job_id = response['jobId']
        print(f"提交批处理作业: {job_id}")
        
        return job_id
    
    def invoke_lambda_function(self, function_name: str, payload: Dict):
        """调用Lambda函数"""
        response = self.lambda_client.invoke(
            FunctionName=function_name,
            InvocationType='RequestResponse',
            Payload=json.dumps(payload)
        )
        
        result = json.loads(response['Payload'].read())
        return result
    
    def upload_to_s3(self, bucket_name: str, key: str, data: bytes):
        """上传数据到S3"""
        self.s3.put_object(
            Bucket=bucket_name,
            Key=key,
            Body=data
        )
        print(f"数据已上传到 s3://{bucket_name}/{key}")
    
    def download_from_s3(self, bucket_name: str, key: str) -> bytes:
        """从S3下载数据"""
        response = self.s3.get_object(
            Bucket=bucket_name,
            Key=key
        )
        return response['Body'].read()

# Lambda函数示例
def lambda_compute_function(event, context):
    """Lambda计算函数"""
    import numpy as np
    
    # 获取输入数据
    data = event.get('data', [])
    operation = event.get('operation', 'sum')
    
    # 执行计算
    if operation == 'sum':
        result = np.sum(data)
    elif operation == 'mean':
        result = np.mean(data)
    elif operation == 'std':
        result = np.std(data)
    else:
        result = None
    
    return {
        'result': result,
        'operation': operation,
        'input_size': len(data)
    }

# 使用示例
def aws_computing_example():
    """AWS计算示例"""
    framework = AWSComputingFramework()
    
    # 创建计算集群
    instance_ids = framework.create_compute_cluster(count=4)
    
    # 准备数据
    data = list(range(1000000))
    
    # 分块处理
    chunk_size = len(data) // 4
    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
    
    # 并行处理
    results = []
    for i, chunk in enumerate(chunks):
        payload = {
            'data': chunk,
            'operation': 'sum'
        }
        
        result = framework.invoke_lambda_function('compute-function', payload)
        results.append(result['result'])
    
    total_sum = sum(results)
    print(f"分布式计算结果: {total_sum}")
    
    return total_sum
```

## 5. 大数据处理

### Spark分布式计算

```python
# Spark分布式计算框架
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import time

class SparkComputingFramework:
    def __init__(self, app_name: str = "SparkComputing"):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        print(f"Spark版本: {self.spark.version}")
    
    def create_test_data(self, size: int = 1000000):
        """创建测试数据"""
        # 创建随机数据
        data = []
        for i in range(size):
            data.append({
                'id': i,
                'value': i % 100,
                'category': f'cat_{i % 10}',
                'timestamp': time.time() + i
            })
        
        return self.spark.createDataFrame(data)
    
    def spark_aggregation(self, df):
        """Spark聚合操作"""
        start_time = time.time()
        
        result = df.groupBy('category') \
            .agg(
                count('*').alias('count'),
                sum('value').alias('sum'),
                avg('value').alias('avg'),
                stddev('value').alias('std')
            )
        
        result.collect()
        processing_time = time.time() - start_time
        
        print(f"Spark聚合操作耗时: {processing_time:.4f}秒")
        return result
    
    def spark_join_operation(self, df1, df2):
        """Spark连接操作"""
        start_time = time.time()
        
        result = df1.join(df2, 'id', 'inner')
        result.collect()
        
        processing_time = time.time() - start_time
        print(f"Spark连接操作耗时: {processing_time:.4f}秒")
        return result
    
    def spark_window_functions(self, df):
        """Spark窗口函数"""
        from pyspark.sql.window import Window
        from pyspark.sql.functions import row_number, rank, dense_rank
        
        start_time = time.time()
        
        window_spec = Window.partitionBy('category').orderBy('value')
        
        result = df.withColumn('row_number', row_number().over(window_spec)) \
            .withColumn('rank', rank().over(window_spec)) \
            .withColumn('dense_rank', dense_rank().over(window_spec))
        
        result.collect()
        processing_time = time.time() - start_time
        
        print(f"Spark窗口函数耗时: {processing_time:.4f}秒")
        return result
    
    def spark_machine_learning(self, df):
        """Spark机器学习"""
        from pyspark.ml.feature import VectorAssembler
        from pyspark.ml.regression import LinearRegression
        from pyspark.ml.evaluation import RegressionEvaluator
        
        start_time = time.time()
        
        # 准备特征
        assembler = VectorAssembler(
            inputCols=['value'],
            outputCol='features'
        )
        
        df_features = assembler.transform(df)
        
        # 分割训练和测试数据
        train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=42)
        
        # 训练模型
        lr = LinearRegression(featuresCol='features', labelCol='id')
        model = lr.fit(train_df)
        
        # 预测
        predictions = model.transform(test_df)
        
        # 评估
        evaluator = RegressionEvaluator(labelCol='id', predictionCol='prediction')
        rmse = evaluator.evaluate(predictions)
        
        processing_time = time.time() - start_time
        print(f"Spark机器学习耗时: {processing_time:.4f}秒")
        print(f"RMSE: {rmse:.4f}")
        
        return model, rmse
    
    def close(self):
        """关闭Spark会话"""
        self.spark.stop()

# 使用示例
def spark_computing_example():
    """Spark计算示例"""
    framework = SparkComputingFramework()
    
    # 创建测试数据
    df = framework.create_test_data(1000000)
    print(f"数据行数: {df.count()}")
    
    # 执行聚合操作
    agg_result = framework.spark_aggregation(df)
    agg_result.show()
    
    # 执行连接操作
    df2 = framework.create_test_data(500000)
    join_result = framework.spark_join_operation(df, df2)
    
    # 执行窗口函数
    window_result = framework.spark_window_functions(df)
    
    # 执行机器学习
    model, rmse = framework.spark_machine_learning(df)
    
    framework.close()
    
    return agg_result, join_result, window_result, model
```

## 6. 工具与平台

### 并行计算工具

1. **MPI**：消息传递接口
2. **OpenMP**：共享内存并行编程
3. **CUDA**：NVIDIA GPU编程
4. **OpenCL**：跨平台并行编程

### 分布式计算平台

1. **Apache Spark**：大数据处理平台
2. **Hadoop**：分布式存储和计算
3. **Dask**：Python并行计算
4. **Ray**：分布式计算框架

### 云计算平台

1. **AWS**：亚马逊云服务
2. **Google Cloud**：谷歌云平台
3. **Azure**：微软云平台
4. **阿里云**：阿里云服务

## 7. 最佳实践

### 性能优化

1. **算法优化**：选择合适的数据结构和算法
2. **内存管理**：合理使用内存，避免内存泄漏
3. **缓存策略**：利用缓存减少重复计算
4. **负载均衡**：均匀分配计算任务
5. **监控调优**：持续监控和优化性能

### 可扩展性

1. **模块化设计**：将系统分解为独立模块
2. **水平扩展**：通过增加节点扩展系统
3. **垂直扩展**：通过增加资源扩展系统
4. **弹性伸缩**：根据负载自动调整资源
5. **容错机制**：处理节点故障和数据丢失

### 成本优化

1. **资源利用率**：最大化资源使用效率
2. **按需付费**：根据实际使用量付费
3. **预留实例**：长期使用可考虑预留实例
4. **竞价实例**：非关键任务可使用竞价实例
5. **多区域部署**：选择成本较低的区域
