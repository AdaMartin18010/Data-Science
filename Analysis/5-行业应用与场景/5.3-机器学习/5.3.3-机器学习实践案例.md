# æœºå™¨å­¦ä¹ å®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [æœºå™¨å­¦ä¹ å®è·µæ¡ˆä¾‹](#æœºå™¨å­¦ä¹ å®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. åŸºç¡€æœºå™¨å­¦ä¹ ](#2-åŸºç¡€æœºå™¨å­¦ä¹ )
    - [2.1. çº¿æ€§å›å½’](#21-çº¿æ€§å›å½’)
  - [3. é€»è¾‘å›å½’](#3-é€»è¾‘å›å½’)
  - [4. å†³ç­–æ ‘](#4-å†³ç­–æ ‘)
  - [5. éšæœºæ£®æ—](#5-éšæœºæ£®æ—)
  - [6. æ·±åº¦å­¦ä¹ ](#6-æ·±åº¦å­¦ä¹ )
    - [6.1. ç®€å•ç¥ç»ç½‘ç»œ](#61-ç®€å•ç¥ç»ç½‘ç»œ)
  - [7. å·ç§¯ç¥ç»ç½‘ç»œ](#7-å·ç§¯ç¥ç»ç½‘ç»œ)
  - [8. å¾ªç¯ç¥ç»ç½‘ç»œ](#8-å¾ªç¯ç¥ç»ç½‘ç»œ)
  - [9. æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–](#9-æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–)
    - [9.1. äº¤å‰éªŒè¯](#91-äº¤å‰éªŒè¯)
  - [10. è¶…å‚æ•°è°ƒä¼˜](#10-è¶…å‚æ•°è°ƒä¼˜)
  - [11. æ€»ç»“](#11-æ€»ç»“)

---


## 1. æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æœºå™¨å­¦ä¹ æ¨¡å—çš„å®è·µæ¡ˆä¾‹ï¼ŒåŒ…æ‹¬åŸºç¡€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å…·ä½“åº”ç”¨ã€‚

## 2. åŸºç¡€æœºå™¨å­¦ä¹ 

### 2.1. çº¿æ€§å›å½’

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

class LinearRegressionModel:
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []

    def fit(self, X, y):
        """è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹"""
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for i in range(self.max_iterations):
# å‰å‘ä¼ æ’­
            y_pred = np.dot(X, self.weights) + self.bias

# è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)

# æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

# è®°å½•æˆæœ¬
            cost = np.mean((y_pred - y) ** 2)
            self.cost_history.append(cost)

    def predict(self, X):
        """é¢„æµ‹"""
        return np.dot(X, self.weights) + self.bias

    def plot_cost_history(self):
        """ç»˜åˆ¶æˆæœ¬å†å²"""
        plt.plot(self.cost_history)
        plt.xlabel('Iteration')
        plt.ylabel('Cost')
        plt.title('Cost History')
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X.flatten() + 1 + np.random.normal(0, 1, 100)

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è‡ªå®šä¹‰çº¿æ€§å›å½’
model = LinearRegressionModel(learning_rate=0.01, max_iterations=1000)
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

print("çº¿æ€§å›å½’ç»“æœ:")
print(f"MSE: {mean_squared_error(y_test, y_pred):.4f}")
print(f"RÂ²: {r2_score(y_test, y_pred):.4f}")

# ç»˜åˆ¶ç»“æœ
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.legend()
plt.show()
```

## 3. é€»è¾‘å›å½’

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

class LogisticRegressionModel:
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        """sigmoidå‡½æ•°"""
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for i in range(self.max_iterations):
# å‰å‘ä¼ æ’­
            linear_pred = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_pred)

# è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (predictions - y))
            db = (1/n_samples) * np.sum(predictions - y)

# æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        """é¢„æµ‹"""
        linear_pred = np.dot(X, self.weights) + self.bias
        y_pred = self.sigmoid(linear_pred)
        return (y_pred >= 0.5).astype(int)

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        linear_pred = np.dot(X, self.weights) + self.bias
        return self.sigmoid(linear_pred)

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.datasets import make_classification

# ç”Ÿæˆåˆ†ç±»æ•°æ®
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42, n_clusters_per_class=1)

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è‡ªå®šä¹‰é€»è¾‘å›å½’
model = LogisticRegressionModel(learning_rate=0.1, max_iterations=1000)
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

print("é€»è¾‘å›å½’ç»“æœ:")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print("åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
```

## 4. å†³ç­–æ ‘

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris

class DecisionTree:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def fit(self, X, y):
        """è®­ç»ƒå†³ç­–æ ‘"""
        self.tree = DecisionTreeClassifier(
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=42
        )
        self.tree.fit(X, y)

    def predict(self, X):
        """é¢„æµ‹"""
        return self.tree.predict(X)

    def plot_tree_structure(self):
        """ç»˜åˆ¶å†³ç­–æ ‘ç»“æ„"""
        plt.figure(figsize=(20,10))
        plot_tree(self.tree, filled=True, rounded=True, feature_names=[f'feature_{i}' for i in range(X.shape[1])])
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
iris = load_iris()
X = iris.data
y = iris.target

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# å†³ç­–æ ‘
dt_model = DecisionTree(max_depth=3)
dt_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = dt_model.predict(X_test)

print("å†³ç­–æ ‘ç»“æœ:")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print("åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

## 5. éšæœºæ£®æ—

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import feature_importance

class RandomForest:
    def __init__(self, n_estimators=100, max_depth=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.model = None

    def fit(self, X, y):
        """è®­ç»ƒéšæœºæ£®æ—"""
        self.model = RandomForestClassifier(
            n_estimators=self.n_estimators,
            max_depth=self.max_depth,
            random_state=42
        )
        self.model.fit(X, y)

    def predict(self, X):
        """é¢„æµ‹"""
        return self.model.predict(X)

    def get_feature_importance(self, feature_names=None):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        importance = self.model.feature_importances_
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(len(importance))]

        return dict(zip(feature_names, importance))

# ä½¿ç”¨ç¤ºä¾‹
rf_model = RandomForest(n_estimators=100, max_depth=5)
rf_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = rf_model.predict(X_test)

print("éšæœºæ£®æ—ç»“æœ:")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")

# ç‰¹å¾é‡è¦æ€§
importance = rf_model.get_feature_importance(iris.feature_names)
print("ç‰¹å¾é‡è¦æ€§:")
for feature, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True):
    print(f"{feature}: {imp:.4f}")
```

## 6. æ·±åº¦å­¦ä¹ 

### 6.1. ç®€å•ç¥ç»ç½‘ç»œ

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class SimpleNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x

class NeuralNetworkTrainer:
    def __init__(self, model, learning_rate=0.01):
        self.model = model
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.train_losses = []
        self.val_losses = []

    def train(self, train_loader, val_loader, epochs=100):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        for epoch in range(epochs):
# è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                loss.backward()
                self.optimizer.step()
                train_loss += loss.item()

# éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = self.model(batch_X)
                    loss = self.criterion(outputs, batch_y)
                    val_loss += loss.item()

            self.train_losses.append(train_loss / len(train_loader))
            self.val_losses.append(val_loss / len(val_loader))

            if epoch % 10 == 0:
                print(f'Epoch {epoch}: Train Loss: {self.train_losses[-1]:.4f}, Val Loss: {self.val_losses[-1]:.4f}')

    def predict(self, X):
        """é¢„æµ‹"""
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X)
            outputs = self.model(X_tensor)
            _, predicted = torch.max(outputs, 1)
            return predicted.numpy()

# ä½¿ç”¨ç¤ºä¾‹
# å‡†å¤‡æ•°æ®
X_tensor = torch.FloatTensor(X_train)
y_tensor = torch.LongTensor(y_train)

train_dataset = TensorDataset(X_tensor, y_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

X_val_tensor = torch.FloatTensor(X_test)
y_val_tensor = torch.LongTensor(y_test)

val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# åˆ›å»ºæ¨¡å‹
model = SimpleNeuralNetwork(input_size=4, hidden_size=10, output_size=3)
trainer = NeuralNetworkTrainer(model, learning_rate=0.01)

# è®­ç»ƒæ¨¡å‹
trainer.train(train_loader, val_loader, epochs=100)

# é¢„æµ‹
y_pred = trainer.predict(X_test)

print("ç¥ç»ç½‘ç»œç»“æœ:")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
```

## 7. å·ç§¯ç¥ç»ç½‘ç»œ

```python
import torchvision
import torchvision.transforms as transforms

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨éšæœºæ•°æ®ï¼‰
def create_mock_mnist_data():
    """åˆ›å»ºæ¨¡æ‹ŸMNISTæ•°æ®"""
    X = torch.randn(1000, 1, 28, 28)  # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    y = torch.randint(0, 10, (1000,))  # æ¨¡æ‹Ÿæ ‡ç­¾
    return X, y

# å‡†å¤‡æ•°æ®
X_mock, y_mock = create_mock_mnist_data()
X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(
    X_mock, y_mock, test_size=0.2, random_state=42
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_dataset = TensorDataset(X_train_cnn, y_train_cnn)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

val_dataset = TensorDataset(X_test_cnn, y_test_cnn)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# åˆ›å»ºCNNæ¨¡å‹
cnn_model = CNN(num_classes=10)
cnn_trainer = NeuralNetworkTrainer(cnn_model, learning_rate=0.001)

# è®­ç»ƒæ¨¡å‹
cnn_trainer.train(train_loader, val_loader, epochs=50)

# é¢„æµ‹
y_pred_cnn = cnn_trainer.predict(X_test_cnn)

print("CNNç»“æœ:")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test_cnn, y_pred_cnn):.4f}")
```

## 8. å¾ªç¯ç¥ç»ç½‘ç»œ

```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆåºåˆ—åˆ†ç±»ï¼‰
def create_sequence_data():
    """åˆ›å»ºåºåˆ—æ•°æ®"""
    X = torch.randn(1000, 10, 5)  # 1000ä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—10ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ—¶é—´æ­¥5ä¸ªç‰¹å¾
    y = torch.randint(0, 3, (1000,))  # 3ä¸ªç±»åˆ«
    return X, y

# å‡†å¤‡æ•°æ®
X_seq, y_seq = create_sequence_data()
X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(
    X_seq, y_seq, test_size=0.2, random_state=42
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_dataset = TensorDataset(X_train_seq, y_train_seq)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

val_dataset = TensorDataset(X_test_seq, y_test_seq)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# åˆ›å»ºRNNæ¨¡å‹
rnn_model = RNN(input_size=5, hidden_size=64, num_layers=2, num_classes=3)
rnn_trainer = NeuralNetworkTrainer(rnn_model, learning_rate=0.01)

# è®­ç»ƒæ¨¡å‹
rnn_trainer.train(train_loader, val_loader, epochs=50)

# é¢„æµ‹
y_pred_rnn = rnn_trainer.predict(X_test_seq)

print("RNNç»“æœ:")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test_seq, y_pred_rnn):.4f}")
```

## 9. æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–

### 9.1. äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import make_scorer, accuracy_score

class ModelEvaluator:
    @staticmethod
    def cross_validate_model(model, X, y, cv=5):
        """äº¤å‰éªŒè¯"""
        kfold = KFold(n_splits=cv, shuffle=True, random_state=42)
        scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

        print(f"äº¤å‰éªŒè¯ç»“æœ:")
        print(f"å¹³å‡å‡†ç¡®ç‡: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
        return scores

    @staticmethod
    def plot_learning_curves(trainer):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        plt.plot(trainer.train_losses, label='Training Loss')
        plt.plot(trainer.val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Learning Curves')
        plt.legend()
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
print("æ¨¡å‹è¯„ä¼°:")
# äº¤å‰éªŒè¯
scores = ModelEvaluator.cross_validate_model(rf_model.model, X, y, cv=5)

# å­¦ä¹ æ›²çº¿
ModelEvaluator.plot_learning_curves(trainer)
```

## 10. è¶…å‚æ•°è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV

class HyperparameterTuner:
    @staticmethod
    def tune_random_forest(X, y):
        """è°ƒä¼˜éšæœºæ£®æ—è¶…å‚æ•°"""
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7, None],
            'min_samples_split': [2, 5, 10]
        }

        rf = RandomForestClassifier(random_state=42)
        grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X, y)

        print("æœ€ä½³å‚æ•°:", grid_search.best_params_)
        print("æœ€ä½³å¾—åˆ†:", grid_search.best_score_)

        return grid_search.best_estimator_

# ä½¿ç”¨ç¤ºä¾‹
print("è¶…å‚æ•°è°ƒä¼˜:")
best_rf = HyperparameterTuner.tune_random_forest(X, y)
```

## 11. æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†æœºå™¨å­¦ä¹ æ¨¡å—çš„å®è·µæ¡ˆä¾‹ï¼ŒåŒ…æ‹¬ï¼š

1. **åŸºç¡€æœºå™¨å­¦ä¹ **ï¼šçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—
2. **æ·±åº¦å­¦ä¹ **ï¼šç®€å•ç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œ
3. **æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–**ï¼šäº¤å‰éªŒè¯ã€è¶…å‚æ•°è°ƒä¼˜

è¿™äº›å®è·µæ¡ˆä¾‹å±•ç¤ºäº†å¦‚ä½•å°†æœºå™¨å­¦ä¹ ç†è®ºåº”ç”¨åˆ°å®é™…çš„æ•°æ®åˆ†æå’Œé¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¸ºæ„å»ºæ™ºèƒ½ç³»ç»Ÿæä¾›äº†å®ç”¨çš„å·¥å…·å’Œæ–¹æ³•ã€‚
