# 深度学习深化

## 1. 神经网络架构设计

### 基础神经网络

```python
# 基础神经网络实现
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Optional, Dict

class BasicNeuralNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int, 
                 activation: str = 'relu', dropout: float = 0.1):
        super(BasicNeuralNetwork, self).__init__()
        
        self.layers = nn.ModuleList()
        self.activation = activation
        self.dropout = dropout
        
        # 构建网络层
        layer_sizes = [input_size] + hidden_sizes + [output_size]
        
        for i in range(len(layer_sizes) - 1):
            layer = nn.Linear(layer_sizes[i], layer_sizes[i + 1])
            nn.init.xavier_uniform_(layer.weight)
            nn.init.zeros_(layer.bias)
            self.layers.append(layer)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x)
            
            # 激活函数
            if self.activation == 'relu':
                x = F.relu(x)
            elif self.activation == 'tanh':
                x = torch.tanh(x)
            elif self.activation == 'sigmoid':
                x = torch.sigmoid(x)
            elif self.activation == 'leaky_relu':
                x = F.leaky_relu(x)
            
            # Dropout
            if self.dropout > 0 and i < len(self.layers) - 2:
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        # 输出层
        x = self.layers[-1](x)
        return x
    
    def get_activation_stats(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """获取激活统计信息"""
        stats = {}
        activations = []
        
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x)
            
            # 记录激活统计
            if self.activation == 'relu':
                x = F.relu(x)
                stats[f'layer_{i}_relu_mean'] = x.mean()
                stats[f'layer_{i}_relu_std'] = x.std()
                stats[f'layer_{i}_dead_neurons'] = (x == 0).float().mean()
            
            activations.append(x.clone())
            
            if self.dropout > 0 and i < len(self.layers) - 2:
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        x = self.layers[-1](x)
        stats['output_mean'] = x.mean()
        stats['output_std'] = x.std()
        
        return stats, activations

# 残差连接
class ResidualBlock(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, activation: str = 'relu'):
        super(ResidualBlock, self).__init__()
        
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, input_size)
        self.activation = activation
        
        # 初始化权重
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.zeros_(self.fc1.bias)
        nn.init.zeros_(self.fc2.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        
        x = self.fc1(x)
        if self.activation == 'relu':
            x = F.relu(x)
        x = self.fc2(x)
        
        # 残差连接
        return x + residual

class ResidualNetwork(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_blocks: int, 
                 output_size: int, activation: str = 'relu'):
        super(ResidualNetwork, self).__init__()
        
        self.input_layer = nn.Linear(input_size, hidden_size)
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(hidden_size, hidden_size, activation) 
            for _ in range(num_blocks)
        ])
        self.output_layer = nn.Linear(hidden_size, output_size)
        
        # 初始化权重
        nn.init.xavier_uniform_(self.input_layer.weight)
        nn.init.xavier_uniform_(self.output_layer.weight)
        nn.init.zeros_(self.input_layer.bias)
        nn.init.zeros_(self.output_layer.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.input_layer(x)
        x = F.relu(x)
        
        for block in self.residual_blocks:
            x = block(x)
        
        x = self.output_layer(x)
        return x
```

### 注意力机制

```python
# 注意力机制实现
class AttentionMechanism(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_heads: int = 8):
        super(AttentionMechanism, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_size = hidden_size // num_heads
        
        # 线性变换层
        self.query = nn.Linear(input_size, hidden_size)
        self.key = nn.Linear(input_size, hidden_size)
        self.value = nn.Linear(input_size, hidden_size)
        self.output = nn.Linear(hidden_size, input_size)
        
        # 缩放因子
        self.scale = self.head_size ** 0.5
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, input_size = x.size()
        
        # 计算Q, K, V
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_size)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_size)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_size)
        
        # 转置以便计算注意力
        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_size)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)
        
        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        # 应用mask
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 应用softmax
        attention_weights = F.softmax(scores, dim=-1)
        
        # 计算输出
        output = torch.matmul(attention_weights, V)
        output = output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_size
        )
        
        # 输出投影
        output = self.output(output)
        
        return output, attention_weights

# 多头自注意力
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_heads: int = 8, 
                 dropout: float = 0.1):
        super(MultiHeadSelfAttention, self).__init__()
        
        self.attention = AttentionMechanism(input_size, hidden_size, num_heads)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(input_size)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        residual = x
        
        # 自注意力
        output, attention_weights = self.attention(x, mask)
        output = self.dropout(output)
        
        # 残差连接和层归一化
        output = self.layer_norm(output + residual)
        
        return output, attention_weights

# Transformer编码器块
class TransformerEncoderBlock(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_heads: int = 8, 
                 ff_size: int = 2048, dropout: float = 0.1):
        super(TransformerEncoderBlock, self).__init__()
        
        self.attention = MultiHeadSelfAttention(input_size, hidden_size, num_heads, dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(input_size, ff_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(ff_size, input_size),
            nn.Dropout(dropout)
        )
        self.layer_norm1 = nn.LayerNorm(input_size)
        self.layer_norm2 = nn.LayerNorm(input_size)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # 自注意力
        attn_output, attention_weights = self.attention(x, mask)
        x = self.layer_norm1(x + attn_output)
        
        # 前馈网络
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)
        
        return x, attention_weights
```

## 2. 卷积神经网络

### CNN基础架构

```python
# 卷积神经网络实现
class ConvolutionalBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, 
                 stride: int = 1, padding: int = 1, activation: str = 'relu', 
                 batch_norm: bool = True, dropout: float = 0.1):
        super(ConvolutionalBlock, self).__init__()
        
        layers = []
        
        # 卷积层
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, 
                               stride=stride, padding=padding))
        
        # 批归一化
        if batch_norm:
            layers.append(nn.BatchNorm2d(out_channels))
        
        # 激活函数
        if activation == 'relu':
            layers.append(nn.ReLU(inplace=True))
        elif activation == 'leaky_relu':
            layers.append(nn.LeakyReLU(0.1, inplace=True))
        elif activation == 'elu':
            layers.append(nn.ELU(inplace=True))
        
        # Dropout
        if dropout > 0:
            layers.append(nn.Dropout2d(dropout))
        
        self.block = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.block(x)

class CNN(nn.Module):
    def __init__(self, input_channels: int = 3, num_classes: int = 1000):
        super(CNN, self).__init__()
        
        # 特征提取层
        self.features = nn.Sequential(
            ConvolutionalBlock(input_channels, 64, kernel_size=3),
            ConvolutionalBlock(64, 64, kernel_size=3),
            nn.MaxPool2d(2, 2),
            
            ConvolutionalBlock(64, 128, kernel_size=3),
            ConvolutionalBlock(128, 128, kernel_size=3),
            nn.MaxPool2d(2, 2),
            
            ConvolutionalBlock(128, 256, kernel_size=3),
            ConvolutionalBlock(256, 256, kernel_size=3),
            ConvolutionalBlock(256, 256, kernel_size=3),
            nn.MaxPool2d(2, 2),
            
            ConvolutionalBlock(256, 512, kernel_size=3),
            ConvolutionalBlock(512, 512, kernel_size=3),
            ConvolutionalBlock(512, 512, kernel_size=3),
            nn.MaxPool2d(2, 2),
            
            ConvolutionalBlock(512, 512, kernel_size=3),
            ConvolutionalBlock(512, 512, kernel_size=3),
            ConvolutionalBlock(512, 512, kernel_size=3),
            nn.MaxPool2d(2, 2)
        )
        
        # 分类器
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((7, 7)),
            nn.Flatten(),
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes)
        )
        
        # 初始化权重
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.classifier(x)
        return x
    
    def get_feature_maps(self, x: torch.Tensor) -> List[torch.Tensor]:
        """获取特征图"""
        feature_maps = []
        
        for layer in self.features:
            x = layer(x)
            if isinstance(layer, ConvolutionalBlock):
                feature_maps.append(x.clone())
        
        return feature_maps
```

### 残差网络

```python
# 残差网络实现
class ResidualBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, 
                 expansion: int = 1):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 残差连接
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                          stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = self.shortcut(x)
        
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        out = F.relu(out)
        
        return out

class ResNet(nn.Module):
    def __init__(self, block: nn.Module, num_blocks: List[int], num_classes: int = 1000):
        super(ResNet, self).__init__()
        
        self.in_channels = 64
        
        # 初始卷积层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # 残差层
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        
        # 分类器
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        
        # 初始化权重
        self._initialize_weights()
    
    def _make_layer(self, block: nn.Module, out_channels: int, num_blocks: int, 
                   stride: int) -> nn.Sequential:
        layers = []
        layers.append(block(self.in_channels, out_channels, stride))
        self.in_channels = out_channels
        
        for _ in range(1, num_blocks):
            layers.append(block(out_channels, out_channels))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

# ResNet-18
def resnet18(num_classes: int = 1000) -> ResNet:
    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes)

# ResNet-34
def resnet34(num_classes: int = 1000) -> ResNet:
    return ResNet(ResidualBlock, [3, 4, 6, 3], num_classes)
```

## 3. 循环神经网络

### LSTM实现

```python
# LSTM实现
import math

class LSTMCell(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, bias: bool = True):
        super(LSTMCell, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias
        
        # 输入门、遗忘门、输出门、候选值
        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))
        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))
        
        if bias:
            self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))
            self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))
        else:
            self.register_parameter('bias_ih', None)
            self.register_parameter('bias_hh', None)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)
    
    def forward(self, input: torch.Tensor, hx: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        h_prev, c_prev = hx
        
        gates = (torch.mm(input, self.weight_ih.t()) + 
                torch.mm(h_prev, self.weight_hh.t()))
        
        if self.bias:
            gates += self.bias_ih + self.bias_hh
        
        # 分离门控值
        gates = gates.chunk(4, 1)
        input_gate = torch.sigmoid(gates[0])
        forget_gate = torch.sigmoid(gates[1])
        output_gate = torch.sigmoid(gates[2])
        candidate = torch.tanh(gates[3])
        
        # LSTM更新
        c_next = forget_gate * c_prev + input_gate * candidate
        h_next = output_gate * torch.tanh(c_next)
        
        return h_next, c_next

class LSTM(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1, 
                 bias: bool = True, batch_first: bool = False, dropout: float = 0.0):
        super(LSTM, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        
        # LSTM层
        self.lstm_cells = nn.ModuleList()
        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size
            self.lstm_cells.append(LSTMCell(layer_input_size, hidden_size, bias))
        
        # Dropout层
        if dropout > 0 and num_layers > 1:
            self.dropout_layer = nn.Dropout(dropout)
        else:
            self.dropout_layer = None
    
    def forward(self, input: torch.Tensor, hx: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        if self.batch_first:
            input = input.transpose(0, 1)
        
        seq_len, batch_size, input_size = input.size()
        
        # 初始化隐藏状态
        if hx is None:
            h = torch.zeros(self.num_layers, batch_size, self.hidden_size, 
                           device=input.device, dtype=input.dtype)
            c = torch.zeros(self.num_layers, batch_size, self.hidden_size, 
                           device=input.device, dtype=input.dtype)
        else:
            h, c = hx
        
        # 前向传播
        outputs = []
        for t in range(seq_len):
            x_t = input[t]
            
            for layer in range(self.num_layers):
                if layer == 0:
                    h[layer], c[layer] = self.lstm_cells[layer](x_t, (h[layer], c[layer]))
                else:
                    if self.dropout_layer is not None:
                        x_t = self.dropout_layer(h[layer-1])
                    else:
                        x_t = h[layer-1]
                    h[layer], c[layer] = self.lstm_cells[layer](x_t, (h[layer], c[layer]))
            
            outputs.append(h[-1])
        
        outputs = torch.stack(outputs)
        
        if self.batch_first:
            outputs = outputs.transpose(0, 1)
        
        return outputs, (h, c)
```

### GRU实现

```python
# GRU实现
class GRUCell(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, bias: bool = True):
        super(GRUCell, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias
        
        # 重置门、更新门、候选隐藏状态
        self.weight_ih = nn.Parameter(torch.randn(3 * hidden_size, input_size))
        self.weight_hh = nn.Parameter(torch.randn(3 * hidden_size, hidden_size))
        
        if bias:
            self.bias_ih = nn.Parameter(torch.randn(3 * hidden_size))
            self.bias_hh = nn.Parameter(torch.randn(3 * hidden_size))
        else:
            self.register_parameter('bias_ih', None)
            self.register_parameter('bias_hh', None)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)
    
    def forward(self, input: torch.Tensor, hx: torch.Tensor) -> torch.Tensor:
        gates = (torch.mm(input, self.weight_ih.t()) + 
                torch.mm(hx, self.weight_hh.t()))
        
        if self.bias:
            gates += self.bias_ih + self.bias_hh
        
        # 分离门控值
        gates = gates.chunk(3, 1)
        reset_gate = torch.sigmoid(gates[0])
        update_gate = torch.sigmoid(gates[1])
        candidate = torch.tanh(gates[2])
        
        # GRU更新
        h_next = update_gate * hx + (1 - update_gate) * candidate
        
        return h_next

class GRU(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1, 
                 bias: bool = True, batch_first: bool = False, dropout: float = 0.0):
        super(GRU, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        
        # GRU层
        self.gru_cells = nn.ModuleList()
        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size
            self.gru_cells.append(GRUCell(layer_input_size, hidden_size, bias))
        
        # Dropout层
        if dropout > 0 and num_layers > 1:
            self.dropout_layer = nn.Dropout(dropout)
        else:
            self.dropout_layer = None
    
    def forward(self, input: torch.Tensor, hx: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        if self.batch_first:
            input = input.transpose(0, 1)
        
        seq_len, batch_size, input_size = input.size()
        
        # 初始化隐藏状态
        if hx is None:
            h = torch.zeros(self.num_layers, batch_size, self.hidden_size, 
                           device=input.device, dtype=input.dtype)
        else:
            h = hx
        
        # 前向传播
        outputs = []
        for t in range(seq_len):
            x_t = input[t]
            
            for layer in range(self.num_layers):
                if layer == 0:
                    h[layer] = self.gru_cells[layer](x_t, h[layer])
                else:
                    if self.dropout_layer is not None:
                        x_t = self.dropout_layer(h[layer-1])
                    else:
                        x_t = h[layer-1]
                    h[layer] = self.gru_cells[layer](x_t, h[layer])
            
            outputs.append(h[-1])
        
        outputs = torch.stack(outputs)
        
        if self.batch_first:
            outputs = outputs.transpose(0, 1)
        
        return outputs, h
```

## 4. 生成对抗网络

### GAN基础实现

```python
# 生成对抗网络实现
class Generator(nn.Module):
    def __init__(self, latent_dim: int = 100, hidden_dims: List[int] = [256, 512, 1024], 
                 output_channels: int = 3):
        super(Generator, self).__init__()
        
        self.latent_dim = latent_dim
        self.hidden_dims = hidden_dims
        
        layers = []
        in_channels = latent_dim
        
        # 构建生成器网络
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_channels, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(inplace=True)
            ])
            in_channels = hidden_dim
        
        # 输出层
        layers.append(nn.Linear(in_channels, output_channels * 64 * 64))
        layers.append(nn.Tanh())
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        x = self.network(z)
        x = x.view(x.size(0), 3, 64, 64)  # 重塑为图像格式
        return x

class Discriminator(nn.Module):
    def __init__(self, input_channels: int = 3, hidden_dims: List[int] = [64, 128, 256]):
        super(Discriminator, self).__init__()
        
        self.hidden_dims = hidden_dims
        
        layers = []
        in_channels = input_channels
        
        # 构建判别器网络
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Conv2d(in_channels, hidden_dim, kernel_size=4, stride=2, padding=1),
                nn.BatchNorm2d(hidden_dim),
                nn.LeakyReLU(0.2, inplace=True)
            ])
            in_channels = hidden_dim
        
        # 输出层
        layers.extend([
            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1),
            nn.Sigmoid()
        ])
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x).view(-1, 1).squeeze(1)

class GAN(nn.Module):
    def __init__(self, latent_dim: int = 100, generator_hidden_dims: List[int] = [256, 512, 1024],
                 discriminator_hidden_dims: List[int] = [64, 128, 256]):
        super(GAN, self).__init__()
        
        self.generator = Generator(latent_dim, generator_hidden_dims)
        self.discriminator = Discriminator(3, discriminator_hidden_dims)
        
        # 损失函数
        self.criterion = nn.BCELoss()
    
    def train_step(self, real_images: torch.Tensor, batch_size: int, 
                   device: torch.device) -> Dict[str, float]:
        """训练一步"""
        # 真实图像标签
        real_labels = torch.ones(batch_size, device=device)
        # 生成图像标签
        fake_labels = torch.zeros(batch_size, device=device)
        
        # 训练判别器
        self.discriminator.zero_grad()
        
        # 真实图像
        real_outputs = self.discriminator(real_images)
        d_loss_real = self.criterion(real_outputs, real_labels)
        
        # 生成图像
        z = torch.randn(batch_size, self.generator.latent_dim, device=device)
        fake_images = self.generator(z)
        fake_outputs = self.discriminator(fake_images.detach())
        d_loss_fake = self.criterion(fake_outputs, fake_labels)
        
        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()
        
        # 训练生成器
        self.generator.zero_grad()
        
        fake_outputs = self.discriminator(fake_images)
        g_loss = self.criterion(fake_outputs, real_labels)
        g_loss.backward()
        
        return {
            'discriminator_loss': d_loss.item(),
            'generator_loss': g_loss.item()
        }
    
    def generate_images(self, num_images: int, device: torch.device) -> torch.Tensor:
        """生成图像"""
        self.generator.eval()
        with torch.no_grad():
            z = torch.randn(num_images, self.generator.latent_dim, device=device)
            fake_images = self.generator(z)
        return fake_images
```

### DCGAN实现

```python
# 深度卷积GAN实现
class DCGenerator(nn.Module):
    def __init__(self, latent_dim: int = 100, ngf: int = 64, nc: int = 3):
        super(DCGenerator, self).__init__()
        
        self.main = nn.Sequential(
            # 输入: latent_dim x 1 x 1
            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # 状态尺寸: (ngf*8) x 4 x 4
            
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # 状态尺寸: (ngf*4) x 8 x 8
            
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # 状态尺寸: (ngf*2) x 16 x 16
            
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # 状态尺寸: (ngf) x 32 x 32
            
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # 状态尺寸: (nc) x 64 x 64
        )
    
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        return self.main(z)

class DCDiscriminator(nn.Module):
    def __init__(self, nc: int = 3, ndf: int = 64):
        super(DCDiscriminator, self).__init__()
        
        self.main = nn.Sequential(
            # 输入: (nc) x 64 x 64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # 状态尺寸: (ndf) x 32 x 32
            
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # 状态尺寸: (ndf*2) x 16 x 16
            
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # 状态尺寸: (ndf*4) x 8 x 8
            
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # 状态尺寸: (ndf*8) x 4 x 4
            
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.main(x).view(-1, 1).squeeze(1)

class DCGAN(nn.Module):
    def __init__(self, latent_dim: int = 100, ngf: int = 64, ndf: int = 64, nc: int = 3):
        super(DCGAN, self).__init__()
        
        self.generator = DCGenerator(latent_dim, ngf, nc)
        self.discriminator = DCDiscriminator(nc, ndf)
        
        # 损失函数
        self.criterion = nn.BCELoss()
    
    def train_step(self, real_images: torch.Tensor, batch_size: int, 
                   device: torch.device) -> Dict[str, float]:
        """训练一步"""
        # 真实图像标签
        real_labels = torch.ones(batch_size, device=device)
        # 生成图像标签
        fake_labels = torch.zeros(batch_size, device=device)
        
        # 训练判别器
        self.discriminator.zero_grad()
        
        # 真实图像
        real_outputs = self.discriminator(real_images)
        d_loss_real = self.criterion(real_outputs, real_labels)
        
        # 生成图像
        z = torch.randn(batch_size, self.generator.latent_dim, device=device)
        fake_images = self.generator(z)
        fake_outputs = self.discriminator(fake_images.detach())
        d_loss_fake = self.criterion(fake_outputs, fake_labels)
        
        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()
        
        # 训练生成器
        self.generator.zero_grad()
        
        fake_outputs = self.discriminator(fake_images)
        g_loss = self.criterion(fake_outputs, real_labels)
        g_loss.backward()
        
        return {
            'discriminator_loss': d_loss.item(),
            'generator_loss': g_loss.item()
        }
```

## 5. 工具与平台

### 深度学习框架

1. **PyTorch**：动态计算图，易于调试
2. **TensorFlow**：静态计算图，生产部署
3. **Keras**：高级API，快速原型
4. **JAX**：自动微分和GPU加速

### 模型训练工具

1. **Weights & Biases**：实验跟踪
2. **TensorBoard**：可视化工具
3. **MLflow**：机器学习生命周期管理
4. **Optuna**：超参数优化

### 预训练模型

1. **Hugging Face**：NLP模型库
2. **TorchVision**：计算机视觉模型
3. **TensorFlow Hub**：模型仓库
4. **ONNX**：模型交换格式

## 6. 最佳实践

### 模型设计

1. **架构选择**：根据任务选择合适的网络架构
2. **正则化**：使用Dropout、BatchNorm等防止过拟合
3. **初始化**：合理的权重初始化策略
4. **激活函数**：选择合适的激活函数
5. **残差连接**：使用残差连接训练深层网络

### 训练策略

1. **学习率调度**：使用学习率衰减策略
2. **数据增强**：增加训练数据的多样性
3. **早停**：防止过拟合的早停机制
4. **梯度裁剪**：防止梯度爆炸
5. **混合精度**：使用混合精度训练加速

### 模型评估

1. **交叉验证**：使用交叉验证评估模型
2. **混淆矩阵**：分析分类性能
3. **ROC曲线**：评估二分类性能
4. **特征重要性**：分析模型决策过程
5. **可解释性**：使用可解释性技术
