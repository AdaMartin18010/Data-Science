# 知识图谱系统集成指南

## 系统架构概览

### 1. 总体架构设计

```mermaid
graph TB
    subgraph "数据层"
        A[Neo4j图数据库] --> B[PostgreSQL关系数据库]
        B --> C[Redis缓存]
        C --> D[Elasticsearch搜索引擎]
    end
    
    subgraph "服务层"
        E[知识图谱核心服务] --> F[API网关]
        F --> G[认证授权服务]
        G --> H[流处理服务]
        H --> I[分析服务]
    end
    
    subgraph "应用层"
        J[Web前端] --> K[移动应用]
        K --> L[可视化工具]
        L --> M[管理控制台]
    end
    
    subgraph "外部集成"
        N[数据源系统] --> O[第三方API]
        O --> P[消息队列]
        P --> Q[监控系统]
    end
```

### 2. 核心组件集成

```python
from typing import Dict, List, Any, Optional
import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime
import json

@dataclass
class SystemComponent:
    """系统组件定义"""
    name: str
    type: str
    config: Dict[str, Any]
    dependencies: List[str]
    health_check_url: Optional[str] = None
    
@dataclass
class IntegrationConfig:
    """集成配置"""
    component: str
    endpoint: str
    credentials: Dict[str, Any]
    timeout: int = 30
    retry_count: int = 3
    
class KGSystemIntegrator:
    """知识图谱系统集成器"""
    
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.components: Dict[str, SystemComponent] = {}
        self.integrations: Dict[str, IntegrationConfig] = {}
        self.logger = logging.getLogger(__name__)
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """加载系统配置"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
            
    async def initialize_system(self):
        """初始化系统"""
        self.logger.info("开始初始化知识图谱系统")
        
        # 加载组件配置
        await self._load_components()
        
        # 按依赖顺序启动组件
        await self._start_components_in_order()
        
        # 验证系统健康状态
        await self._verify_system_health()
        
        self.logger.info("知识图谱系统初始化完成")
        
    async def _load_components(self):
        """加载系统组件"""
        components_config = self.config.get('components', {})
        
        for name, config in components_config.items():
            component = SystemComponent(
                name=name,
                type=config['type'],
                config=config,
                dependencies=config.get('dependencies', []),
                health_check_url=config.get('health_check_url')
            )
            self.components[name] = component
            
    async def _start_components_in_order(self):
        """按依赖顺序启动组件"""
        started_components = set()
        
        while len(started_components) < len(self.components):
            for name, component in self.components.items():
                if name in started_components:
                    continue
                    
                # 检查依赖是否已启动
                dependencies_ready = all(
                    dep in started_components 
                    for dep in component.dependencies
                )
                
                if dependencies_ready:
                    await self._start_component(component)
                    started_components.add(name)
                    
    async def _start_component(self, component: SystemComponent):
        """启动单个组件"""
        self.logger.info(f"启动组件: {component.name}")
        
        try:
            if component.type == 'database':
                await self._start_database_component(component)
            elif component.type == 'service':
                await self._start_service_component(component)
            elif component.type == 'cache':
                await self._start_cache_component(component)
            elif component.type == 'search':
                await self._start_search_component(component)
            else:
                self.logger.warning(f"未知组件类型: {component.type}")
                
        except Exception as e:
            self.logger.error(f"启动组件 {component.name} 失败: {e}")
            raise
            
    async def _start_database_component(self, component: SystemComponent):
        """启动数据库组件"""
        if component.name == 'neo4j':
            await self._setup_neo4j(component.config)
        elif component.name == 'postgresql':
            await self._setup_postgresql(component.config)
            
    async def _start_service_component(self, component: SystemComponent):
        """启动服务组件"""
        # 实现服务启动逻辑
        pass
        
    async def _start_cache_component(self, component: SystemComponent):
        """启动缓存组件"""
        if component.name == 'redis':
            await self._setup_redis(component.config)
            
    async def _start_search_component(self, component: SystemComponent):
        """启动搜索组件"""
        if component.name == 'elasticsearch':
            await self._setup_elasticsearch(component.config)
            
    async def _setup_neo4j(self, config: Dict[str, Any]):
        """设置Neo4j"""
        from neo4j import AsyncGraphDatabase
        
        uri = config['uri']
        username = config['username']
        password = config['password']
        
        driver = AsyncGraphDatabase.driver(uri, auth=(username, password))
        
        # 验证连接
        async with driver.session() as session:
            result = await session.run("RETURN 1 as test")
            await result.single()
            
        self.logger.info("Neo4j连接成功")
        
        # 创建索引和约束
        await self._create_neo4j_schema(driver)
        
        await driver.close()
        
    async def _create_neo4j_schema(self, driver):
        """创建Neo4j模式"""
        schema_queries = [
            "CREATE CONSTRAINT IF NOT EXISTS FOR (n:Entity) REQUIRE n.id IS UNIQUE",
            "CREATE CONSTRAINT IF NOT EXISTS FOR (n:Person) REQUIRE n.id IS UNIQUE",
            "CREATE CONSTRAINT IF NOT EXISTS FOR (n:Organization) REQUIRE n.id IS UNIQUE",
            "CREATE INDEX IF NOT EXISTS FOR (n:Entity) ON (n.name)",
            "CREATE INDEX IF NOT EXISTS FOR (n:Entity) ON (n.type)",
            "CREATE INDEX IF NOT EXISTS FOR (n:Entity) ON (n.created_at)",
        ]
        
        async with driver.session() as session:
            for query in schema_queries:
                await session.run(query)
                
    async def _setup_postgresql(self, config: Dict[str, Any]):
        """设置PostgreSQL"""
        import asyncpg
        
        connection_string = config['connection_string']
        
        conn = await asyncpg.connect(connection_string)
        
        # 验证连接
        await conn.fetchval("SELECT 1")
        
        self.logger.info("PostgreSQL连接成功")
        
        # 创建表结构
        await self._create_postgresql_schema(conn)
        
        await conn.close()
        
    async def _create_postgresql_schema(self, conn):
        """创建PostgreSQL模式"""
        schema_sql = """
        CREATE TABLE IF NOT EXISTS kg_metadata (
            id SERIAL PRIMARY KEY,
            entity_type VARCHAR(100) NOT NULL,
            entity_id VARCHAR(255) NOT NULL,
            metadata JSONB,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE TABLE IF NOT EXISTS kg_analytics (
            id SERIAL PRIMARY KEY,
            metric_name VARCHAR(100) NOT NULL,
            metric_value NUMERIC,
            dimensions JSONB,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE TABLE IF NOT EXISTS kg_change_log (
            id SERIAL PRIMARY KEY,
            change_type VARCHAR(50) NOT NULL,
            entity_id VARCHAR(255) NOT NULL,
            old_state JSONB,
            new_state JSONB,
            user_id VARCHAR(255),
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_kg_metadata_entity ON kg_metadata(entity_type, entity_id);
        CREATE INDEX IF NOT EXISTS idx_kg_analytics_timestamp ON kg_analytics(timestamp);
        CREATE INDEX IF NOT EXISTS idx_kg_change_log_timestamp ON kg_change_log(timestamp);
        """
        
        await conn.execute(schema_sql)
        
    async def _setup_redis(self, config: Dict[str, Any]):
        """设置Redis"""
        import aioredis
        
        redis_url = config['url']
        
        redis = aioredis.from_url(redis_url)
        
        # 验证连接
        await redis.ping()
        
        self.logger.info("Redis连接成功")
        
        await redis.close()
        
    async def _setup_elasticsearch(self, config: Dict[str, Any]):
        """设置Elasticsearch"""
        from elasticsearch import AsyncElasticsearch
        
        hosts = config['hosts']
        
        es = AsyncElasticsearch(hosts=hosts)
        
        # 验证连接
        await es.ping()
        
        self.logger.info("Elasticsearch连接成功")
        
        # 创建索引
        await self._create_elasticsearch_indices(es)
        
        await es.close()
        
    async def _create_elasticsearch_indices(self, es):
        """创建Elasticsearch索引"""
        # 节点索引
        node_mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "label": {"type": "keyword"},
                    "name": {"type": "text", "analyzer": "standard"},
                    "description": {"type": "text"},
                    "properties": {"type": "object"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
        
        # 关系索引
        relationship_mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "type": {"type": "keyword"},
                    "from_id": {"type": "keyword"},
                    "to_id": {"type": "keyword"},
                    "properties": {"type": "object"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # 创建索引
        await es.indices.create(index="kg_nodes", body=node_mapping, ignore=400)
        await es.indices.create(index="kg_relationships", body=relationship_mapping, ignore=400)
        
    async def _verify_system_health(self):
        """验证系统健康状态"""
        self.logger.info("验证系统健康状态")
        
        health_results = {}
        
        for name, component in self.components.items():
            if component.health_check_url:
                try:
                    health_status = await self._check_component_health(component)
                    health_results[name] = health_status
                except Exception as e:
                    health_results[name] = {'status': 'unhealthy', 'error': str(e)}
                    
        # 输出健康检查结果
        for name, result in health_results.items():
            status = result.get('status', 'unknown')
            if status == 'healthy':
                self.logger.info(f"组件 {name} 健康状态: {status}")
            else:
                self.logger.error(f"组件 {name} 健康状态: {status}, 错误: {result.get('error', 'unknown')}")
                
    async def _check_component_health(self, component: SystemComponent) -> Dict[str, Any]:
        """检查组件健康状态"""
        import aiohttp
        
        async with aiohttp.ClientSession() as session:
            async with session.get(component.health_check_url, timeout=10) as response:
                if response.status == 200:
                    return {'status': 'healthy'}
                else:
                    return {'status': 'unhealthy', 'http_status': response.status}
```

### 3. API集成层

```python
class APIIntegrationLayer:
    """API集成层"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.api_clients = {}
        
    async def initialize(self):
        """初始化API集成层"""
        await self._setup_api_clients()
        await self._setup_api_gateway()
        
    async def _setup_api_clients(self):
        """设置API客户端"""
        from .clients import (
            Neo4jClient, PostgreSQLClient, 
            RedisClient, ElasticsearchClient
        )
        
        # Neo4j客户端
        neo4j_config = self.config['neo4j']
        self.api_clients['neo4j'] = Neo4jClient(neo4j_config)
        
        # PostgreSQL客户端
        pg_config = self.config['postgresql']
        self.api_clients['postgresql'] = PostgreSQLClient(pg_config)
        
        # Redis客户端
        redis_config = self.config['redis']
        self.api_clients['redis'] = RedisClient(redis_config)
        
        # Elasticsearch客户端
        es_config = self.config['elasticsearch']
        self.api_clients['elasticsearch'] = ElasticsearchClient(es_config)
        
    async def _setup_api_gateway(self):
        """设置API网关"""
        from fastapi import FastAPI, Depends, HTTPException
        from fastapi.middleware.cors import CORSMiddleware
        from .auth import get_current_user
        from .endpoints import create_kg_endpoints
        
        app = FastAPI(title="Knowledge Graph API", version="1.0.0")
        
        # 添加CORS中间件
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # 添加认证中间件
        from .middleware import AuthenticationMiddleware
        app.add_middleware(AuthenticationMiddleware)
        
        # 创建API端点
        endpoints = create_kg_endpoints(self.api_clients)
        app.include_router(endpoints, prefix="/api/v1")
        
        return app

class DataSynchronizer:
    """数据同步器"""
    
    def __init__(self, api_clients: Dict[str, Any]):
        self.api_clients = api_clients
        self.logger = logging.getLogger(__name__)
        
    async def sync_node_to_search(self, node_data: Dict[str, Any]):
        """同步节点到搜索引擎"""
        try:
            # 转换数据格式
            search_doc = {
                'id': node_data['id'],
                'type': 'node',
                'label': node_data.get('label'),
                'name': node_data.get('properties', {}).get('name'),
                'description': node_data.get('properties', {}).get('description'),
                'properties': node_data.get('properties', {}),
                'created_at': node_data.get('created_at'),
                'updated_at': datetime.now().isoformat()
            }
            
            # 索引到Elasticsearch
            await self.api_clients['elasticsearch'].index_document(
                index='kg_nodes',
                doc_id=node_data['id'],
                document=search_doc
            )
            
            self.logger.debug(f"节点 {node_data['id']} 已同步到搜索引擎")
            
        except Exception as e:
            self.logger.error(f"同步节点到搜索引擎失败: {e}")
            
    async def sync_relationship_to_search(self, rel_data: Dict[str, Any]):
        """同步关系到搜索引擎"""
        try:
            search_doc = {
                'id': rel_data['id'],
                'type': 'relationship',
                'rel_type': rel_data.get('type'),
                'from_id': rel_data.get('from_id'),
                'to_id': rel_data.get('to_id'),
                'properties': rel_data.get('properties', {}),
                'created_at': rel_data.get('created_at')
            }
            
            await self.api_clients['elasticsearch'].index_document(
                index='kg_relationships',
                doc_id=rel_data['id'],
                document=search_doc
            )
            
            self.logger.debug(f"关系 {rel_data['id']} 已同步到搜索引擎")
            
        except Exception as e:
            self.logger.error(f"同步关系到搜索引擎失败: {e}")
            
    async def cache_frequent_queries(self):
        """缓存频繁查询"""
        frequent_queries = [
            ("popular_entities", "MATCH (n) RETURN n.name, count(*) as connections ORDER BY connections DESC LIMIT 20"),
            ("recent_updates", "MATCH (n) WHERE n.updated_at > datetime() - duration('P1D') RETURN n ORDER BY n.updated_at DESC LIMIT 50")
        ]
        
        for cache_key, query in frequent_queries:
            try:
                result = await self.api_clients['neo4j'].execute_query(query)
                
                # 缓存结果
                await self.api_clients['redis'].set_cache(
                    cache_key, 
                    json.dumps(result), 
                    expire=3600  # 1小时过期
                )
                
                self.logger.debug(f"查询结果已缓存: {cache_key}")
                
            except Exception as e:
                self.logger.error(f"缓存查询失败: {e}")
```

### 4. 外部系统集成

```python
class ExternalSystemIntegrator:
    """外部系统集成器"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.integrations = {}
        
    async def initialize(self):
        """初始化外部系统集成"""
        await self._setup_data_source_integrations()
        await self._setup_notification_integrations()
        await self._setup_monitoring_integrations()
        
    async def _setup_data_source_integrations(self):
        """设置数据源集成"""
        data_sources = self.config.get('data_sources', [])
        
        for source_config in data_sources:
            source_type = source_config['type']
            
            if source_type == 'rest_api':
                integration = RESTAPIIntegration(source_config)
            elif source_type == 'database':
                integration = DatabaseIntegration(source_config)
            elif source_type == 'file_system':
                integration = FileSystemIntegration(source_config)
            elif source_type == 'message_queue':
                integration = MessageQueueIntegration(source_config)
            else:
                self.logger.warning(f"未支持的数据源类型: {source_type}")
                continue
                
            self.integrations[source_config['name']] = integration
            await integration.initialize()
            
    async def _setup_notification_integrations(self):
        """设置通知集成"""
        notification_config = self.config.get('notifications', {})
        
        if 'email' in notification_config:
            self.integrations['email'] = EmailNotificationIntegration(
                notification_config['email']
            )
            
        if 'slack' in notification_config:
            self.integrations['slack'] = SlackNotificationIntegration(
                notification_config['slack']
            )
            
        if 'webhook' in notification_config:
            self.integrations['webhook'] = WebhookNotificationIntegration(
                notification_config['webhook']
            )
            
    async def _setup_monitoring_integrations(self):
        """设置监控集成"""
        monitoring_config = self.config.get('monitoring', {})
        
        if 'prometheus' in monitoring_config:
            self.integrations['prometheus'] = PrometheusIntegration(
                monitoring_config['prometheus']
            )
            
        if 'grafana' in monitoring_config:
            self.integrations['grafana'] = GrafanaIntegration(
                monitoring_config['grafana']
            )

class RESTAPIIntegration:
    """REST API集成"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
    async def initialize(self):
        """初始化REST API集成"""
        self.base_url = self.config['base_url']
        self.headers = self.config.get('headers', {})
        self.auth = self.config.get('auth', {})
        
    async def fetch_data(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """获取数据"""
        import aiohttp
        
        url = f"{self.base_url}/{endpoint}"
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=self.headers, params=params) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    raise Exception(f"API请求失败: {response.status}")
                    
    async def push_data(self, endpoint: str, data: Dict[str, Any]) -> bool:
        """推送数据"""
        import aiohttp
        
        url = f"{self.base_url}/{endpoint}"
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=self.headers, json=data) as response:
                return response.status in [200, 201, 204]

class DatabaseIntegration:
    """数据库集成"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.connection = None
        
    async def initialize(self):
        """初始化数据库连接"""
        db_type = self.config['db_type']
        
        if db_type == 'postgresql':
            import asyncpg
            self.connection = await asyncpg.connect(self.config['connection_string'])
        elif db_type == 'mysql':
            import aiomysql
            self.connection = await aiomysql.connect(**self.config['connection_params'])
        else:
            raise ValueError(f"不支持的数据库类型: {db_type}")
            
    async def extract_data(self, query: str) -> List[Dict[str, Any]]:
        """提取数据"""
        if not self.connection:
            raise RuntimeError("数据库连接未初始化")
            
        rows = await self.connection.fetch(query)
        return [dict(row) for row in rows]
        
    async def close(self):
        """关闭连接"""
        if self.connection:
            await self.connection.close()

class MessageQueueIntegration:
    """消息队列集成"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
    async def initialize(self):
        """初始化消息队列"""
        queue_type = self.config['type']
        
        if queue_type == 'kafka':
            await self._setup_kafka()
        elif queue_type == 'rabbitmq':
            await self._setup_rabbitmq()
        elif queue_type == 'redis':
            await self._setup_redis_queue()
            
    async def _setup_kafka(self):
        """设置Kafka"""
        from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
        
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.config['bootstrap_servers']
        )
        await self.producer.start()
        
        self.consumer = AIOKafkaConsumer(
            *self.config['topics'],
            bootstrap_servers=self.config['bootstrap_servers'],
            group_id=self.config.get('group_id', 'kg_integration')
        )
        await self.consumer.start()
        
    async def publish_message(self, topic: str, message: Dict[str, Any]):
        """发布消息"""
        if hasattr(self, 'producer'):
            await self.producer.send(topic, json.dumps(message).encode())
            
    async def consume_messages(self):
        """消费消息"""
        if hasattr(self, 'consumer'):
            async for message in self.consumer:
                yield json.loads(message.value.decode())
```

### 5. 部署和配置管理

```python
class DeploymentManager:
    """部署管理器"""
    
    def __init__(self, environment: str):
        self.environment = environment
        self.logger = logging.getLogger(__name__)
        
    async def deploy_system(self, deployment_config: Dict[str, Any]):
        """部署系统"""
        self.logger.info(f"开始部署到 {self.environment} 环境")
        
        # 验证部署配置
        await self._validate_deployment_config(deployment_config)
        
        # 准备部署环境
        await self._prepare_environment()
        
        # 部署数据库
        await self._deploy_databases(deployment_config['databases'])
        
        # 部署应用服务
        await self._deploy_services(deployment_config['services'])
        
        # 配置负载均衡
        await self._configure_load_balancer(deployment_config['load_balancer'])
        
        # 验证部署
        await self._verify_deployment()
        
        self.logger.info(f"部署到 {self.environment} 环境完成")
        
    async def _validate_deployment_config(self, config: Dict[str, Any]):
        """验证部署配置"""
        required_sections = ['databases', 'services', 'load_balancer']
        
        for section in required_sections:
            if section not in config:
                raise ValueError(f"缺少必需的配置节: {section}")
                
    async def _prepare_environment(self):
        """准备部署环境"""
        if self.environment == 'docker':
            await self._prepare_docker_environment()
        elif self.environment == 'kubernetes':
            await self._prepare_k8s_environment()
        elif self.environment == 'cloud':
            await self._prepare_cloud_environment()
            
    async def _deploy_databases(self, db_config: Dict[str, Any]):
        """部署数据库"""
        for db_name, config in db_config.items():
            await self._deploy_single_database(db_name, config)
            
    async def _deploy_services(self, services_config: Dict[str, Any]):
        """部署应用服务"""
        for service_name, config in services_config.items():
            await self._deploy_single_service(service_name, config)
            
    async def _configure_load_balancer(self, lb_config: Dict[str, Any]):
        """配置负载均衡器"""
        # 实现负载均衡器配置逻辑
        pass
        
    async def _verify_deployment(self):
        """验证部署"""
        # 实现部署验证逻辑
        pass

# 配置示例
SYSTEM_CONFIG = {
    "components": {
        "neo4j": {
            "type": "database",
            "uri": "bolt://localhost:7687",
            "username": "neo4j",
            "password": "password",
            "dependencies": [],
            "health_check_url": "http://localhost:7474/db/data/"
        },
        "postgresql": {
            "type": "database",
            "connection_string": "postgresql://user:pass@localhost/kg_db",
            "dependencies": [],
            "health_check_url": "http://localhost:5432/health"
        },
        "redis": {
            "type": "cache",
            "url": "redis://localhost:6379",
            "dependencies": [],
            "health_check_url": "http://localhost:6379/ping"
        },
        "elasticsearch": {
            "type": "search",
            "hosts": ["localhost:9200"],
            "dependencies": [],
            "health_check_url": "http://localhost:9200/_cluster/health"
        },
        "kg_api": {
            "type": "service",
            "port": 8000,
            "dependencies": ["neo4j", "postgresql", "redis", "elasticsearch"],
            "health_check_url": "http://localhost:8000/health"
        }
    },
    "data_sources": [
        {
            "name": "external_api",
            "type": "rest_api",
            "base_url": "https://api.example.com",
            "auth": {"type": "bearer", "token": "xxx"}
        }
    ],
    "notifications": {
        "email": {
            "smtp_server": "smtp.example.com",
            "port": 587,
            "username": "user@example.com",
            "password": "password"
        },
        "slack": {
            "webhook_url": "https://hooks.slack.com/xxx"
        }
    },
    "monitoring": {
        "prometheus": {
            "endpoint": "http://localhost:9090"
        },
        "grafana": {
            "url": "http://localhost:3000",
            "api_key": "xxx"
        }
    }
}

# 主函数
async def main():
    """主函数"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # 创建系统集成器
    integrator = KGSystemIntegrator("system_config.json")
    
    try:
        # 初始化系统
        await integrator.initialize_system()
        
        # 保持系统运行
        while True:
            await asyncio.sleep(60)
            
    except KeyboardInterrupt:
        logging.info("收到停止信号，正在关闭系统...")
    except Exception as e:
        logging.error(f"系统运行错误: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
```

这个系统集成指南提供了：

1. **完整的架构设计** - 包含数据层、服务层、应用层的整体架构
2. **组件集成管理** - 自动化的组件启动和依赖管理
3. **API集成层** - 统一的API网关和数据同步机制
4. **外部系统集成** - 支持多种数据源和第三方系统
5. **部署管理** - 自动化部署和配置管理
6. **健康检查** - 系统组件的健康状态监控
7. **配置管理** - 集中化的配置管理和环境适配

系统确保了知识图谱的完整集成和稳定运行。
