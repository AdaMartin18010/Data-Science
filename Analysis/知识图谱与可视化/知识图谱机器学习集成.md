# çŸ¥è¯†å›¾è°±æœºå™¨å­¦ä¹ é›†æˆ

## ğŸ“‘ ç›®å½•

- [çŸ¥è¯†å›¾è°±æœºå™¨å­¦ä¹ é›†æˆ](#çŸ¥è¯†å›¾è°±æœºå™¨å­¦ä¹ é›†æˆ)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. å›¾ç¥ç»ç½‘ç»œ (GNN)](#1-å›¾ç¥ç»ç½‘ç»œ-gnn)
    - [1.1. GNNåŸºç¡€æ¡†æ¶](#11-gnnåŸºç¡€æ¡†æ¶)
  - [2. çŸ¥è¯†å›¾è°±åµŒå…¥](#2-çŸ¥è¯†å›¾è°±åµŒå…¥)
    - [2.1. çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å‹](#21-çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å‹)
  - [3. æ¨èç³»ç»Ÿ](#3-æ¨èç³»ç»Ÿ)
    - [3.1. åŸºäºçŸ¥è¯†å›¾è°±çš„æ¨èç³»ç»Ÿ](#31-åŸºäºçŸ¥è¯†å›¾è°±çš„æ¨èç³»ç»Ÿ)
  - [4. å¼‚å¸¸æ£€æµ‹](#4-å¼‚å¸¸æ£€æµ‹)
    - [4.1. åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¼‚å¸¸æ£€æµ‹](#41-åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¼‚å¸¸æ£€æµ‹)
  - [5. å·¥å…·ä¸å¹³å°](#5-å·¥å…·ä¸å¹³å°)
    - [5.1. æœºå™¨å­¦ä¹ æ¡†æ¶](#51-æœºå™¨å­¦ä¹ æ¡†æ¶)
    - [5.2. çŸ¥è¯†å›¾è°±å·¥å…·](#52-çŸ¥è¯†å›¾è°±å·¥å…·)
    - [5.3. æ¨èç³»ç»Ÿå·¥å…·](#53-æ¨èç³»ç»Ÿå·¥å…·)
    - [5.4. å¼‚å¸¸æ£€æµ‹å·¥å…·](#54-å¼‚å¸¸æ£€æµ‹å·¥å…·)
  - [6. æœ€ä½³å®è·µ](#6-æœ€ä½³å®è·µ)
    - [6.1. æ¨¡å‹é€‰æ‹©](#61-æ¨¡å‹é€‰æ‹©)
    - [6.2. è®­ç»ƒç­–ç•¥](#62-è®­ç»ƒç­–ç•¥)
    - [6.3. è¯„ä¼°æŒ‡æ ‡](#63-è¯„ä¼°æŒ‡æ ‡)
    - [6.4. éƒ¨ç½²è€ƒè™‘](#64-éƒ¨ç½²è€ƒè™‘)

---


## 1. å›¾ç¥ç»ç½‘ç»œ (GNN)

### 1.1. GNNåŸºç¡€æ¡†æ¶

```python
# å›¾ç¥ç»ç½‘ç»œåŸºç¡€æ¡†æ¶
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, SAGEConv
from torch_geometric.data import Data, DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional
import networkx as nx

class GraphNeuralNetwork(nn.Module):
    """å›¾ç¥ç»ç½‘ç»œåŸºç¡€ç±»"""

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,
                 num_layers: int = 2, dropout: float = 0.5):
        super(GraphNeuralNetwork, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.dropout = dropout

# å›¾å·ç§¯å±‚
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(input_dim, hidden_dim))

        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))

        self.convs.append(GCNConv(hidden_dim, output_dim))

# æ‰¹å½’ä¸€åŒ–å±‚
        self.batch_norms = nn.ModuleList()
        for _ in range(num_layers):
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        for i in range(self.num_layers - 1):
            x = self.convs[i](x, edge_index)
            x = self.batch_norms[i](x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.convs[-1](x, edge_index)
        return x

class GraphAttentionNetwork(nn.Module):
    """å›¾æ³¨æ„åŠ›ç½‘ç»œ"""

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,
                 num_heads: int = 8, num_layers: int = 2, dropout: float = 0.5):
        super(GraphAttentionNetwork, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.dropout = dropout

# å›¾æ³¨æ„åŠ›å±‚
        self.gat_layers = nn.ModuleList()
        self.gat_layers.append(GATConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout))

        for _ in range(num_layers - 2):
            self.gat_layers.append(GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout))

        self.gat_layers.append(GATConv(hidden_dim * num_heads, output_dim, heads=1, dropout=dropout))

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        for i in range(self.num_layers - 1):
            x = self.gat_layers[i](x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.gat_layers[-1](x, edge_index)
        return x

class GraphSAGE(nn.Module):
    """GraphSAGEç½‘ç»œ"""

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int,
                 num_layers: int = 2, dropout: float = 0.5):
        super(GraphSAGE, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.dropout = dropout

# GraphSAGEå±‚
        self.sage_layers = nn.ModuleList()
        self.sage_layers.append(SAGEConv(input_dim, hidden_dim))

        for _ in range(num_layers - 2):
            self.sage_layers.append(SAGEConv(hidden_dim, hidden_dim))

        self.sage_layers.append(SAGEConv(hidden_dim, output_dim))

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        for i in range(self.num_layers - 1):
            x = self.sage_layers[i](x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.sage_layers[-1](x, edge_index)
        return x

# GNNè®­ç»ƒå™¨
class GNNTrainer:
    """GNNè®­ç»ƒå™¨"""

    def __init__(self, model: nn.Module, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
        self.criterion = nn.CrossEntropyLoss()

    def train_epoch(self, train_loader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for batch in train_loader:
            batch = batch.to(self.device)
            self.optimizer.zero_grad()

            out = self.model(batch.x, batch.edge_index)
            loss = self.criterion(out[batch.train_mask], batch.y[batch.train_mask])

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(train_loader)

    def evaluate(self, data_loader: DataLoader) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in data_loader:
                batch = batch.to(self.device)
                out = self.model(batch.x, batch.edge_index)

                loss = self.criterion(out[batch.val_mask], batch.y[batch.val_mask])
                total_loss += loss.item()

                pred = out.argmax(dim=1)
                correct += int((pred[batch.val_mask] == batch.y[batch.val_mask]).sum())
                total += int(batch.val_mask.sum())

        return total_loss / len(data_loader), correct / total

# ä½¿ç”¨ç¤ºä¾‹
def train_gnn_model():
    """è®­ç»ƒGNNæ¨¡å‹"""
# å‡†å¤‡æ•°æ®
# è¿™é‡Œéœ€è¦ä»çŸ¥è¯†å›¾è°±ä¸­æå–èŠ‚ç‚¹ç‰¹å¾å’Œè¾¹ä¿¡æ¯
# ç¤ºä¾‹æ•°æ®
    num_nodes = 1000
    num_features = 128
    num_classes = 10

# éšæœºç”Ÿæˆç¤ºä¾‹æ•°æ®
    x = torch.randn(num_nodes, num_features)
    edge_index = torch.randint(0, num_nodes, (2, 2000))
    y = torch.randint(0, num_classes, (num_nodes,))

# åˆ›å»ºæ•°æ®å¯¹è±¡
    data = Data(x=x, edge_index=edge_index, y=y)
    data.train_mask = torch.rand(num_nodes) < 0.8
    data.val_mask = torch.rand(num_nodes) < 0.1

# åˆ›å»ºæ¨¡å‹
    model = GraphNeuralNetwork(
        input_dim=num_features,
        hidden_dim=64,
        output_dim=num_classes
    )

# è®­ç»ƒæ¨¡å‹
    trainer = GNNTrainer(model)

    for epoch in range(100):
        loss = trainer.train_epoch([data])
        val_loss, val_acc = trainer.evaluate([data])

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss={loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")
```

## 2. çŸ¥è¯†å›¾è°±åµŒå…¥

### 2.1. çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å‹

```python
# çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å‹
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import numpy as np

class TransE(nn.Module):
    """TransEåµŒå…¥æ¨¡å‹"""

    def __init__(self, num_entities: int, num_relations: int, embedding_dim: int = 100):
        super(TransE, self).__init__()

        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim

# å®ä½“å’Œå…³ç³»åµŒå…¥
        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)
        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)

# åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)
        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)

    def forward(self, heads: torch.Tensor, relations: torch.Tensor, tails: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        head_embeddings = self.entity_embeddings(heads)
        relation_embeddings = self.relation_embeddings(relations)
        tail_embeddings = self.entity_embeddings(tails)

# TransE: h + r â‰ˆ t
        scores = head_embeddings + relation_embeddings - tail_embeddings
        return torch.norm(scores, p=2, dim=1)

    def predict(self, heads: torch.Tensor, relations: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹å°¾å®ä½“"""
        head_embeddings = self.entity_embeddings(heads)
        relation_embeddings = self.relation_embeddings(relations)

# è®¡ç®—æ‰€æœ‰å®ä½“çš„å¾—åˆ†
        all_entity_embeddings = self.entity_embeddings.weight
        scores = torch.norm(
            head_embeddings.unsqueeze(1) + relation_embeddings.unsqueeze(1) - all_entity_embeddings.unsqueeze(0),
            p=2, dim=2
        )

        return scores

class DistMult(nn.Module):
    """DistMultåµŒå…¥æ¨¡å‹"""

    def __init__(self, num_entities: int, num_relations: int, embedding_dim: int = 100):
        super(DistMult, self).__init__()

        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim

# å®ä½“å’Œå…³ç³»åµŒå…¥
        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)
        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)

# åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)
        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)

    def forward(self, heads: torch.Tensor, relations: torch.Tensor, tails: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        head_embeddings = self.entity_embeddings(heads)
        relation_embeddings = self.relation_embeddings(relations)
        tail_embeddings = self.entity_embeddings(tails)

# DistMult: <h, r, t>
        scores = torch.sum(head_embeddings * relation_embeddings * tail_embeddings, dim=1)
        return scores

    def predict(self, heads: torch.Tensor, relations: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹å°¾å®ä½“"""
        head_embeddings = self.entity_embeddings(heads)
        relation_embeddings = self.relation_embeddings(relations)

# è®¡ç®—æ‰€æœ‰å®ä½“çš„å¾—åˆ†
        all_entity_embeddings = self.entity_embeddings.weight
        scores = torch.sum(
            head_embeddings.unsqueeze(1) * relation_embeddings.unsqueeze(1) * all_entity_embeddings.unsqueeze(0),
            dim=2
        )

        return scores

class ComplEx(nn.Module):
    """ComplExåµŒå…¥æ¨¡å‹"""

    def __init__(self, num_entities: int, num_relations: int, embedding_dim: int = 100):
        super(ComplEx, self).__init__()

        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim

# å¤æ•°åµŒå…¥
        self.entity_real = nn.Embedding(num_entities, embedding_dim)
        self.entity_imag = nn.Embedding(num_entities, embedding_dim)
        self.relation_real = nn.Embedding(num_relations, embedding_dim)
        self.relation_imag = nn.Embedding(num_relations, embedding_dim)

# åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.entity_real.weight.data)
        nn.init.xavier_uniform_(self.entity_imag.weight.data)
        nn.init.xavier_uniform_(self.relation_real.weight.data)
        nn.init.xavier_uniform_(self.relation_imag.weight.data)

    def forward(self, heads: torch.Tensor, relations: torch.Tensor, tails: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        h_real = self.entity_real(heads)
        h_imag = self.entity_imag(heads)
        r_real = self.relation_real(relations)
        r_imag = self.relation_imag(relations)
        t_real = self.entity_real(tails)
        t_imag = self.entity_imag(tails)

# ComplEx: Re(<h, r, tÌ„>)
        real_score = h_real * r_real * t_real + h_real * r_imag * t_imag + h_imag * r_real * t_imag - h_imag * r_imag * t_real
        return torch.sum(real_score, dim=1)

    def predict(self, heads: torch.Tensor, relations: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹å°¾å®ä½“"""
        h_real = self.entity_real(heads)
        h_imag = self.entity_imag(heads)
        r_real = self.relation_real(relations)
        r_imag = self.relation_imag(relations)

# è®¡ç®—æ‰€æœ‰å®ä½“çš„å¾—åˆ†
        all_t_real = self.entity_real.weight
        all_t_imag = self.entity_imag.weight

        real_score = (h_real.unsqueeze(1) * r_real.unsqueeze(1) * all_t_real.unsqueeze(0) +
                     h_real.unsqueeze(1) * r_imag.unsqueeze(1) * all_t_imag.unsqueeze(0) +
                     h_imag.unsqueeze(1) * r_real.unsqueeze(1) * all_t_imag.unsqueeze(0) -
                     h_imag.unsqueeze(1) * r_imag.unsqueeze(1) * all_t_real.unsqueeze(0))

        return torch.sum(real_score, dim=2)

# çŸ¥è¯†å›¾è°±åµŒå…¥è®­ç»ƒå™¨
class KGEmbeddingTrainer:
    """çŸ¥è¯†å›¾è°±åµŒå…¥è®­ç»ƒå™¨"""

    def __init__(self, model: nn.Module, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        self.criterion = nn.MarginRankingLoss(margin=1.0)

    def generate_negative_samples(self, positive_triples: torch.Tensor, num_entities: int) -> torch.Tensor:
        """ç”Ÿæˆè´Ÿæ ·æœ¬"""
        negative_triples = positive_triples.clone()

# éšæœºæ›¿æ¢å¤´å®ä½“æˆ–å°¾å®ä½“
        for i in range(positive_triples.size(0)):
            if torch.rand(1) < 0.5:
# æ›¿æ¢å¤´å®ä½“
                negative_triples[i, 0] = torch.randint(0, num_entities, (1,))
            else:
# æ›¿æ¢å°¾å®ä½“
                negative_triples[i, 2] = torch.randint(0, num_entities, (1,))

        return negative_triples

    def train_epoch(self, positive_triples: torch.Tensor) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

# ç”Ÿæˆè´Ÿæ ·æœ¬
        negative_triples = self.generate_negative_samples(positive_triples, self.model.num_entities)

# è®¡ç®—æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„å¾—åˆ†
        pos_scores = self.model(positive_triples[:, 0], positive_triples[:, 1], positive_triples[:, 2])
        neg_scores = self.model(negative_triples[:, 0], negative_triples[:, 1], negative_triples[:, 2])

# è®¡ç®—æŸå¤±
        target = torch.ones(pos_scores.size(0)).to(self.device)
        loss = self.criterion(pos_scores, neg_scores, target)

# åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def evaluate(self, test_triples: torch.Tensor) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()

        with torch.no_grad():
# è®¡ç®—æµ‹è¯•ä¸‰å…ƒç»„çš„å¾—åˆ†
            scores = self.model(test_triples[:, 0], test_triples[:, 1], test_triples[:, 2])

# è®¡ç®—æ’åæŒ‡æ ‡
            ranks = self._calculate_ranks(test_triples, scores)

            metrics = {
                "mean_rank": np.mean(ranks),
                "hits@1": np.mean([1 if rank <= 1 else 0 for rank in ranks]),
                "hits@3": np.mean([1 if rank <= 3 else 0 for rank in ranks]),
                "hits@10": np.mean([1 if rank <= 10 else 0 for rank in ranks])
            }

        return metrics

    def _calculate_ranks(self, triples: torch.Tensor, scores: torch.Tensor) -> List[int]:
        """è®¡ç®—æ’å"""
        ranks = []

        for i, (head, relation, tail) in enumerate(triples):
# é¢„æµ‹æ‰€æœ‰å¯èƒ½çš„å°¾å®ä½“
            predictions = self.model.predict(head.unsqueeze(0), relation.unsqueeze(0))

# è®¡ç®—æ­£ç¡®å°¾å®ä½“çš„æ’å
            correct_score = predictions[0, tail]
            rank = (predictions[0] >= correct_score).sum().item()
            ranks.append(rank)

        return ranks

# ä½¿ç”¨ç¤ºä¾‹
def train_kg_embeddings():
    """è®­ç»ƒçŸ¥è¯†å›¾è°±åµŒå…¥"""
# å‡†å¤‡æ•°æ®
    num_entities = 1000
    num_relations = 50
    embedding_dim = 100

# ç”Ÿæˆç¤ºä¾‹ä¸‰å…ƒç»„
    num_triples = 5000
    triples = torch.randint(0, num_entities, (num_triples, 3))

# åˆ›å»ºæ¨¡å‹
    model = TransE(num_entities, num_relations, embedding_dim)
    trainer = KGEmbeddingTrainer(model)

# è®­ç»ƒæ¨¡å‹
    for epoch in range(100):
        loss = trainer.train_epoch(triples)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss={loss:.4f}")

# è¯„ä¼°æ¨¡å‹
    test_triples = torch.randint(0, num_entities, (100, 3))
    metrics = trainer.evaluate(test_triples)
    print(f"è¯„ä¼°ç»“æœ: {metrics}")
```

## 3. æ¨èç³»ç»Ÿ

### 3.1. åŸºäºçŸ¥è¯†å›¾è°±çš„æ¨èç³»ç»Ÿ

```python
# åŸºäºçŸ¥è¯†å›¾è°±çš„æ¨èç³»ç»Ÿ
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import numpy as np

class KGRecommender(nn.Module):
    """åŸºäºçŸ¥è¯†å›¾è°±çš„æ¨èç³»ç»Ÿ"""

    def __init__(self, num_users: int, num_items: int, num_entities: int, num_relations: int,
                 embedding_dim: int = 100, hidden_dim: int = 64):
        super(KGRecommender, self).__init__()

        self.num_users = num_users
        self.num_items = num_items
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

# ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        self.user_embeddings = nn.Embedding(num_users, embedding_dim)
        self.item_embeddings = nn.Embedding(num_items, embedding_dim)

# çŸ¥è¯†å›¾è°±åµŒå…¥
        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)
        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)

# æ¨èç½‘ç»œ
        self.recommendation_net = nn.Sequential(
            nn.Linear(embedding_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )

# åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.user_embeddings.weight.data)
        nn.init.xavier_uniform_(self.item_embeddings.weight.data)
        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)
        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)

    def forward(self, user_ids: torch.Tensor, item_ids: torch.Tensor,
                kg_triples: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
# è·å–ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        user_embeddings = self.user_embeddings(user_ids)
        item_embeddings = self.item_embeddings(item_ids)

# è·å–çŸ¥è¯†å›¾è°±ä¿¡æ¯
        kg_embeddings = self._get_kg_embeddings(kg_triples)

# èåˆç”¨æˆ·-ç‰©å“åµŒå…¥å’ŒçŸ¥è¯†å›¾è°±åµŒå…¥
        combined_embeddings = torch.cat([user_embeddings, item_embeddings + kg_embeddings], dim=1)

# é¢„æµ‹è¯„åˆ†
        scores = self.recommendation_net(combined_embeddings)
        return scores.squeeze()

    def _get_kg_embeddings(self, kg_triples: torch.Tensor) -> torch.Tensor:
        """è·å–çŸ¥è¯†å›¾è°±åµŒå…¥"""
        if kg_triples.size(0) == 0:
            return torch.zeros(kg_triples.size(0), self.embedding_dim).to(kg_triples.device)

        head_embeddings = self.entity_embeddings(kg_triples[:, 0])
        relation_embeddings = self.relation_embeddings(kg_triples[:, 1])
        tail_embeddings = self.entity_embeddings(kg_triples[:, 2])

# è®¡ç®—çŸ¥è¯†å›¾è°±åµŒå…¥
        kg_embeddings = head_embeddings + relation_embeddings + tail_embeddings
        return kg_embeddings.mean(dim=0) if kg_embeddings.size(0) > 0 else torch.zeros(self.embedding_dim).to(kg_triples.device)

class KGRecommenderTrainer:
    """çŸ¥è¯†å›¾è°±æ¨èç³»ç»Ÿè®­ç»ƒå™¨"""

    def __init__(self, model: KGRecommender, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        self.criterion = nn.BCELoss()

    def train_epoch(self, user_item_pairs: torch.Tensor, ratings: torch.Tensor,
                   kg_triples: torch.Tensor) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for i in range(0, len(user_item_pairs), 32):
            batch_users = user_item_pairs[i:i+32, 0]
            batch_items = user_item_pairs[i:i+32, 1]
            batch_ratings = ratings[i:i+32]
            batch_kg_triples = kg_triples[i:i+32] if i < len(kg_triples) else torch.empty(0, 3)

# å‰å‘ä¼ æ’­
            predictions = self.model(batch_users, batch_items, batch_kg_triples)
            loss = self.criterion(predictions, batch_ratings.float())

# åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / (len(user_item_pairs) // 32)

    def recommend(self, user_id: int, top_k: int = 10) -> List[Tuple[int, float]]:
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        self.model.eval()

        with torch.no_grad():
            user_tensor = torch.tensor([user_id]).to(self.device)

# ä¸ºæ‰€æœ‰ç‰©å“è®¡ç®—æ¨èåˆ†æ•°
            all_items = torch.arange(self.model.num_items).to(self.device)
            all_users = user_tensor.repeat(self.model.num_items)

# è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„KGä¸‰å…ƒç»„æ¥è·å–ç›¸å…³ä¸‰å…ƒç»„
# ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ç©ºçš„ä¸‰å…ƒç»„
            kg_triples = torch.empty(0, 3).to(self.device)

            scores = self.model(all_users, all_items, kg_triples)

# è·å–top-kæ¨è
            top_scores, top_indices = torch.topk(scores, top_k)

            recommendations = [(item_id.item(), score.item())
                             for item_id, score in zip(top_indices, top_scores)]

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
def train_kg_recommender():
    """è®­ç»ƒçŸ¥è¯†å›¾è°±æ¨èç³»ç»Ÿ"""
# å‡†å¤‡æ•°æ®
    num_users = 1000
    num_items = 500
    num_entities = 2000
    num_relations = 100

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
    num_interactions = 10000
    user_item_pairs = torch.randint(0, num_users, (num_interactions, 2))
    ratings = torch.rand(num_interactions) > 0.5  # äºŒå€¼è¯„åˆ†

# ç”ŸæˆçŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„
    num_kg_triples = 5000
    kg_triples = torch.randint(0, num_entities, (num_kg_triples, 3))

# åˆ›å»ºæ¨¡å‹
    model = KGRecommender(num_users, num_items, num_entities, num_relations)
    trainer = KGRecommenderTrainer(model)

# è®­ç»ƒæ¨¡å‹
    for epoch in range(50):
        loss = trainer.train_epoch(user_item_pairs, ratings, kg_triples)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss={loss:.4f}")

# ç”Ÿæˆæ¨è
    recommendations = trainer.recommend(user_id=0, top_k=10)
    print(f"ç”¨æˆ·0çš„æ¨è: {recommendations}")
```

## 4. å¼‚å¸¸æ£€æµ‹

### 4.1. åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¼‚å¸¸æ£€æµ‹

```python
# åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¼‚å¸¸æ£€æµ‹
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from typing import Dict, List, Tuple, Optional
import numpy as np

class GraphAnomalyDetector(nn.Module):
    """å›¾å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int = 2):
        super(GraphAnomalyDetector, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

# å›¾å·ç§¯å±‚
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(input_dim, hidden_dim))

        for _ in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))

# å¼‚å¸¸æ£€æµ‹å¤´
        self.anomaly_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )

# é‡æ„å¤´
        self.reconstruction_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
# å›¾å·ç§¯ç¼–ç 
        h = x
        for conv in self.convs:
            h = conv(h, edge_index)
            h = F.relu(h)

# å¼‚å¸¸æ£€æµ‹
        anomaly_scores = self.anomaly_head(h)

# é‡æ„
        reconstructed = self.reconstruction_head(h)

        return anomaly_scores.squeeze(), reconstructed

    def detect_anomalies(self, x: torch.Tensor, edge_index: torch.Tensor,
                        threshold: float = 0.5) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomaly_scores, reconstructed = self.forward(x, edge_index)

# è®¡ç®—é‡æ„è¯¯å·®
        reconstruction_error = F.mse_loss(reconstructed, x, reduction='none').mean(dim=1)

# ç»¼åˆå¼‚å¸¸åˆ†æ•°
        combined_scores = anomaly_scores + reconstruction_error

# æ£€æµ‹å¼‚å¸¸
        anomalies = combined_scores > threshold

        return anomalies, combined_scores

class AnomalyDetectorTrainer:
    """å¼‚å¸¸æ£€æµ‹è®­ç»ƒå™¨"""

    def __init__(self, model: GraphAnomalyDetector, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    def train_epoch(self, data_loader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for batch in data_loader:
            batch = batch.to(self.device)
            self.optimizer.zero_grad()

# å‰å‘ä¼ æ’­
            anomaly_scores, reconstructed = self.model(batch.x, batch.edge_index)

# è®¡ç®—æŸå¤±
            reconstruction_loss = F.mse_loss(reconstructed, batch.x)
            anomaly_loss = F.binary_cross_entropy(anomaly_scores, torch.zeros_like(anomaly_scores))

            total_loss = reconstruction_loss + 0.1 * anomaly_loss

# åå‘ä¼ æ’­
            total_loss.backward()
            self.optimizer.step()

        return total_loss.item()

    def evaluate(self, data_loader) -> Dict[str, float]:
        """è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ€§èƒ½"""
        self.model.eval()

        all_anomalies = []
        all_scores = []
        all_labels = []

        with torch.no_grad():
            for batch in data_loader:
                batch = batch.to(self.device)

                anomalies, scores = self.model.detect_anomalies(batch.x, batch.edge_index)

                all_anomalies.extend(anomalies.cpu().numpy())
                all_scores.extend(scores.cpu().numpy())

# å‡è®¾æœ‰æ ‡ç­¾
                if hasattr(batch, 'y'):
                    all_labels.extend(batch.y.cpu().numpy())

# è®¡ç®—æŒ‡æ ‡
        if all_labels:
            from sklearn.metrics import precision_recall_fscore_support, roc_auc_score

            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_anomalies, average='binary')
            auc = roc_auc_score(all_labels, all_scores)

            return {
                "precision": precision,
                "recall": recall,
                "f1": f1,
                "auc": auc
            }
        else:
            return {
                "anomaly_rate": np.mean(all_anomalies),
                "mean_score": np.mean(all_scores)
            }

# ä½¿ç”¨ç¤ºä¾‹
def train_anomaly_detector():
    """è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨"""
# å‡†å¤‡æ•°æ®
    num_nodes = 1000
    num_features = 128

# ç”Ÿæˆæ­£å¸¸æ•°æ®
    x = torch.randn(num_nodes, num_features)
    edge_index = torch.randint(0, num_nodes, (2, 2000))

# æ·»åŠ ä¸€äº›å¼‚å¸¸èŠ‚ç‚¹
    anomaly_indices = torch.randperm(num_nodes)[:50]  # 5%çš„å¼‚å¸¸èŠ‚ç‚¹
    x[anomaly_indices] += torch.randn(50, num_features) * 2  # æ·»åŠ å™ªå£°

# åˆ›å»ºæ•°æ®å¯¹è±¡
    data = Data(x=x, edge_index=edge_index)

# åˆ›å»ºæ¨¡å‹
    model = GraphAnomalyDetector(input_dim=num_features, hidden_dim=64)
    trainer = AnomalyDetectorTrainer(model)

# è®­ç»ƒæ¨¡å‹
    for epoch in range(100):
        loss = trainer.train_epoch([data])

        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Loss={loss:.4f}")

# æ£€æµ‹å¼‚å¸¸
    anomalies, scores = model.detect_anomalies(data.x, data.edge_index)
    print(f"æ£€æµ‹åˆ°çš„å¼‚å¸¸èŠ‚ç‚¹æ•°é‡: {anomalies.sum().item()}")
    print(f"å¼‚å¸¸åˆ†æ•°èŒƒå›´: {scores.min().item():.4f} - {scores.max().item():.4f}")
```

## 5. å·¥å…·ä¸å¹³å°

### 5.1. æœºå™¨å­¦ä¹ æ¡†æ¶

1. **PyTorch Geometric**ï¼šå›¾ç¥ç»ç½‘ç»œåº“
2. **DGL**ï¼šæ·±åº¦å›¾åº“
3. **Spektral**ï¼šKeraså›¾ç¥ç»ç½‘ç»œ
4. **NetworkX**ï¼šå›¾åˆ†æåº“

### 5.2. çŸ¥è¯†å›¾è°±å·¥å…·

1. **OpenKE**ï¼šçŸ¥è¯†å›¾è°±åµŒå…¥æ¡†æ¶
2. **DGL-KE**ï¼šåˆ†å¸ƒå¼çŸ¥è¯†å›¾è°±åµŒå…¥
3. **PyKEEN**ï¼šPythonçŸ¥è¯†å›¾è°±åµŒå…¥
4. **AmpliGraph**ï¼šçŸ¥è¯†å›¾è°±åµŒå…¥åº“

### 5.3. æ¨èç³»ç»Ÿå·¥å…·

1. **Surprise**ï¼šæ¨èç³»ç»Ÿåº“
2. **LightFM**ï¼šæ··åˆæ¨èç³»ç»Ÿ
3. **RecBole**ï¼šæ¨èç³»ç»ŸåŸºå‡†
4. **DeepCTR**ï¼šæ·±åº¦CTRé¢„æµ‹

### 5.4. å¼‚å¸¸æ£€æµ‹å·¥å…·

1. **PyOD**ï¼šå¼‚å¸¸æ£€æµ‹åº“
2. **Isolation Forest**ï¼šéš”ç¦»æ£®æ—
3. **LOF**ï¼šå±€éƒ¨å¼‚å¸¸å› å­
4. **One-Class SVM**ï¼šä¸€ç±»æ”¯æŒå‘é‡æœº

## 6. æœ€ä½³å®è·µ

### 6.1. æ¨¡å‹é€‰æ‹©

1. **ä»»åŠ¡åŒ¹é…**ï¼šæ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹
2. **æ•°æ®è§„æ¨¡**ï¼šè€ƒè™‘æ•°æ®è§„æ¨¡é€‰æ‹©æ¨¡å‹å¤æ‚åº¦
3. **è®¡ç®—èµ„æº**ï¼šæ ¹æ®å¯ç”¨èµ„æºé€‰æ‹©æ¨¡å‹
4. **å¯è§£é‡Šæ€§**ï¼šè€ƒè™‘æ¨¡å‹çš„å¯è§£é‡Šæ€§éœ€æ±‚

### 6.2. è®­ç»ƒç­–ç•¥

1. **æ•°æ®é¢„å¤„ç†**ï¼šæ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–
2. **è¶…å‚æ•°è°ƒä¼˜**ï¼šç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–
3. **æ­£åˆ™åŒ–**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
4. **æ—©åœ**ï¼šé¿å…è¿‡åº¦è®­ç»ƒ

### 6.3. è¯„ä¼°æŒ‡æ ‡

1. **åˆ†ç±»ä»»åŠ¡**ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
2. **æ¨èä»»åŠ¡**ï¼šNDCGã€MAPã€MRR
3. **å¼‚å¸¸æ£€æµ‹**ï¼šAUCã€ç²¾ç¡®ç‡ã€å¬å›ç‡
4. **çŸ¥è¯†å›¾è°±**ï¼šHits@Kã€Mean Rank

### 6.4. éƒ¨ç½²è€ƒè™‘

1. **æ¨¡å‹å‹ç¼©**ï¼šæ¨¡å‹é‡åŒ–å’Œå‰ªæ
2. **åœ¨çº¿å­¦ä¹ **ï¼šå¢é‡æ›´æ–°æ¨¡å‹
3. **A/Bæµ‹è¯•**ï¼šæ¨¡å‹æ•ˆæœå¯¹æ¯”
4. **ç›‘æ§å‘Šè­¦**ï¼šæ¨¡å‹æ€§èƒ½ç›‘æ§
