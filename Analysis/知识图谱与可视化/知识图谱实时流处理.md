# 知识图谱实时流处理

## 实时流处理架构

### 1. 流处理核心框架

```python
import asyncio
import json
import logging
from typing import Dict, List, Any, Optional, Callable, AsyncGenerator
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
import uuid
from abc import ABC, abstractmethod

class EventType(Enum):
    """事件类型"""
    NODE_CREATED = "node_created"
    NODE_UPDATED = "node_updated"
    NODE_DELETED = "node_deleted"
    RELATIONSHIP_CREATED = "relationship_created"
    RELATIONSHIP_UPDATED = "relationship_updated"
    RELATIONSHIP_DELETED = "relationship_deleted"
    SCHEMA_CHANGED = "schema_changed"
    QUERY_EXECUTED = "query_executed"

@dataclass
class StreamEvent:
    """流事件数据结构"""
    id: str
    event_type: EventType
    timestamp: datetime
    source: str
    data: Dict[str, Any]
    metadata: Dict[str, Any]
    correlation_id: Optional[str] = None

class StreamProcessor(ABC):
    """流处理器基类"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"{__name__}.{name}")
        
    @abstractmethod
    async def process(self, event: StreamEvent) -> Optional[StreamEvent]:
        """处理流事件"""
        pass
        
    @abstractmethod
    async def setup(self):
        """设置处理器"""
        pass
        
    @abstractmethod
    async def cleanup(self):
        """清理处理器"""
        pass

class KGStreamEngine:
    """知识图谱流处理引擎"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.processors: List[StreamProcessor] = []
        self.event_queue = asyncio.Queue(maxsize=config.get('queue_size', 1000))
        self.running = False
        self.metrics = {
            'events_processed': 0,
            'events_failed': 0,
            'processing_time_total': 0.0
        }
        
    async def start(self):
        """启动流处理引擎"""
        self.logger.info("启动知识图谱流处理引擎")
        self.running = True
        
        # 初始化所有处理器
        for processor in self.processors:
            await processor.setup()
            
        # 启动处理循环
        asyncio.create_task(self._processing_loop())
        
    async def stop(self):
        """停止流处理引擎"""
        self.logger.info("停止知识图谱流处理引擎")
        self.running = False
        
        # 清理所有处理器
        for processor in self.processors:
            await processor.cleanup()
            
    def add_processor(self, processor: StreamProcessor):
        """添加流处理器"""
        self.processors.append(processor)
        self.logger.info(f"添加流处理器: {processor.name}")
        
    async def publish_event(self, event: StreamEvent):
        """发布事件到流"""
        try:
            await self.event_queue.put(event)
            self.logger.debug(f"发布事件: {event.event_type.value}")
        except asyncio.QueueFull:
            self.logger.error("事件队列已满，丢弃事件")
            self.metrics['events_failed'] += 1
            
    async def _processing_loop(self):
        """处理循环"""
        while self.running:
            try:
                # 获取事件
                event = await asyncio.wait_for(
                    self.event_queue.get(), 
                    timeout=1.0
                )
                
                # 处理事件
                await self._process_event(event)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                self.logger.error(f"处理循环错误: {e}")
                
    async def _process_event(self, event: StreamEvent):
        """处理单个事件"""
        start_time = asyncio.get_event_loop().time()
        
        try:
            # 依次通过所有处理器
            current_event = event
            
            for processor in self.processors:
                if current_event is None:
                    break
                    
                try:
                    current_event = await processor.process(current_event)
                except Exception as e:
                    self.logger.error(f"处理器 {processor.name} 处理失败: {e}")
                    self.metrics['events_failed'] += 1
                    return
                    
            self.metrics['events_processed'] += 1
            
        except Exception as e:
            self.logger.error(f"事件处理失败: {e}")
            self.metrics['events_failed'] += 1
            
        finally:
            processing_time = asyncio.get_event_loop().time() - start_time
            self.metrics['processing_time_total'] += processing_time
```

### 2. 数据变更捕获

```python
class ChangeDataCaptureProcessor(StreamProcessor):
    """变更数据捕获处理器"""
    
    def __init__(self, kg_client, change_log_storage):
        super().__init__("CDC")
        self.kg_client = kg_client
        self.change_log = change_log_storage
        
    async def setup(self):
        """设置CDC处理器"""
        # 初始化变更日志存储
        await self.change_log.initialize()
        
    async def cleanup(self):
        """清理CDC处理器"""
        await self.change_log.close()
        
    async def process(self, event: StreamEvent) -> Optional[StreamEvent]:
        """处理变更事件"""
        # 记录变更到日志
        change_record = {
            'event_id': event.id,
            'event_type': event.event_type.value,
            'timestamp': event.timestamp.isoformat(),
            'source': event.source,
            'data': event.data,
            'metadata': event.metadata
        }
        
        await self.change_log.append_change(change_record)
        
        # 更新知识图谱
        await self._apply_change_to_kg(event)
        
        return event
        
    async def _apply_change_to_kg(self, event: StreamEvent):
        """将变更应用到知识图谱"""
        try:
            if event.event_type == EventType.NODE_CREATED:
                await self._handle_node_creation(event.data)
            elif event.event_type == EventType.NODE_UPDATED:
                await self._handle_node_update(event.data)
            elif event.event_type == EventType.NODE_DELETED:
                await self._handle_node_deletion(event.data)
            elif event.event_type == EventType.RELATIONSHIP_CREATED:
                await self._handle_relationship_creation(event.data)
            elif event.event_type == EventType.RELATIONSHIP_UPDATED:
                await self._handle_relationship_update(event.data)
            elif event.event_type == EventType.RELATIONSHIP_DELETED:
                await self._handle_relationship_deletion(event.data)
                
        except Exception as e:
            self.logger.error(f"应用变更到知识图谱失败: {e}")
            raise
            
    async def _handle_node_creation(self, data: Dict[str, Any]):
        """处理节点创建"""
        node_id = data.get('id')
        label = data.get('label')
        properties = data.get('properties', {})
        
        await self.kg_client.create_node_with_id(node_id, label, properties)
        
    async def _handle_node_update(self, data: Dict[str, Any]):
        """处理节点更新"""
        node_id = data.get('id')
        properties = data.get('properties', {})
        
        await self.kg_client.update_node(node_id, properties)
        
    async def _handle_node_deletion(self, data: Dict[str, Any]):
        """处理节点删除"""
        node_id = data.get('id')
        await self.kg_client.delete_node(node_id)
        
    async def _handle_relationship_creation(self, data: Dict[str, Any]):
        """处理关系创建"""
        rel_id = data.get('id')
        from_id = data.get('from_id')
        to_id = data.get('to_id')
        rel_type = data.get('type')
        properties = data.get('properties', {})
        
        await self.kg_client.create_relationship_with_id(
            rel_id, from_id, to_id, rel_type, properties
        )
        
    async def _handle_relationship_update(self, data: Dict[str, Any]):
        """处理关系更新"""
        rel_id = data.get('id')
        properties = data.get('properties', {})
        
        await self.kg_client.update_relationship(rel_id, properties)
        
    async def _handle_relationship_deletion(self, data: Dict[str, Any]):
        """处理关系删除"""
        rel_id = data.get('id')
        await self.kg_client.delete_relationship(rel_id)

class RealTimeIndexProcessor(StreamProcessor):
    """实时索引处理器"""
    
    def __init__(self, search_index_client):
        super().__init__("RealTimeIndex")
        self.search_client = search_index_client
        
    async def setup(self):
        """设置索引处理器"""
        await self.search_client.connect()
        
    async def cleanup(self):
        """清理索引处理器"""
        await self.search_client.disconnect()
        
    async def process(self, event: StreamEvent) -> Optional[StreamEvent]:
        """处理索引更新事件"""
        try:
            if event.event_type in [EventType.NODE_CREATED, EventType.NODE_UPDATED]:
                await self._update_node_index(event.data)
            elif event.event_type == EventType.NODE_DELETED:
                await self._delete_from_index(event.data.get('id'))
            elif event.event_type in [EventType.RELATIONSHIP_CREATED, EventType.RELATIONSHIP_UPDATED]:
                await self._update_relationship_index(event.data)
            elif event.event_type == EventType.RELATIONSHIP_DELETED:
                await self._delete_from_index(event.data.get('id'))
                
        except Exception as e:
            self.logger.error(f"索引更新失败: {e}")
            
        return event
        
    async def _update_node_index(self, data: Dict[str, Any]):
        """更新节点索引"""
        doc = {
            'id': data.get('id'),
            'type': 'node',
            'label': data.get('label'),
            'properties': data.get('properties', {}),
            'updated_at': datetime.now().isoformat()
        }
        
        await self.search_client.index_document(doc)
        
    async def _update_relationship_index(self, data: Dict[str, Any]):
        """更新关系索引"""
        doc = {
            'id': data.get('id'),
            'type': 'relationship',
            'from_id': data.get('from_id'),
            'to_id': data.get('to_id'),
            'rel_type': data.get('type'),
            'properties': data.get('properties', {}),
            'updated_at': datetime.now().isoformat()
        }
        
        await self.search_client.index_document(doc)
        
    async def _delete_from_index(self, doc_id: str):
        """从索引中删除"""
        await self.search_client.delete_document(doc_id)
```

### 3. 实时分析处理器

```python
class RealTimeAnalyticsProcessor(StreamProcessor):
    """实时分析处理器"""
    
    def __init__(self, analytics_storage, window_size: int = 60):
        super().__init__("RealTimeAnalytics")
        self.analytics_storage = analytics_storage
        self.window_size = window_size  # 时间窗口大小（秒）
        self.metrics_buffer = {}
        
    async def setup(self):
        """设置分析处理器"""
        await self.analytics_storage.initialize()
        
        # 启动定期聚合任务
        asyncio.create_task(self._periodic_aggregation())
        
    async def cleanup(self):
        """清理分析处理器"""
        await self.analytics_storage.close()
        
    async def process(self, event: StreamEvent) -> Optional[StreamEvent]:
        """处理分析事件"""
        # 更新实时指标
        await self._update_real_time_metrics(event)
        
        # 检测异常模式
        await self._detect_anomalies(event)
        
        # 更新统计信息
        await self._update_statistics(event)
        
        return event
        
    async def _update_real_time_metrics(self, event: StreamEvent):
        """更新实时指标"""
        current_time = datetime.now()
        time_bucket = int(current_time.timestamp()) // self.window_size
        
        if time_bucket not in self.metrics_buffer:
            self.metrics_buffer[time_bucket] = {
                'event_counts': {},
                'node_operations': 0,
                'relationship_operations': 0,
                'total_events': 0
            }
            
        bucket = self.metrics_buffer[time_bucket]
        
        # 更新事件计数
        event_type_str = event.event_type.value
        bucket['event_counts'][event_type_str] = bucket['event_counts'].get(event_type_str, 0) + 1
        bucket['total_events'] += 1
        
        # 更新操作类型计数
        if 'node' in event_type_str:
            bucket['node_operations'] += 1
        elif 'relationship' in event_type_str:
            bucket['relationship_operations'] += 1
            
    async def _detect_anomalies(self, event: StreamEvent):
        """检测异常模式"""
        # 检测高频操作
        recent_events = await self._get_recent_events(60)  # 最近1分钟
        
        if len(recent_events) > 100:  # 每分钟超过100个事件
            await self._trigger_anomaly_alert({
                'type': 'high_frequency_operations',
                'event_count': len(recent_events),
                'time_window': '1_minute'
            })
            
        # 检测批量删除
        recent_deletes = [
            e for e in recent_events 
            if e.event_type in [EventType.NODE_DELETED, EventType.RELATIONSHIP_DELETED]
        ]
        
        if len(recent_deletes) > 20:  # 1分钟内超过20个删除操作
            await self._trigger_anomaly_alert({
                'type': 'mass_deletion',
                'delete_count': len(recent_deletes),
                'time_window': '1_minute'
            })
            
    async def _update_statistics(self, event: StreamEvent):
        """更新统计信息"""
        stats = {
            'event_type': event.event_type.value,
            'source': event.source,
            'timestamp': event.timestamp.isoformat(),
            'processing_time': datetime.now().isoformat()
        }
        
        # 如果是节点或关系操作，记录额外信息
        if event.event_type in [EventType.NODE_CREATED, EventType.NODE_UPDATED]:
            stats['node_label'] = event.data.get('label')
        elif event.event_type in [EventType.RELATIONSHIP_CREATED, EventType.RELATIONSHIP_UPDATED]:
            stats['relationship_type'] = event.data.get('type')
            
        await self.analytics_storage.record_event_stats(stats)
        
    async def _periodic_aggregation(self):
        """定期聚合指标"""
        while True:
            try:
                current_time = datetime.now()
                current_bucket = int(current_time.timestamp()) // self.window_size
                
                # 聚合完成的时间桶
                completed_buckets = [
                    bucket for bucket in self.metrics_buffer.keys() 
                    if bucket < current_bucket - 1
                ]
                
                for bucket in completed_buckets:
                    bucket_data = self.metrics_buffer.pop(bucket)
                    bucket_timestamp = datetime.fromtimestamp(bucket * self.window_size)
                    
                    # 保存聚合指标
                    await self.analytics_storage.save_aggregated_metrics(
                        bucket_timestamp, bucket_data
                    )
                    
                await asyncio.sleep(self.window_size)
                
            except Exception as e:
                self.logger.error(f"定期聚合失败: {e}")
                await asyncio.sleep(10)
                
    async def _get_recent_events(self, seconds: int) -> List[StreamEvent]:
        """获取最近的事件"""
        # 这里应该从事件存储中获取最近的事件
        # 简化实现，返回空列表
        return []
        
    async def _trigger_anomaly_alert(self, anomaly: Dict[str, Any]):
        """触发异常告警"""
        self.logger.warning(f"检测到异常: {anomaly}")
        
        # 实际实现中应该发送告警通知
        # await self.notification_service.send_alert(anomaly)

class EventCorrelationProcessor(StreamProcessor):
    """事件关联处理器"""
    
    def __init__(self, correlation_window: int = 300):
        super().__init__("EventCorrelation")
        self.correlation_window = correlation_window  # 关联窗口（秒）
        self.correlation_buffer = {}
        
    async def setup(self):
        """设置关联处理器"""
        # 启动定期清理任务
        asyncio.create_task(self._periodic_cleanup())
        
    async def cleanup(self):
        """清理关联处理器"""
        pass
        
    async def process(self, event: StreamEvent) -> Optional[StreamEvent]:
        """处理事件关联"""
        # 查找相关事件
        related_events = await self._find_related_events(event)
        
        if related_events:
            # 生成关联事件
            correlation_event = await self._create_correlation_event(event, related_events)
            
            # 添加关联信息到原事件
            event.metadata['correlations'] = [e.id for e in related_events]
            event.metadata['correlation_patterns'] = await self._analyze_patterns(
                [event] + related_events
            )
            
        # 将事件添加到关联缓冲区
        await self._add_to_correlation_buffer(event)
        
        return event
        
    async def _find_related_events(self, event: StreamEvent) -> List[StreamEvent]:
        """查找相关事件"""
        related_events = []
        current_time = event.timestamp
        
        # 在时间窗口内查找相关事件
        for buffered_events in self.correlation_buffer.values():
            for buffered_event in buffered_events:
                time_diff = abs((current_time - buffered_event.timestamp).total_seconds())
                
                if time_diff <= self.correlation_window:
                    if await self._are_events_related(event, buffered_event):
                        related_events.append(buffered_event)
                        
        return related_events
        
    async def _are_events_related(self, event1: StreamEvent, event2: StreamEvent) -> bool:
        """判断两个事件是否相关"""
        # 相同的关联ID
        if (event1.correlation_id and event2.correlation_id and 
            event1.correlation_id == event2.correlation_id):
            return True
            
        # 相同的数据实体
        if (event1.data.get('id') and event2.data.get('id') and
            event1.data['id'] == event2.data['id']):
            return True
            
        # 关系事件的端点匹配
        if (event1.event_type.name.startswith('RELATIONSHIP') and
            event2.event_type.name.startswith('NODE')):
            if (event1.data.get('from_id') == event2.data.get('id') or
                event1.data.get('to_id') == event2.data.get('id')):
                return True
                
        return False
        
    async def _create_correlation_event(self, event: StreamEvent, related_events: List[StreamEvent]) -> StreamEvent:
        """创建关联事件"""
        correlation_data = {
            'primary_event': event.id,
            'related_events': [e.id for e in related_events],
            'pattern_type': await self._determine_pattern_type(event, related_events),
            'correlation_strength': await self._calculate_correlation_strength(event, related_events)
        }
        
        return StreamEvent(
            id=str(uuid.uuid4()),
            event_type=EventType.NODE_CREATED,  # 使用通用类型
            timestamp=datetime.now(),
            source="correlation_processor",
            data=correlation_data,
            metadata={'event_correlation': True}
        )
        
    async def _analyze_patterns(self, events: List[StreamEvent]) -> List[str]:
        """分析事件模式"""
        patterns = []
        
        # 检测事务模式
        if len(events) > 1 and all(e.correlation_id for e in events):
            patterns.append("transaction_pattern")
            
        # 检测级联操作模式
        node_events = [e for e in events if 'node' in e.event_type.value]
        rel_events = [e for e in events if 'relationship' in e.event_type.value]
        
        if node_events and rel_events:
            patterns.append("cascade_operation")
            
        return patterns
        
    async def _add_to_correlation_buffer(self, event: StreamEvent):
        """添加事件到关联缓冲区"""
        event_type = event.event_type.value
        
        if event_type not in self.correlation_buffer:
            self.correlation_buffer[event_type] = []
            
        self.correlation_buffer[event_type].append(event)
        
    async def _periodic_cleanup(self):
        """定期清理过期事件"""
        while True:
            try:
                current_time = datetime.now()
                
                for event_type, events in self.correlation_buffer.items():
                    # 移除超过关联窗口的事件
                    self.correlation_buffer[event_type] = [
                        event for event in events
                        if (current_time - event.timestamp).total_seconds() <= self.correlation_window
                    ]
                    
                await asyncio.sleep(60)  # 每分钟清理一次
                
            except Exception as e:
                self.logger.error(f"定期清理失败: {e}")
                await asyncio.sleep(10)
```

### 4. 流数据源集成

```python
class StreamDataSource(ABC):
    """流数据源基类"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{name}")
        
    @abstractmethod
    async def connect(self):
        """连接数据源"""
        pass
        
    @abstractmethod
    async def disconnect(self):
        """断开数据源"""
        pass
        
    @abstractmethod
    async def consume_events(self) -> AsyncGenerator[StreamEvent, None]:
        """消费事件流"""
        pass

class KafkaStreamSource(StreamDataSource):
    """Kafka流数据源"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__("Kafka", config)
        self.consumer = None
        
    async def connect(self):
        """连接Kafka"""
        from aiokafka import AIOKafkaConsumer
        
        self.consumer = AIOKafkaConsumer(
            self.config['topic'],
            bootstrap_servers=self.config['bootstrap_servers'],
            group_id=self.config.get('group_id', 'kg_stream_processor'),
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        
        await self.consumer.start()
        self.logger.info("Kafka消费者已启动")
        
    async def disconnect(self):
        """断开Kafka"""
        if self.consumer:
            await self.consumer.stop()
            self.logger.info("Kafka消费者已停止")
            
    async def consume_events(self) -> AsyncGenerator[StreamEvent, None]:
        """消费Kafka事件"""
        if not self.consumer:
            raise RuntimeError("Kafka消费者未连接")
            
        async for message in self.consumer:
            try:
                event_data = message.value
                
                event = StreamEvent(
                    id=event_data.get('id', str(uuid.uuid4())),
                    event_type=EventType(event_data['event_type']),
                    timestamp=datetime.fromisoformat(event_data['timestamp']),
                    source=event_data.get('source', 'kafka'),
                    data=event_data['data'],
                    metadata=event_data.get('metadata', {}),
                    correlation_id=event_data.get('correlation_id')
                )
                
                yield event
                
            except Exception as e:
                self.logger.error(f"解析Kafka消息失败: {e}")

class WebSocketStreamSource(StreamDataSource):
    """WebSocket流数据源"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__("WebSocket", config)
        self.websocket = None
        
    async def connect(self):
        """连接WebSocket"""
        import websockets
        
        uri = self.config['uri']
        self.websocket = await websockets.connect(uri)
        self.logger.info(f"WebSocket已连接到 {uri}")
        
    async def disconnect(self):
        """断开WebSocket"""
        if self.websocket:
            await self.websocket.close()
            self.logger.info("WebSocket连接已关闭")
            
    async def consume_events(self) -> AsyncGenerator[StreamEvent, None]:
        """消费WebSocket事件"""
        if not self.websocket:
            raise RuntimeError("WebSocket未连接")
            
        try:
            async for message in self.websocket:
                try:
                    event_data = json.loads(message)
                    
                    event = StreamEvent(
                        id=event_data.get('id', str(uuid.uuid4())),
                        event_type=EventType(event_data['event_type']),
                        timestamp=datetime.fromisoformat(event_data['timestamp']),
                        source=event_data.get('source', 'websocket'),
                        data=event_data['data'],
                        metadata=event_data.get('metadata', {}),
                        correlation_id=event_data.get('correlation_id')
                    )
                    
                    yield event
                    
                except Exception as e:
                    self.logger.error(f"解析WebSocket消息失败: {e}")
                    
        except websockets.exceptions.ConnectionClosed:
            self.logger.warning("WebSocket连接已关闭")

class DatabaseStreamSource(StreamDataSource):
    """数据库变更流数据源"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__("Database", config)
        self.db_connection = None
        self.last_processed_id = 0
        
    async def connect(self):
        """连接数据库"""
        # 实现数据库连接逻辑
        self.logger.info("数据库流数据源已连接")
        
    async def disconnect(self):
        """断开数据库"""
        if self.db_connection:
            await self.db_connection.close()
            self.logger.info("数据库连接已关闭")
            
    async def consume_events(self) -> AsyncGenerator[StreamEvent, None]:
        """消费数据库变更事件"""
        while True:
            try:
                # 轮询数据库变更日志
                changes = await self._poll_database_changes()
                
                for change in changes:
                    event = await self._convert_change_to_event(change)
                    yield event
                    
                # 更新最后处理的ID
                if changes:
                    self.last_processed_id = max(c['id'] for c in changes)
                    
                await asyncio.sleep(self.config.get('poll_interval', 5))
                
            except Exception as e:
                self.logger.error(f"轮询数据库变更失败: {e}")
                await asyncio.sleep(10)
                
    async def _poll_database_changes(self) -> List[Dict[str, Any]]:
        """轮询数据库变更"""
        # 实现数据库变更轮询逻辑
        return []
        
    async def _convert_change_to_event(self, change: Dict[str, Any]) -> StreamEvent:
        """将数据库变更转换为流事件"""
        return StreamEvent(
            id=str(uuid.uuid4()),
            event_type=EventType(change['operation']),
            timestamp=datetime.fromisoformat(change['timestamp']),
            source='database',
            data=change['data'],
            metadata={'table': change['table']}
        )
```

### 5. 流处理管理器

```python
class StreamProcessingManager:
    """流处理管理器"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.stream_engine = KGStreamEngine(config)
        self.data_sources: List[StreamDataSource] = []
        self.running = False
        
    async def initialize(self):
        """初始化流处理管理器"""
        # 创建流处理器
        await self._create_processors()
        
        # 创建数据源
        await self._create_data_sources()
        
        self.logger.info("流处理管理器初始化完成")
        
    async def _create_processors(self):
        """创建流处理器"""
        # 变更数据捕获处理器
        # cdc_processor = ChangeDataCaptureProcessor(kg_client, change_log_storage)
        # self.stream_engine.add_processor(cdc_processor)
        
        # 实时索引处理器
        # index_processor = RealTimeIndexProcessor(search_index_client)
        # self.stream_engine.add_processor(index_processor)
        
        # 实时分析处理器
        # analytics_processor = RealTimeAnalyticsProcessor(analytics_storage)
        # self.stream_engine.add_processor(analytics_processor)
        
        # 事件关联处理器
        correlation_processor = EventCorrelationProcessor()
        self.stream_engine.add_processor(correlation_processor)
        
    async def _create_data_sources(self):
        """创建数据源"""
        sources_config = self.config.get('data_sources', [])
        
        for source_config in sources_config:
            source_type = source_config['type']
            
            if source_type == 'kafka':
                source = KafkaStreamSource(source_config)
            elif source_type == 'websocket':
                source = WebSocketStreamSource(source_config)
            elif source_type == 'database':
                source = DatabaseStreamSource(source_config)
            else:
                self.logger.warning(f"未知的数据源类型: {source_type}")
                continue
                
            self.data_sources.append(source)
            
    async def start(self):
        """启动流处理"""
        if self.running:
            return
            
        self.logger.info("启动流处理管理器")
        self.running = True
        
        # 启动流处理引擎
        await self.stream_engine.start()
        
        # 连接所有数据源
        for source in self.data_sources:
            await source.connect()
            
        # 启动数据消费任务
        for source in self.data_sources:
            asyncio.create_task(self._consume_from_source(source))
            
        self.logger.info("流处理管理器已启动")
        
    async def stop(self):
        """停止流处理"""
        if not self.running:
            return
            
        self.logger.info("停止流处理管理器")
        self.running = False
        
        # 断开所有数据源
        for source in self.data_sources:
            await source.disconnect()
            
        # 停止流处理引擎
        await self.stream_engine.stop()
        
        self.logger.info("流处理管理器已停止")
        
    async def _consume_from_source(self, source: StreamDataSource):
        """从数据源消费事件"""
        try:
            async for event in source.consume_events():
                if not self.running:
                    break
                    
                await self.stream_engine.publish_event(event)
                
        except Exception as e:
            self.logger.error(f"从数据源 {source.name} 消费事件失败: {e}")
            
    async def get_processing_metrics(self) -> Dict[str, Any]:
        """获取处理指标"""
        return self.stream_engine.metrics.copy()

# 配置示例
STREAM_CONFIG = {
    "queue_size": 10000,
    "data_sources": [
        {
            "type": "kafka",
            "topic": "kg_events",
            "bootstrap_servers": ["localhost:9092"],
            "group_id": "kg_stream_processor"
        },
        {
            "type": "websocket",
            "uri": "ws://localhost:8080/stream"
        },
        {
            "type": "database",
            "connection_string": "postgresql://user:pass@localhost/kg_db",
            "poll_interval": 5
        }
    ]
}

# 主函数
async def main():
    """主函数"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # 创建并启动流处理管理器
    manager = StreamProcessingManager(STREAM_CONFIG)
    await manager.initialize()
    await manager.start()
    
    try:
        # 运行流处理
        while True:
            # 定期输出处理指标
            metrics = await manager.get_processing_metrics()
            logging.info(f"处理指标: {metrics}")
            
            await asyncio.sleep(60)
            
    except KeyboardInterrupt:
        logging.info("收到停止信号")
    finally:
        await manager.stop()

if __name__ == "__main__":
    asyncio.run(main())
```

这个实时流处理系统提供了：

1. **完整的流处理框架** - 支持事件驱动的实时数据处理
2. **变更数据捕获** - 实时捕获和应用数据变更
3. **实时分析** - 事件统计、异常检测和模式识别
4. **事件关联** - 识别和关联相关事件
5. **多种数据源** - 支持Kafka、WebSocket、数据库等
6. **性能监控** - 实时处理指标和性能统计
7. **异常检测** - 自动检测异常操作模式

系统确保了知识图谱的实时更新和分析能力。
