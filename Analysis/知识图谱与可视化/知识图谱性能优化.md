# 知识图谱性能优化

## 1. 查询优化

### Neo4j查询优化
```python
# Neo4j查询优化器
from neo4j import GraphDatabase
from typing import Dict, List, Any, Optional
import time
import logging

logger = logging.getLogger(__name__)

class Neo4jQueryOptimizer:
    """Neo4j查询优化器"""
    
    def __init__(self, uri: str, username: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
    
    def optimize_query(self, query: str, parameters: Dict = None) -> Dict[str, Any]:
        """优化查询"""
        try:
            with self.driver.session() as session:
                # 启用查询计划分析
                session.run("CALL dbms.queryJit.enabled = true")
                
                # 执行EXPLAIN获取查询计划
                explain_query = f"EXPLAIN {query}"
                explain_result = session.run(explain_query, parameters or {})
                
                # 分析查询计划
                plan_analysis = self.analyze_query_plan(explain_result)
                
                # 执行实际查询并计时
                start_time = time.time()
                result = session.run(query, parameters or {})
                records = list(result)
                execution_time = time.time() - start_time
                
                return {
                    "query": query,
                    "parameters": parameters,
                    "execution_time": execution_time,
                    "result_count": len(records),
                    "plan_analysis": plan_analysis,
                    "optimization_suggestions": self.generate_suggestions(plan_analysis)
                }
        
        except Exception as e:
            logger.error(f"查询优化失败: {e}")
            raise
    
    def analyze_query_plan(self, explain_result) -> Dict[str, Any]:
        """分析查询计划"""
        plan_info = {
            "total_db_hits": 0,
            "total_estimated_rows": 0,
            "operations": []
        }
        
        for record in explain_result:
            plan = record["plan"]
            if "dbHits" in plan:
                plan_info["total_db_hits"] += plan["dbHits"]
            if "estimatedRows" in plan:
                plan_info["total_estimated_rows"] += plan["estimatedRows"]
            
            operation = {
                "operator": plan.get("operatorType", ""),
                "db_hits": plan.get("dbHits", 0),
                "estimated_rows": plan.get("estimatedRows", 0),
                "arguments": plan.get("args", {})
            }
            plan_info["operations"].append(operation)
        
        return plan_info
    
    def generate_suggestions(self, plan_analysis: Dict[str, Any]) -> List[str]:
        """生成优化建议"""
        suggestions = []
        
        # 检查数据库命中次数
        if plan_analysis["total_db_hits"] > 1000:
            suggestions.append("考虑添加索引以减少数据库命中次数")
        
        # 检查扫描操作
        scan_operations = [op for op in plan_analysis["operations"] 
                          if "Scan" in op["operator"]]
        if scan_operations:
            suggestions.append("检测到全表扫描，建议添加适当的索引")
        
        # 检查连接操作
        join_operations = [op for op in plan_analysis["operations"] 
                          if "Join" in op["operator"]]
        if len(join_operations) > 2:
            suggestions.append("检测到多个连接操作，考虑优化查询结构")
        
        return suggestions
    
    def create_indexes(self, index_configs: List[Dict[str, str]]):
        """创建索引"""
        try:
            with self.driver.session() as session:
                for config in index_configs:
                    label = config["label"]
                    property = config["property"]
                    
                    # 创建索引
                    query = f"CREATE INDEX IF NOT EXISTS FOR (n:{label}) ON (n.{property})"
                    session.run(query)
                    
                    logger.info(f"创建索引: {label}.{property}")
        
        except Exception as e:
            logger.error(f"创建索引失败: {e}")
            raise
    
    def analyze_index_usage(self) -> Dict[str, Any]:
        """分析索引使用情况"""
        try:
            with self.driver.session() as session:
                # 获取索引信息
                query = "SHOW INDEXES"
                result = session.run(query)
                
                indexes = []
                for record in result:
                    index_info = {
                        "name": record["name"],
                        "type": record["type"],
                        "labels": record["labelsOrTypes"],
                        "properties": record["properties"],
                        "state": record["state"]
                    }
                    indexes.append(index_info)
                
                return {"indexes": indexes, "count": len(indexes)}
        
        except Exception as e:
            logger.error(f"分析索引使用情况失败: {e}")
            raise

# 查询优化示例
def optimize_knowledge_graph_queries():
    """优化知识图谱查询"""
    optimizer = Neo4jQueryOptimizer(
        uri="bolt://localhost:7687",
        username="neo4j",
        password="password"
    )
    
    # 创建常用索引
    index_configs = [
        {"label": "Concept", "property": "name"},
        {"label": "Concept", "property": "type"},
        {"label": "Relationship", "property": "type"},
        {"label": "Concept", "property": "category"}
    ]
    optimizer.create_indexes(index_configs)
    
    # 优化复杂查询
    complex_query = """
    MATCH (c1:Concept)-[r:RELATES_TO]->(c2:Concept)
    WHERE c1.type = 'theory' AND c2.type = 'application'
    RETURN c1.name, c2.name, r.weight
    ORDER BY r.weight DESC
    LIMIT 10
    """
    
    result = optimizer.optimize_query(complex_query)
    print(f"查询执行时间: {result['execution_time']:.3f}秒")
    print(f"结果数量: {result['result_count']}")
    print(f"优化建议: {result['optimization_suggestions']}")
```

### 查询缓存策略
```python
# 查询缓存管理器
import redis
import hashlib
import json
import pickle
from typing import Any, Optional, Dict
import time

class QueryCacheManager:
    """查询缓存管理器"""
    
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=False)
        self.default_ttl = 3600  # 默认1小时过期
    
    def generate_cache_key(self, query: str, parameters: Dict = None) -> str:
        """生成缓存键"""
        cache_data = {
            "query": query,
            "parameters": parameters or {}
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def get_cached_result(self, query: str, parameters: Dict = None) -> Optional[Any]:
        """获取缓存结果"""
        try:
            cache_key = self.generate_cache_key(query, parameters)
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                return pickle.loads(cached_data)
            
            return None
        
        except Exception as e:
            logger.error(f"获取缓存失败: {e}")
            return None
    
    def set_cached_result(self, query: str, parameters: Dict, result: Any, ttl: int = None):
        """设置缓存结果"""
        try:
            cache_key = self.generate_cache_key(query, parameters)
            serialized_result = pickle.dumps(result)
            
            self.redis_client.setex(
                cache_key,
                ttl or self.default_ttl,
                serialized_result
            )
            
            logger.info(f"缓存查询结果: {cache_key}")
        
        except Exception as e:
            logger.error(f"设置缓存失败: {e}")
    
    def invalidate_cache(self, pattern: str = "*"):
        """清除缓存"""
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)
                logger.info(f"清除缓存: {len(keys)} 个键")
        
        except Exception as e:
            logger.error(f"清除缓存失败: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """获取缓存统计"""
        try:
            info = self.redis_client.info()
            return {
                "total_keys": info.get("db0", {}).get("keys", 0),
                "memory_usage": info.get("used_memory_human", "0B"),
                "hit_rate": info.get("keyspace_hits", 0) / max(info.get("keyspace_misses", 1), 1)
            }
        
        except Exception as e:
            logger.error(f"获取缓存统计失败: {e}")
            return {}

# 缓存优化示例
def implement_query_caching():
    """实现查询缓存"""
    cache_manager = QueryCacheManager()
    
    # 缓存查询结果
    query = "MATCH (c:Concept) RETURN c.name LIMIT 100"
    cached_result = cache_manager.get_cached_result(query)
    
    if cached_result is None:
        # 执行查询
        with GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password")) as driver:
            with driver.session() as session:
                result = session.run(query)
                cached_result = [dict(record) for record in result]
        
        # 缓存结果
        cache_manager.set_cached_result(query, {}, cached_result, ttl=1800)
    
    return cached_result
```

## 2. 索引优化

### 复合索引策略
```python
# 复合索引管理器
class CompositeIndexManager:
    """复合索引管理器"""
    
    def __init__(self, driver: GraphDatabase):
        self.driver = driver
    
    def create_composite_index(self, label: str, properties: List[str]):
        """创建复合索引"""
        try:
            with self.driver.session() as session:
                properties_str = ", ".join([f"n.{prop}" for prop in properties])
                query = f"CREATE INDEX IF NOT EXISTS FOR (n:{label}) ON ({properties_str})"
                session.run(query)
                
                logger.info(f"创建复合索引: {label} -> {properties}")
        
        except Exception as e:
            logger.error(f"创建复合索引失败: {e}")
            raise
    
    def create_text_index(self, label: str, property: str):
        """创建文本索引"""
        try:
            with self.driver.session() as session:
                query = f"CREATE TEXT INDEX IF NOT EXISTS FOR (n:{label}) ON (n.{property})"
                session.run(query)
                
                logger.info(f"创建文本索引: {label}.{property}")
        
        except Exception as e:
            logger.error(f"创建文本索引失败: {e}")
            raise
    
    def create_fulltext_index(self, label: str, properties: List[str]):
        """创建全文索引"""
        try:
            with self.driver.session() as session:
                properties_str = ", ".join([f"n.{prop}" for prop in properties])
                query = f"CREATE FULLTEXT INDEX IF NOT EXISTS FOR (n:{label}) ON ({properties_str})"
                session.run(query)
                
                logger.info(f"创建全文索引: {label} -> {properties}")
        
        except Exception as e:
            logger.error(f"创建全文索引失败: {e}")
            raise
    
    def analyze_index_performance(self) -> Dict[str, Any]:
        """分析索引性能"""
        try:
            with self.driver.session() as session:
                # 获取索引使用统计
                query = """
                CALL db.indexes() YIELD name, type, labelsOrTypes, properties, state
                RETURN name, type, labelsOrTypes, properties, state
                """
                result = session.run(query)
                
                indexes = []
                for record in result:
                    index_info = {
                        "name": record["name"],
                        "type": record["type"],
                        "labels": record["labelsOrTypes"],
                        "properties": record["properties"],
                        "state": record["state"]
                    }
                    indexes.append(index_info)
                
                # 分析索引效果
                performance_analysis = {
                    "total_indexes": len(indexes),
                    "active_indexes": len([idx for idx in indexes if idx["state"] == "ONLINE"]),
                    "index_types": {},
                    "recommendations": []
                }
                
                for index in indexes:
                    index_type = index["type"]
                    performance_analysis["index_types"][index_type] = \
                        performance_analysis["index_types"].get(index_type, 0) + 1
                
                return performance_analysis
        
        except Exception as e:
            logger.error(f"分析索引性能失败: {e}")
            raise

# 索引优化示例
def optimize_indexes():
    """优化索引"""
    driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))
    index_manager = CompositeIndexManager(driver)
    
    # 创建常用复合索引
    composite_indexes = [
        {"label": "Concept", "properties": ["type", "category"]},
        {"label": "Concept", "properties": ["name", "type"]},
        {"label": "Relationship", "properties": ["type", "weight"]}
    ]
    
    for index_config in composite_indexes:
        index_manager.create_composite_index(
            index_config["label"],
            index_config["properties"]
        )
    
    # 创建文本索引
    text_indexes = [
        {"label": "Concept", "property": "description"},
        {"label": "Concept", "property": "content"}
    ]
    
    for index_config in text_indexes:
        index_manager.create_text_index(
            index_config["label"],
            index_config["property"]
        )
    
    # 分析索引性能
    performance = index_manager.analyze_index_performance()
    print(f"索引性能分析: {performance}")
```

## 3. 并行处理优化

### 并行查询处理器
```python
# 并行查询处理器
import asyncio
import concurrent.futures
from typing import List, Dict, Any, Callable
import multiprocessing

class ParallelQueryProcessor:
    """并行查询处理器"""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(32, (multiprocessing.cpu_count() + 4))
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers)
    
    async def execute_parallel_queries(self, queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """并行执行查询"""
        loop = asyncio.get_event_loop()
        
        # 提交所有查询任务
        futures = []
        for query_info in queries:
            future = loop.run_in_executor(
                self.executor,
                self.execute_single_query,
                query_info
            )
            futures.append(future)
        
        # 等待所有查询完成
        results = await asyncio.gather(*futures, return_exceptions=True)
        
        return results
    
    def execute_single_query(self, query_info: Dict[str, Any]) -> Dict[str, Any]:
        """执行单个查询"""
        try:
            query = query_info["query"]
            parameters = query_info.get("parameters", {})
            
            with GraphDatabase.driver(
                query_info["uri"],
                auth=(query_info["username"], query_info["password"])
            ) as driver:
                with driver.session() as session:
                    start_time = time.time()
                    result = session.run(query, parameters)
                    records = list(result)
                    execution_time = time.time() - start_time
                    
                    return {
                        "query": query,
                        "execution_time": execution_time,
                        "result_count": len(records),
                        "success": True,
                        "data": records
                    }
        
        except Exception as e:
            return {
                "query": query_info.get("query", ""),
                "execution_time": 0,
                "result_count": 0,
                "success": False,
                "error": str(e)
            }
    
    def execute_batch_queries(self, queries: List[str], batch_size: int = 10) -> List[Dict[str, Any]]:
        """批量执行查询"""
        results = []
        
        for i in range(0, len(queries), batch_size):
            batch = queries[i:i + batch_size]
            batch_results = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_query = {
                    executor.submit(self.execute_single_query, {"query": query}): query
                    for query in batch
                }
                
                for future in concurrent.futures.as_completed(future_to_query):
                    result = future.result()
                    batch_results.append(result)
            
            results.extend(batch_results)
        
        return results

# 并行处理示例
async def parallel_query_optimization():
    """并行查询优化"""
    processor = ParallelQueryProcessor(max_workers=8)
    
    # 准备并行查询
    queries = [
        {
            "query": "MATCH (c:Concept {type: 'theory'}) RETURN c.name LIMIT 100",
            "uri": "bolt://localhost:7687",
            "username": "neo4j",
            "password": "password"
        },
        {
            "query": "MATCH (c:Concept {type: 'application'}) RETURN c.name LIMIT 100",
            "uri": "bolt://localhost:7687",
            "username": "neo4j",
            "password": "password"
        },
        {
            "query": "MATCH (c:Concept {type: 'method'}) RETURN c.name LIMIT 100",
            "uri": "bolt://localhost:7687",
            "username": "neo4j",
            "password": "password"
        }
    ]
    
    # 并行执行查询
    results = await processor.execute_parallel_queries(queries)
    
    total_time = sum(r["execution_time"] for r in results if r["success"])
    print(f"并行查询总时间: {total_time:.3f}秒")
    
    for result in results:
        if result["success"]:
            print(f"查询结果: {result['result_count']} 条记录, 耗时: {result['execution_time']:.3f}秒")
        else:
            print(f"查询失败: {result['error']}")
```

## 4. 内存优化

### 内存管理器
```python
# 内存管理器
import psutil
import gc
import weakref
from typing import Dict, Any, Optional

class MemoryManager:
    """内存管理器"""
    
    def __init__(self):
        self.memory_threshold = 0.8  # 内存使用率阈值
        self.cache_objects = weakref.WeakValueDictionary()
    
    def get_memory_usage(self) -> Dict[str, float]:
        """获取内存使用情况"""
        memory = psutil.virtual_memory()
        return {
            "total": memory.total / (1024**3),  # GB
            "available": memory.available / (1024**3),  # GB
            "used": memory.used / (1024**3),  # GB
            "percent": memory.percent,
            "free": memory.free / (1024**3)  # GB
        }
    
    def is_memory_pressure(self) -> bool:
        """检查内存压力"""
        memory_usage = self.get_memory_usage()
        return memory_usage["percent"] > (self.memory_threshold * 100)
    
    def optimize_memory(self):
        """优化内存使用"""
        if self.is_memory_pressure():
            logger.warning("检测到内存压力，开始内存优化")
            
            # 强制垃圾回收
            gc.collect()
            
            # 清理缓存对象
            self.cache_objects.clear()
            
            # 清理弱引用
            gc.collect()
            
            logger.info("内存优化完成")
    
    def cache_object(self, key: str, obj: Any):
        """缓存对象"""
        self.cache_objects[key] = obj
    
    def get_cached_object(self, key: str) -> Optional[Any]:
        """获取缓存对象"""
        return self.cache_objects.get(key)
    
    def monitor_memory_usage(self, interval: int = 60):
        """监控内存使用"""
        import threading
        import time
        
        def monitor():
            while True:
                memory_usage = self.get_memory_usage()
                logger.info(f"内存使用率: {memory_usage['percent']:.1f}%")
                
                if self.is_memory_pressure():
                    self.optimize_memory()
                
                time.sleep(interval)
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()

# 内存优化示例
def implement_memory_optimization():
    """实现内存优化"""
    memory_manager = MemoryManager()
    
    # 启动内存监控
    memory_manager.monitor_memory_usage(interval=30)
    
    # 缓存大对象
    large_data = {"data": [i for i in range(1000000)]}
    memory_manager.cache_object("large_dataset", large_data)
    
    # 检查内存使用
    usage = memory_manager.get_memory_usage()
    print(f"当前内存使用率: {usage['percent']:.1f}%")
    
    if memory_manager.is_memory_pressure():
        memory_manager.optimize_memory()
```

## 5. 连接池优化

### 数据库连接池
```python
# 数据库连接池管理器
from neo4j import GraphDatabase
import queue
import threading
import time
from typing import Optional, Dict, Any

class ConnectionPool:
    """连接池管理器"""
    
    def __init__(self, uri: str, username: str, password: str, 
                 max_connections: int = 20, min_connections: int = 5):
        self.uri = uri
        self.username = username
        self.password = password
        self.max_connections = max_connections
        self.min_connections = min_connections
        
        self.connections = queue.Queue(maxsize=max_connections)
        self.active_connections = 0
        self.lock = threading.Lock()
        
        # 初始化连接池
        self._initialize_pool()
    
    def _initialize_pool(self):
        """初始化连接池"""
        for _ in range(self.min_connections):
            connection = self._create_connection()
            if connection:
                self.connections.put(connection)
    
    def _create_connection(self) -> Optional[GraphDatabase]:
        """创建新连接"""
        try:
            driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))
            # 测试连接
            with driver.session() as session:
                session.run("RETURN 1")
            return driver
        except Exception as e:
            logger.error(f"创建连接失败: {e}")
            return None
    
    def get_connection(self) -> Optional[GraphDatabase]:
        """获取连接"""
        try:
            # 尝试从池中获取连接
            connection = self.connections.get_nowait()
            with self.lock:
                self.active_connections += 1
            return connection
        except queue.Empty:
            # 池为空，尝试创建新连接
            if self.active_connections < self.max_connections:
                connection = self._create_connection()
                if connection:
                    with self.lock:
                        self.active_connections += 1
                    return connection
            
            # 等待可用连接
            connection = self.connections.get()
            with self.lock:
                self.active_connections += 1
            return connection
    
    def return_connection(self, connection: GraphDatabase):
        """归还连接"""
        try:
            with self.lock:
                self.active_connections -= 1
            
            # 测试连接是否仍然有效
            try:
                with connection.session() as session:
                    session.run("RETURN 1")
                self.connections.put(connection)
            except Exception:
                # 连接无效，关闭并创建新连接
                connection.close()
                new_connection = self._create_connection()
                if new_connection:
                    self.connections.put(new_connection)
        
        except Exception as e:
            logger.error(f"归还连接失败: {e}")
    
    def get_pool_stats(self) -> Dict[str, Any]:
        """获取连接池统计"""
        with self.lock:
            return {
                "total_connections": self.max_connections,
                "active_connections": self.active_connections,
                "available_connections": self.connections.qsize(),
                "min_connections": self.min_connections
            }
    
    def close_all_connections(self):
        """关闭所有连接"""
        while not self.connections.empty():
            try:
                connection = self.connections.get_nowait()
                connection.close()
            except queue.Empty:
                break

# 连接池使用示例
def optimize_connection_pool():
    """优化连接池"""
    pool = ConnectionPool(
        uri="bolt://localhost:7687",
        username="neo4j",
        password="password",
        max_connections=20,
        min_connections=5
    )
    
    # 使用连接池执行查询
    def execute_query_with_pool(query: str, parameters: Dict = None):
        connection = pool.get_connection()
        try:
            with connection.session() as session:
                result = session.run(query, parameters or {})
                return [dict(record) for record in result]
        finally:
            pool.return_connection(connection)
    
    # 并行执行查询
    queries = [
        ("MATCH (c:Concept) RETURN c.name LIMIT 10", {}),
        ("MATCH (c:Concept {type: 'theory'}) RETURN c.name LIMIT 10", {}),
        ("MATCH (c:Concept {type: 'application'}) RETURN c.name LIMIT 10", {})
    ]
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [
            executor.submit(execute_query_with_pool, query, params)
            for query, params in queries
        ]
        
        results = [future.result() for future in concurrent.futures.as_completed(futures)]
    
    # 获取连接池统计
    stats = pool.get_pool_stats()
    print(f"连接池统计: {stats}")
    
    # 关闭连接池
    pool.close_all_connections()
```

## 6. 工具与平台

### 性能监控工具
1. **Neo4j Browser**：查询性能分析
2. **Neo4j Cypher Profiler**：查询计划分析
3. **Prometheus**：系统指标监控
4. **Grafana**：性能可视化

### 优化工具
1. **Redis**：查询缓存
2. **Elasticsearch**：全文搜索
3. **Apache Spark**：大数据处理
4. **Apache Kafka**：流数据处理

### 云平台优化
1. **AWS ElastiCache**：Redis缓存服务
2. **Azure Cache for Redis**：Redis缓存服务
3. **Google Cloud Memorystore**：Redis缓存服务
4. **阿里云Redis**：Redis缓存服务

## 7. 最佳实践

### 查询优化
1. **使用索引**：为常用查询字段创建索引
2. **避免全表扫描**：使用WHERE子句限制结果
3. **限制结果集**：使用LIMIT限制返回数量
4. **优化JOIN**：减少不必要的连接操作

### 缓存策略
1. **热点数据缓存**：缓存频繁访问的数据
2. **查询结果缓存**：缓存复杂查询结果
3. **缓存失效策略**：设置合理的过期时间
4. **缓存预热**：系统启动时预加载数据

### 内存管理
1. **对象池化**：重用对象减少GC压力
2. **弱引用**：使用弱引用避免内存泄漏
3. **分页处理**：大数据集分页处理
4. **流式处理**：使用生成器处理大数据

### 连接管理
1. **连接池**：使用连接池管理数据库连接
2. **连接复用**：复用连接减少开销
3. **连接监控**：监控连接状态和性能
4. **故障恢复**：连接失败自动重试 