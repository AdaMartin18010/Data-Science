# çŸ¥è¯†å›¾è°±è‡ªç„¶è¯­è¨€å¤„ç†é›†æˆ

## ğŸ“‘ ç›®å½•

- [çŸ¥è¯†å›¾è°±è‡ªç„¶è¯­è¨€å¤„ç†é›†æˆ](#çŸ¥è¯†å›¾è°±è‡ªç„¶è¯­è¨€å¤„ç†é›†æˆ)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
  - [1. å®ä½“è¯†åˆ«ä¸é“¾æ¥](#1-å®ä½“è¯†åˆ«ä¸é“¾æ¥)
    - [1.1. å‘½åå®ä½“è¯†åˆ« (NER)](#11-å‘½åå®ä½“è¯†åˆ«-ner)
  - [2. å…³ç³»æŠ½å–](#2-å…³ç³»æŠ½å–)
    - [2.1. åŸºäºBERTçš„å…³ç³»æŠ½å–](#21-åŸºäºbertçš„å…³ç³»æŠ½å–)
  - [3. çŸ¥è¯†å›¾è°±é—®ç­”](#3-çŸ¥è¯†å›¾è°±é—®ç­”)
    - [3.1. åŸºäºæ£€ç´¢çš„é—®ç­”ç³»ç»Ÿ](#31-åŸºäºæ£€ç´¢çš„é—®ç­”ç³»ç»Ÿ)
  - [4. æ–‡æœ¬ç”Ÿæˆ](#4-æ–‡æœ¬ç”Ÿæˆ)
    - [4.1. åŸºäºçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬ç”Ÿæˆ](#41-åŸºäºçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬ç”Ÿæˆ)
  - [5. å·¥å…·ä¸å¹³å°](#5-å·¥å…·ä¸å¹³å°)
    - [5.1. NLPå·¥å…·](#51-nlpå·¥å…·)
    - [5.2. é—®ç­”ç³»ç»Ÿ](#52-é—®ç­”ç³»ç»Ÿ)
    - [5.3. æ–‡æœ¬ç”Ÿæˆ](#53-æ–‡æœ¬ç”Ÿæˆ)
    - [5.4. å®ä½“è¯†åˆ«](#54-å®ä½“è¯†åˆ«)
  - [6. æœ€ä½³å®è·µ](#6-æœ€ä½³å®è·µ)
    - [6.1. *å®ä½“è¯†åˆ«*](#61-å®ä½“è¯†åˆ«)
    - [6.2. å…³ç³»æŠ½å–](#62-å…³ç³»æŠ½å–)
    - [6.3. *é—®ç­”ç³»ç»Ÿ*](#63-é—®ç­”ç³»ç»Ÿ)
    - [6.4. *æ–‡æœ¬ç”Ÿæˆ*](#64-æ–‡æœ¬ç”Ÿæˆ)

---


## 1. å®ä½“è¯†åˆ«ä¸é“¾æ¥

### 1.1. å‘½åå®ä½“è¯†åˆ« (NER)

```python
# å‘½åå®ä½“è¯†åˆ«ç³»ç»Ÿ
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple, Optional
import spacy
import re

class NERModel(nn.Module):
    """å‘½åå®ä½“è¯†åˆ«æ¨¡å‹"""

    def __init__(self, model_name: str = "bert-base-chinese", num_labels: int = 9):
        super(NERModel, self).__init__()

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

# æ ‡ç­¾æ˜ å°„
        self.label2id = {
            'O': 0,
            'B-PERSON': 1, 'I-PERSON': 2,
            'B-ORGANIZATION': 3, 'I-ORGANIZATION': 4,
            'B-LOCATION': 5, 'I-LOCATION': 6,
            'B-MISC': 7, 'I-MISC': 8
        }
        self.id2label = {v: k for k, v in self.label2id.items()}

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)
        return logits

    def predict(self, text: str) -> List[Tuple[str, str, int, int]]:
        """é¢„æµ‹å®ä½“"""
# åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)

# æ·»åŠ ç‰¹æ®Šæ ‡è®°
        input_ids = [self.tokenizer.cls_token_id] + token_ids + [self.tokenizer.sep_token_id]
        attention_mask = [1] * len(input_ids)

# è½¬æ¢ä¸ºå¼ é‡
        input_ids = torch.tensor([input_ids])
        attention_mask = torch.tensor([attention_mask])

# é¢„æµ‹
        with torch.no_grad():
            logits = self.forward(input_ids, attention_mask)
            predictions = torch.argmax(logits, dim=2)

# è§£ç é¢„æµ‹ç»“æœ
        entities = []
        current_entity = None

        for i, (token, pred_id) in enumerate(zip(tokens, predictions[0][1:-1])):
            label = self.id2label[pred_id.item()]

            if label.startswith('B-'):
                if current_entity:
                    entities.append(current_entity)
                current_entity = {
                    'text': token,
                    'label': label[2:],
                    'start': i,
                    'end': i + 1
                }
            elif label.startswith('I-') and current_entity and label[2:] == current_entity['label']:
                current_entity['text'] += token
                current_entity['end'] = i + 1
            else:
                if current_entity:
                    entities.append(current_entity)
                current_entity = None

        if current_entity:
            entities.append(current_entity)

        return [(entity['text'], entity['label'], entity['start'], entity['end'])
                for entity in entities]

class EntityLinker:
    """å®ä½“é“¾æ¥å™¨"""

    def __init__(self, knowledge_graph):
        self.kg = knowledge_graph
        self.entity_embeddings = {}
        self._build_entity_index()

    def _build_entity_index(self):
        """æ„å»ºå®ä½“ç´¢å¼•"""
# ä»çŸ¥è¯†å›¾è°±ä¸­æå–å®ä½“
        entities = self.kg.get_nodes_by_type('entity')

        for entity_id in entities:
            entity_data = self.kg.nodes[entity_id]
            name = entity_data['properties'].get('name', '')
            aliases = entity_data['properties'].get('aliases', [])

# å­˜å‚¨å®ä½“åç§°å’Œåˆ«å
            self.entity_embeddings[name.lower()] = entity_id
            for alias in aliases:
                self.entity_embeddings[alias.lower()] = entity_id

    def link_entities(self, entities: List[Tuple[str, str, int, int]]) -> List[Dict]:
        """é“¾æ¥å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        linked_entities = []

        for entity_text, entity_type, start, end in entities:
# æ¨¡ç³ŠåŒ¹é…
            best_match = self._find_best_match(entity_text)

            if best_match:
                linked_entities.append({
                    'text': entity_text,
                    'type': entity_type,
                    'start': start,
                    'end': end,
                    'kg_id': best_match,
                    'confidence': self._calculate_confidence(entity_text, best_match)
                })
            else:
                linked_entities.append({
                    'text': entity_text,
                    'type': entity_type,
                    'start': start,
                    'end': end,
                    'kg_id': None,
                    'confidence': 0.0
                })

        return linked_entities

    def _find_best_match(self, entity_text: str) -> Optional[str]:
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„å®ä½“"""
        entity_lower = entity_text.lower()

# ç²¾ç¡®åŒ¹é…
        if entity_lower in self.entity_embeddings:
            return self.entity_embeddings[entity_lower]

# æ¨¡ç³ŠåŒ¹é…
        best_match = None
        best_score = 0.0

        for kg_name, kg_id in self.entity_embeddings.items():
            score = self._calculate_similarity(entity_lower, kg_name)
            if score > best_score and score > 0.8:
                best_score = score
                best_match = kg_id

        return best_match

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
# ç®€å•çš„ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher
        return SequenceMatcher(None, text1, text2).ratio()

    def _calculate_confidence(self, entity_text: str, kg_id: str) -> float:
        """è®¡ç®—é“¾æ¥ç½®ä¿¡åº¦"""
        kg_entity = self.kg.nodes[kg_id]
        kg_name = kg_entity['properties'].get('name', '')

        return self._calculate_similarity(entity_text.lower(), kg_name.lower())

# ä½¿ç”¨ç¤ºä¾‹
def perform_ner_and_linking():
    """æ‰§è¡ŒNERå’Œå®ä½“é“¾æ¥"""
# åˆå§‹åŒ–æ¨¡å‹
    ner_model = NERModel()
    entity_linker = EntityLinker(knowledge_graph)

# ç¤ºä¾‹æ–‡æœ¬
    text = "è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°çš„iPhoneäº§å“ï¼Œä¹”å¸ƒæ–¯æ˜¯å…¬å¸çš„åˆ›å§‹äººã€‚"

# å‘½åå®ä½“è¯†åˆ«
    entities = ner_model.predict(text)
    print(f"è¯†åˆ«åˆ°çš„å®ä½“: {entities}")

# å®ä½“é“¾æ¥
    linked_entities = entity_linker.link_entities(entities)
    print(f"é“¾æ¥ç»“æœ: {linked_entities}")
```

## 2. å…³ç³»æŠ½å–

### 2.1. åŸºäºBERTçš„å…³ç³»æŠ½å–

```python
# å…³ç³»æŠ½å–æ¨¡å‹
class RelationExtractor(nn.Module):
    """å…³ç³»æŠ½å–æ¨¡å‹"""

    def __init__(self, model_name: str = "bert-base-chinese", num_relations: int = 50):
        super(RelationExtractor, self).__init__()

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_relations)

        self.num_relations = num_relations
        self.relation_labels = [
            "is_a", "part_of", "located_in", "founded_by", "works_for",
            "married_to", "parent_of", "child_of", "sibling_of", "friend_of"
        ]

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,
                entity1_pos: torch.Tensor, entity2_pos: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state

# è·å–å®ä½“ä½ç½®çš„è¡¨ç¤º
        batch_size = sequence_output.size(0)
        entity1_repr = torch.gather(sequence_output, 1,
                                   entity1_pos.unsqueeze(-1).expand(-1, -1, sequence_output.size(-1)))
        entity2_repr = torch.gather(sequence_output, 1,
                                   entity2_pos.unsqueeze(-1).expand(-1, -1, sequence_output.size(-1)))

# èåˆå®ä½“è¡¨ç¤º
        combined_repr = torch.cat([entity1_repr.mean(dim=1), entity2_repr.mean(dim=1)], dim=1)
        combined_repr = self.dropout(combined_repr)

        logits = self.classifier(combined_repr)
        return logits

    def extract_relations(self, text: str, entities: List[Dict]) -> List[Dict]:
        """æŠ½å–å…³ç³»"""
        relations = []

# ç”Ÿæˆå®ä½“å¯¹
        entity_pairs = []
        for i in range(len(entities)):
            for j in range(i + 1, len(entities)):
                entity_pairs.append((entities[i], entities[j]))

        for entity1, entity2 in entity_pairs:
# æ„å»ºè¾“å…¥
            tokens = self.tokenizer.tokenize(text)

# æ ‡è®°å®ä½“ä½ç½®
            entity1_start = entity1['start']
            entity1_end = entity1['end']
            entity2_start = entity2['start']
            entity2_end = entity2['end']

# æ·»åŠ ç‰¹æ®Šæ ‡è®°
            input_text = f"[CLS] {text} [SEP]"
            input_ids = self.tokenizer.encode(input_text)
            attention_mask = [1] * len(input_ids)

# è®¡ç®—å®ä½“ä½ç½®
            entity1_pos = list(range(entity1_start + 1, entity1_end + 1))
            entity2_pos = list(range(entity2_start + 1, entity2_end + 1))

# è½¬æ¢ä¸ºå¼ é‡
            input_ids = torch.tensor([input_ids])
            attention_mask = torch.tensor([attention_mask])
            entity1_pos = torch.tensor([entity1_pos])
            entity2_pos = torch.tensor([entity2_pos])

# é¢„æµ‹å…³ç³»
            with torch.no_grad():
                logits = self.forward(input_ids, attention_mask, entity1_pos, entity2_pos)
                probs = F.softmax(logits, dim=1)
                pred_relation = torch.argmax(probs, dim=1)
                confidence = torch.max(probs, dim=1)[0]

            if confidence > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                relation = {
                    'entity1': entity1,
                    'entity2': entity2,
                    'relation': self.relation_labels[pred_relation.item()],
                    'confidence': confidence.item()
                }
                relations.append(relation)

        return relations

class PatternBasedRelationExtractor:
    """åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–å™¨"""

    def __init__(self):
        self.patterns = {
            'is_a': [
                r'(\w+)æ˜¯(\w+)',
                r'(\w+)å±äº(\w+)',
                r'(\w+)ä¸º(\w+)'
            ],
            'part_of': [
                r'(\w+)åŒ…å«(\w+)',
                r'(\w+)åŒ…æ‹¬(\w+)',
                r'(\w+)ç”±(\w+)ç»„æˆ'
            ],
            'located_in': [
                r'(\w+)ä½äº(\w+)',
                r'(\w+)åœ¨(\w+)',
                r'(\w+)åè½äº(\w+)'
            ],
            'founded_by': [
                r'(\w+)ç”±(\w+)åˆ›ç«‹',
                r'(\w+)çš„åˆ›å§‹äººæ˜¯(\w+)',
                r'(\w+)ç”±(\w+)åˆ›å»º'
            ]
        }

    def extract_relations(self, text: str) -> List[Dict]:
        """åŸºäºæ¨¡å¼æŠ½å–å…³ç³»"""
        relations = []

        for relation_type, patterns in self.patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    entity1 = match.group(1)
                    entity2 = match.group(2)

                    relation = {
                        'entity1': entity1,
                        'entity2': entity2,
                        'relation': relation_type,
                        'confidence': 0.8,  # æ¨¡å¼åŒ¹é…çš„ç½®ä¿¡åº¦
                        'source': 'pattern'
                    }
                    relations.append(relation)

        return relations

# ä½¿ç”¨ç¤ºä¾‹
def perform_relation_extraction():
    """æ‰§è¡Œå…³ç³»æŠ½å–"""
# åˆå§‹åŒ–æ¨¡å‹
    relation_extractor = RelationExtractor()
    pattern_extractor = PatternBasedRelationExtractor()

# ç¤ºä¾‹æ–‡æœ¬
    text = "è‹¹æœå…¬å¸ç”±ä¹”å¸ƒæ–¯åˆ›ç«‹ï¼Œæ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·ã€‚"

# å‡è®¾å·²ç»è¯†åˆ«å‡ºå®ä½“
    entities = [
        {'text': 'è‹¹æœå…¬å¸', 'type': 'ORGANIZATION', 'start': 0, 'end': 4},
        {'text': 'ä¹”å¸ƒæ–¯', 'type': 'PERSON', 'start': 5, 'end': 7},
        {'text': 'åŠ åˆ©ç¦å°¼äºšå·', 'type': 'LOCATION', 'start': 12, 'end': 17}
    ]

# åŸºäºBERTçš„å…³ç³»æŠ½å–
    bert_relations = relation_extractor.extract_relations(text, entities)
    print(f"BERTæŠ½å–çš„å…³ç³»: {bert_relations}")

# åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–
    pattern_relations = pattern_extractor.extract_relations(text)
    print(f"æ¨¡å¼æŠ½å–çš„å…³ç³»: {pattern_relations}")
```

## 3. çŸ¥è¯†å›¾è°±é—®ç­”

### 3.1. åŸºäºæ£€ç´¢çš„é—®ç­”ç³»ç»Ÿ

```python
# çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ
class KGQuestionAnswering:
    """çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, knowledge_graph, entity_linker):
        self.kg = knowledge_graph
        self.entity_linker = entity_linker
        self.question_patterns = self._load_question_patterns()

    def _load_question_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½é—®é¢˜æ¨¡å¼"""
        return {
            'what_is': [
                r'ä»€ä¹ˆæ˜¯(\w+)',
                r'(\w+)æ˜¯ä»€ä¹ˆ',
                r'(\w+)çš„å®šä¹‰'
            ],
            'who_is': [
                r'è°æ˜¯(\w+)',
                r'(\w+)æ˜¯è°',
                r'(\w+)çš„ä¸ªäººä¿¡æ¯'
            ],
            'where_is': [
                r'(\w+)åœ¨å“ªé‡Œ',
                r'(\w+)çš„ä½ç½®',
                r'(\w+)åè½äº'
            ],
            'when_was': [
                r'(\w+)ä»€ä¹ˆæ—¶å€™',
                r'(\w+)çš„æ—¶é—´',
                r'(\w+)çš„æ—¥æœŸ'
            ],
            'how_many': [
                r'(\w+)æœ‰å¤šå°‘',
                r'(\w+)çš„æ•°é‡',
                r'(\w+)çš„è§„æ¨¡'
            ]
        }

    def answer_question(self, question: str) -> Dict:
        """å›ç­”é—®é¢˜"""
# 1. é—®é¢˜åˆ†ç±»
        question_type = self._classify_question(question)

# 2. å®ä½“è¯†åˆ«å’Œé“¾æ¥
        entities = self._extract_entities(question)

# 3. æŸ¥è¯¢æ„å»º
        query = self._build_query(question_type, entities)

# 4. æ‰§è¡ŒæŸ¥è¯¢
        answer = self._execute_query(query)

        return {
            'question': question,
            'question_type': question_type,
            'entities': entities,
            'query': query,
            'answer': answer
        }

    def _classify_question(self, question: str) -> str:
        """é—®é¢˜åˆ†ç±»"""
        for qtype, patterns in self.question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question):
                    return qtype
        return 'unknown'

    def _extract_entities(self, question: str) -> List[Dict]:
        """æå–å®ä½“"""
# ä½¿ç”¨NERæ¨¡å‹è¯†åˆ«å®ä½“
        ner_model = NERModel()
        entities = ner_model.predict(question)

# å®ä½“é“¾æ¥
        linked_entities = self.entity_linker.link_entities(entities)
        return linked_entities

    def _build_query(self, question_type: str, entities: List[Dict]) -> str:
        """æ„å»ºæŸ¥è¯¢"""
        if not entities:
            return ""

        entity_id = entities[0]['kg_id']
        if not entity_id:
            return ""

        if question_type == 'what_is':
            return f"MATCH (n {{id: '{entity_id}'}}) RETURN n.description, n.definition"
        elif question_type == 'who_is':
            return f"MATCH (n {{id: '{entity_id}'}}) RETURN n.name, n.biography, n.birth_date"
        elif question_type == 'where_is':
            return f"MATCH (n {{id: '{entity_id}'}})-[:LOCATED_IN]->(location) RETURN location.name"
        elif question_type == 'when_was':
            return f"MATCH (n {{id: '{entity_id}'}}) RETURN n.founded_date, n.created_date"
        elif question_type == 'how_many':
            return f"MATCH (n {{id: '{entity_id}'}}) RETURN n.size, n.count, n.quantity"
        else:
            return f"MATCH (n {{id: '{entity_id}'}}) RETURN n"

    def _execute_query(self, query: str) -> str:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        if not query:
            return "æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        try:
            with self.kg.driver.session() as session:
                result = session.run(query)
                records = list(result)

                if records:
# æ ¼å¼åŒ–ç­”æ¡ˆ
                    answer = self._format_answer(records)
                    return answer
                else:
                    return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        except Exception as e:
            return f"æŸ¥è¯¢å‡ºé”™: {str(e)}"

    def _format_answer(self, records: List) -> str:
        """æ ¼å¼åŒ–ç­”æ¡ˆ"""
        if not records:
            return ""

# ç®€å•çš„ç­”æ¡ˆæ ¼å¼åŒ–
        answer_parts = []
        for record in records:
            for key, value in record.items():
                if value:
                    answer_parts.append(f"{key}: {value}")

        return "ï¼›".join(answer_parts)

class ConversationalQA:
    """å¯¹è¯å¼é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, kg_qa: KGQuestionAnswering):
        self.kg_qa = kg_qa
        self.conversation_history = []

    def ask(self, question: str) -> Dict:
        """æé—®"""
# æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        contextualized_question = self._add_context(question)

# è·å–ç­”æ¡ˆ
        answer = self.kg_qa.answer_question(contextualized_question)

# æ›´æ–°å¯¹è¯å†å²
        self.conversation_history.append({
            'question': question,
            'answer': answer['answer'],
            'timestamp': datetime.datetime.now()
        })

        return answer

    def _add_context(self, question: str) -> str:
        """æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if len(self.conversation_history) > 0:
# ä»å†å²ä¸­æå–ç›¸å…³å®ä½“
            recent_entities = []
            for entry in self.conversation_history[-3:]:  # æœ€è¿‘3è½®å¯¹è¯
# ç®€å•çš„å®ä½“æå–
                entities = re.findall(r'[A-Z][a-z]+', entry['question'])
                recent_entities.extend(entities)

            if recent_entities:
                context = f"å…³äº{', '.join(set(recent_entities))}ï¼Œ{question}"
                return context

        return question

    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history.clear()

# ä½¿ç”¨ç¤ºä¾‹
def test_qa_system():
    """æµ‹è¯•é—®ç­”ç³»ç»Ÿ"""
# åˆå§‹åŒ–ç³»ç»Ÿ
    kg_qa = KGQuestionAnswering(knowledge_graph, entity_linker)
    conversational_qa = ConversationalQA(kg_qa)

# æµ‹è¯•é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "è‹¹æœå…¬å¸åœ¨å“ªé‡Œï¼Ÿ",
        "ä¹”å¸ƒæ–¯æ˜¯è°ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ æœ‰å¤šå°‘ç§ç®—æ³•ï¼Ÿ"
    ]

    for question in questions:
        answer = conversational_qa.ask(question)
        print(f"é—®é¢˜: {question}")
        print(f"ç­”æ¡ˆ: {answer['answer']}")
        print("---")
```

## 4. æ–‡æœ¬ç”Ÿæˆ

### 4.1. åŸºäºçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬ç”Ÿæˆ

```python
# åŸºäºçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬ç”Ÿæˆ
class KGTextGenerator:
    """åŸºäºçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬ç”Ÿæˆå™¨"""

    def __init__(self, knowledge_graph, model_name: str = "gpt2"):
        self.kg = knowledge_graph
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

# çŸ¥è¯†å›¾è°±æ¨¡æ¿
        self.templates = {
            'description': "{entity}æ˜¯ä¸€ä¸ª{type}ï¼Œ{properties}ã€‚",
            'relationship': "{entity1}ä¸{entity2}ä¹‹é—´å­˜åœ¨{relation}å…³ç³»ã€‚",
            'comparison': "{entity1}å’Œ{entity2}éƒ½æ˜¯{common_type}ï¼Œä½†{entity1}{diff1}ï¼Œè€Œ{entity2}{diff2}ã€‚"
        }

    def generate_description(self, entity_id: str) -> str:
        """ç”Ÿæˆå®ä½“æè¿°"""
        entity_data = self.kg.nodes[entity_id]
        entity_type = entity_data['type']
        properties = entity_data['properties']

# æ„å»ºæè¿°æ–‡æœ¬
        description_parts = []
        for key, value in properties.items():
            if key not in ['id', 'name']:
                description_parts.append(f"{key}æ˜¯{value}")

        description = self.templates['description'].format(
            entity=properties.get('name', entity_id),
            type=entity_type,
            properties='ï¼Œ'.join(description_parts)
        )

        return description

    def generate_relationship_text(self, entity1_id: str, entity2_id: str, relation: str) -> str:
        """ç”Ÿæˆå…³ç³»æè¿°æ–‡æœ¬"""
        entity1_data = self.kg.nodes[entity1_id]
        entity2_data = self.kg.nodes[entity2_id]

        text = self.templates['relationship'].format(
            entity1=entity1_data['properties'].get('name', entity1_id),
            entity2=entity2_data['properties'].get('name', entity2_id),
            relation=relation
        )

        return text

    def generate_comparison_text(self, entity1_id: str, entity2_id: str) -> str:
        """ç”Ÿæˆæ¯”è¾ƒæ–‡æœ¬"""
        entity1_data = self.kg.nodes[entity1_id]
        entity2_data = self.kg.nodes[entity2_id]

# æ‰¾åˆ°å…±åŒå±æ€§
        common_properties = set(entity1_data['properties'].keys()) & set(entity2_data['properties'].keys())
        differences = []

        for prop in common_properties:
            val1 = entity1_data['properties'][prop]
            val2 = entity2_data['properties'][prop]
            if val1 != val2:
                differences.append(f"{prop}æ˜¯{val1}ï¼Œè€Œ{entity2_data['properties'].get('name', entity2_id)}çš„{prop}æ˜¯{val2}")

        if differences:
            text = self.templates['comparison'].format(
                entity1=entity1_data['properties'].get('name', entity1_id),
                entity2=entity2_data['properties'].get('name', entity2_id),
                common_type=entity1_data['type'],
                diff1=differences[0] if differences else "",
                diff2=differences[1] if len(differences) > 1 else ""
            )
            return text

        return f"{entity1_data['properties'].get('name', entity1_id)}å’Œ{entity2_data['properties'].get('name', entity2_id)}éƒ½æ˜¯{entity1_data['type']}ã€‚"

    def generate_summary(self, topic: str, max_length: int = 200) -> str:
        """ç”Ÿæˆä¸»é¢˜æ‘˜è¦"""
# æŸ¥æ‰¾ç›¸å…³å®ä½“
        related_entities = self._find_related_entities(topic)

        if not related_entities:
            return f"å…³äº{topic}çš„ä¿¡æ¯ä¸è¶³ã€‚"

# ç”Ÿæˆæ‘˜è¦æ–‡æœ¬
        summary_parts = []
        for entity_id in related_entities[:5]:  # é™åˆ¶å®ä½“æ•°é‡
            description = self.generate_description(entity_id)
            summary_parts.append(description)

        summary = " ".join(summary_parts)

# æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."

        return summary

    def _find_related_entities(self, topic: str) -> List[str]:
        """æŸ¥æ‰¾ç›¸å…³å®ä½“"""
        related_entities = []

# ç®€å•çš„å…³é”®è¯åŒ¹é…
        for entity_id, entity_data in self.kg.nodes.items():
            properties = entity_data['properties']
            entity_text = f"{properties.get('name', '')} {properties.get('description', '')}".lower()

            if topic.lower() in entity_text:
                related_entities.append(entity_id)

        return related_entities

class TemplateBasedGenerator:
    """åŸºäºæ¨¡æ¿çš„æ–‡æœ¬ç”Ÿæˆå™¨"""

    def __init__(self):
        self.templates = {
            'introduction': [
                "è®©æˆ‘ä¸ºæ‚¨ä»‹ç»{entity}ã€‚",
                "å…³äº{entity}ï¼Œæˆ‘æƒ³å‘Šè¯‰æ‚¨ä»¥ä¸‹ä¿¡æ¯ï¼š",
                "{entity}æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„è¯é¢˜ã€‚"
            ],
            'explanation': [
                "{entity}æ˜¯{definition}ã€‚",
                "ç®€å•æ¥è¯´ï¼Œ{entity}å°±æ˜¯{definition}ã€‚",
                "æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£{entity}ï¼š{definition}ã€‚"
            ],
            'comparison': [
                "ä¸{other_entity}ç›¸æ¯”ï¼Œ{entity}å…·æœ‰{advantage}ã€‚",
                "{entity}å’Œ{other_entity}å„æœ‰ç‰¹ç‚¹ã€‚",
                "è®©æˆ‘ä»¬æ¥æ¯”è¾ƒä¸€ä¸‹{entity}å’Œ{other_entity}ã€‚"
            ],
            'conclusion': [
                "æ€»çš„æ¥è¯´ï¼Œ{entity}æ˜¯ä¸€ä¸ª{summary}ã€‚",
                "é€šè¿‡ä»¥ä¸Šä»‹ç»ï¼Œæˆ‘ä»¬å¯¹{entity}æœ‰äº†æ›´æ·±å…¥çš„äº†è§£ã€‚",
                "å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨äº†è§£{entity}æœ‰æ‰€å¸®åŠ©ã€‚"
            ]
        }

    def generate_text(self, template_type: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if template_type not in self.templates:
            return ""

        templates = self.templates[template_type]
        template = random.choice(templates)

        return template.format(**kwargs)

# ä½¿ç”¨ç¤ºä¾‹
def test_text_generation():
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
# åˆå§‹åŒ–ç”Ÿæˆå™¨
    kg_generator = KGTextGenerator(knowledge_graph)
    template_generator = TemplateBasedGenerator()

# ç”Ÿæˆå®ä½“æè¿°
    entity_id = "machine_learning"
    description = kg_generator.generate_description(entity_id)
    print(f"å®ä½“æè¿°: {description}")

# ç”Ÿæˆå…³ç³»æ–‡æœ¬
    relation_text = kg_generator.generate_relationship_text(
        "machine_learning", "deep_learning", "is_a"
    )
    print(f"å…³ç³»æ–‡æœ¬: {relation_text}")

# ç”Ÿæˆæ¯”è¾ƒæ–‡æœ¬
    comparison_text = kg_generator.generate_comparison_text(
        "machine_learning", "deep_learning"
    )
    print(f"æ¯”è¾ƒæ–‡æœ¬: {comparison_text}")

# ç”Ÿæˆæ‘˜è¦
    summary = kg_generator.generate_summary("äººå·¥æ™ºèƒ½")
    print(f"æ‘˜è¦: {summary}")

# ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ–‡æœ¬
    intro_text = template_generator.generate_text(
        'introduction', entity="æœºå™¨å­¦ä¹ "
    )
    print(f"ä»‹ç»æ–‡æœ¬: {intro_text}")
```

## 5. å·¥å…·ä¸å¹³å°

### 5.1. NLPå·¥å…·

1. **spaCy**ï¼šå·¥ä¸šçº§NLPåº“
2. **NLTK**ï¼šè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…
3. **Transformers**ï¼šé¢„è®­ç»ƒæ¨¡å‹åº“
4. **AllenNLP**ï¼šæ·±åº¦å­¦ä¹ NLPåº“

### 5.2. é—®ç­”ç³»ç»Ÿ

1. **Rasa**ï¼šå¯¹è¯å¼AIå¹³å°
2. **Haystack**ï¼šé—®ç­”ç³»ç»Ÿæ¡†æ¶
3. **BERT-QA**ï¼šBERTé—®ç­”ç³»ç»Ÿ
4. **DrQA**ï¼šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

### 5.3. æ–‡æœ¬ç”Ÿæˆ

1. **GPT-2/3**ï¼šç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹
2. **T5**ï¼šæ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢æ¨¡å‹
3. **BART**ï¼šåŒå‘è‡ªå›å½’å˜æ¢å™¨
4. **CTRL**ï¼šæ¡ä»¶å˜æ¢å™¨è¯­è¨€æ¨¡å‹

### 5.4. å®ä½“è¯†åˆ«

1. **Stanford NER**ï¼šæ–¯å¦ç¦å‘½åå®ä½“è¯†åˆ«
2. **spaCy NER**ï¼šspaCyå®ä½“è¯†åˆ«
3. **BERT-NER**ï¼šåŸºäºBERTçš„NER
4. **BiLSTM-CRF**ï¼šåºåˆ—æ ‡æ³¨æ¨¡å‹

## 6. æœ€ä½³å®è·µ

### 6.1. *å®ä½“è¯†åˆ«*

1. **æ•°æ®è´¨é‡**ï¼šç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡
2. **é¢†åŸŸé€‚åº”**ï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸè°ƒæ•´æ¨¡å‹
3. **å®ä½“é“¾æ¥**ï¼šæé«˜å®ä½“é“¾æ¥å‡†ç¡®æ€§
4. **å¢é‡æ›´æ–°**ï¼šæ”¯æŒæ–°å®ä½“å¢é‡å­¦ä¹ 

### 6.2. å…³ç³»æŠ½å–

1. **æ¨¡å¼æŒ–æ˜**ï¼šè‡ªåŠ¨å‘ç°å…³ç³»æ¨¡å¼
2. **è¿œç¨‹ç›‘ç£**ï¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±æ ‡æ³¨æ•°æ®
3. **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒå¤šè¯­è¨€å…³ç³»æŠ½å–
4. **å…³ç³»éªŒè¯**ï¼šéªŒè¯æŠ½å–å…³ç³»çš„å‡†ç¡®æ€§

### 6.3. *é—®ç­”ç³»ç»Ÿ*

1. **é—®é¢˜ç†è§£**ï¼šå‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾
2. **ç­”æ¡ˆç”Ÿæˆ**ï¼šç”Ÿæˆè‡ªç„¶æµç•…çš„ç­”æ¡ˆ
3. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡
4. **ç­”æ¡ˆè¯„ä¼°**ï¼šè¯„ä¼°ç­”æ¡ˆè´¨é‡

### 6.4. *æ–‡æœ¬ç”Ÿæˆ*

1. **å†…å®¹æ§åˆ¶**ï¼šæ§åˆ¶ç”Ÿæˆå†…å®¹çš„è´¨é‡
2. **é£æ ¼ä¸€è‡´**ï¼šä¿æŒç”Ÿæˆæ–‡æœ¬çš„é£æ ¼ä¸€è‡´
3. **äº‹å®å‡†ç¡®**ï¼šç¡®ä¿ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§
4. **å¤šæ ·æ€§**ï¼šå¢åŠ ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§
