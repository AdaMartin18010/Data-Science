# 4. 数据模型理论体系实践案例

## 1. 概述

本文档提供数据模型理论体系在实际项目中的具体应用案例，涵盖数据存储、数据流处理、数据可视化、数据安全等各个方面的实际应用。

## 2. 数据存储模型实践案例

### 2.1 分布式存储系统设计

```python
# 分布式存储系统设计
import hashlib
import json
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import time

@dataclass
class StorageNode:
    """存储节点"""
    id: str
    host: str
    port: int
    capacity: int  # 存储容量(GB)
    used_space: int = 0  # 已使用空间(GB)
    status: str = "active"  # active, inactive, failed
    
    def get_available_space(self) -> int:
        """获取可用空间"""
        return self.capacity - self.used_space
    
    def can_store(self, size: int) -> bool:
        """检查是否可以存储指定大小的数据"""
        return self.get_available_space() >= size and self.status == "active"

@dataclass
class DataChunk:
    """数据块"""
    id: str
    data: bytes
    size: int
    checksum: str
    replica_nodes: List[str]  # 副本存储的节点ID列表
    created_at: float
    
    def verify_checksum(self) -> bool:
        """验证校验和"""
        calculated_checksum = hashlib.md5(self.data).hexdigest()
        return calculated_checksum == self.checksum

class DistributedStorageSystem:
    """分布式存储系统"""
    
    def __init__(self):
        self.nodes: Dict[str, StorageNode] = {}
        self.chunks: Dict[str, DataChunk] = {}
        self.replication_factor = 3  # 副本因子
        
    def add_node(self, node: StorageNode):
        """添加存储节点"""
        self.nodes[node.id] = node
        print(f"添加存储节点: {node.id} ({node.host}:{node.port})")
    
    def remove_node(self, node_id: str):
        """移除存储节点"""
        if node_id in self.nodes:
            # 重新分配该节点上的数据
            self._redistribute_data(node_id)
            del self.nodes[node_id]
            print(f"移除存储节点: {node_id}")
    
    def store_data(self, data: bytes, chunk_size: int = 1024*1024) -> str:
        """存储数据"""
        # 生成数据ID
        data_id = hashlib.sha256(data).hexdigest()
        
        # 分块存储
        chunks = []
        for i in range(0, len(data), chunk_size):
            chunk_data = data[i:i+chunk_size]
            chunk_id = f"{data_id}_chunk_{i//chunk_size}"
            
            # 创建数据块
            chunk = self._create_chunk(chunk_id, chunk_data)
            chunks.append(chunk)
        
        # 保存元数据
        metadata = {
            'data_id': data_id,
            'chunk_ids': [chunk.id for chunk in chunks],
            'total_size': len(data),
            'created_at': time.time()
        }
        
        metadata_chunk = self._create_chunk(f"{data_id}_metadata", json.dumps(metadata).encode())
        chunks.append(metadata_chunk)
        
        print(f"数据 {data_id} 已存储，共 {len(chunks)} 个块")
        return data_id
    
    def retrieve_data(self, data_id: str) -> Optional[bytes]:
        """检索数据"""
        # 获取元数据
        metadata_chunk = self.chunks.get(f"{data_id}_metadata")
        if not metadata_chunk:
            print(f"数据 {data_id} 不存在")
            return None
        
        # 解析元数据
        metadata = json.loads(metadata_chunk.data.decode())
        chunk_ids = metadata['chunk_ids']
        
        # 检索所有数据块
        data_parts = []
        for chunk_id in chunk_ids:
            chunk = self.chunks.get(chunk_id)
            if not chunk:
                print(f"数据块 {chunk_id} 丢失")
                return None
            
            if not chunk.verify_checksum():
                print(f"数据块 {chunk_id} 校验失败")
                return None
            
            data_parts.append(chunk.data)
        
        # 合并数据
        data = b''.join(data_parts)
        print(f"数据 {data_id} 检索成功，大小: {len(data)} 字节")
        return data
    
    def _create_chunk(self, chunk_id: str, data: bytes) -> DataChunk:
        """创建数据块"""
        # 计算校验和
        checksum = hashlib.md5(data).hexdigest()
        
        # 选择存储节点
        replica_nodes = self._select_storage_nodes(len(data))
        
        # 创建数据块
        chunk = DataChunk(
            id=chunk_id,
            data=data,
            size=len(data),
            checksum=checksum,
            replica_nodes=replica_nodes,
            created_at=time.time()
        )
        
        # 存储到选定的节点
        for node_id in replica_nodes:
            if node_id in self.nodes:
                self.nodes[node_id].used_space += len(data)
        
        self.chunks[chunk_id] = chunk
        return chunk
    
    def _select_storage_nodes(self, data_size: int) -> List[str]:
        """选择存储节点"""
        # 过滤可用节点
        available_nodes = [
            node_id for node_id, node in self.nodes.items()
            if node.can_store(data_size)
        ]
        
        if len(available_nodes) < self.replication_factor:
            raise Exception(f"可用节点不足，需要 {self.replication_factor} 个，只有 {len(available_nodes)} 个")
        
        # 按可用空间排序，选择空间最大的节点
        available_nodes.sort(
            key=lambda node_id: self.nodes[node_id].get_available_space(),
            reverse=True
        )
        
        return available_nodes[:self.replication_factor]
    
    def _redistribute_data(self, failed_node_id: str):
        """重新分配数据"""
        chunks_to_redistribute = []
        
        # 找到需要重新分配的数据块
        for chunk_id, chunk in self.chunks.items():
            if failed_node_id in chunk.replica_nodes:
                chunks_to_redistribute.append(chunk)
        
        # 重新分配每个数据块
        for chunk in chunks_to_redistribute:
            # 移除失败的节点
            chunk.replica_nodes.remove(failed_node_id)
            
            # 选择新的存储节点
            new_nodes = self._select_storage_nodes(chunk.size)
            chunk.replica_nodes.extend(new_nodes[:self.replication_factor - len(chunk.replica_nodes)])
            
            # 更新节点使用空间
            for node_id in new_nodes[:self.replication_factor - len(chunk.replica_nodes)]:
                if node_id in self.nodes:
                    self.nodes[node_id].used_space += chunk.size
        
        print(f"重新分配了 {len(chunks_to_redistribute)} 个数据块")
    
    def get_system_status(self) -> Dict:
        """获取系统状态"""
        total_capacity = sum(node.capacity for node in self.nodes.values())
        total_used = sum(node.used_space for node in self.nodes.values())
        
        return {
            'total_nodes': len(self.nodes),
            'active_nodes': len([n for n in self.nodes.values() if n.status == 'active']),
            'total_capacity_gb': total_capacity,
            'used_capacity_gb': total_used,
            'usage_percentage': (total_used / total_capacity * 100) if total_capacity > 0 else 0,
            'total_chunks': len(self.chunks)
        }

# 使用示例
def distributed_storage_example():
    """分布式存储系统示例"""
    storage_system = DistributedStorageSystem()
    
    # 添加存储节点
    nodes = [
        StorageNode("node1", "192.168.1.10", 8080, 1000),
        StorageNode("node2", "192.168.1.11", 8080, 1000),
        StorageNode("node3", "192.168.1.12", 8080, 1000),
        StorageNode("node4", "192.168.1.13", 8080, 1000),
    ]
    
    for node in nodes:
        storage_system.add_node(node)
    
    # 存储数据
    test_data = b"This is a test data for distributed storage system. " * 1000
    data_id = storage_system.store_data(test_data)
    
    # 检索数据
    retrieved_data = storage_system.retrieve_data(data_id)
    print(f"数据一致性: {test_data == retrieved_data}")
    
    # 模拟节点故障
    print("\n模拟节点故障...")
    storage_system.nodes["node1"].status = "failed"
    storage_system._redistribute_data("node1")
    
    # 再次检索数据
    retrieved_data_after_failure = storage_system.retrieve_data(data_id)
    print(f"故障后数据一致性: {test_data == retrieved_data_after_failure}")
    
    # 系统状态
    status = storage_system.get_system_status()
    print(f"\n系统状态: {status}")

if __name__ == "__main__":
    distributed_storage_example()
```

### 2.2 数据分片和负载均衡

```python
# 数据分片和负载均衡
import random
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class Shard:
    """数据分片"""
    id: str
    range_start: int
    range_end: int
    node_id: str
    data: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.data is None:
            self.data = {}

class ShardingStrategy:
    """分片策略"""
    
    @staticmethod
    def hash_based_sharding(key: str, num_shards: int) -> int:
        """基于哈希的分片"""
        return hash(key) % num_shards
    
    @staticmethod
    def range_based_sharding(key: str, ranges: List[Tuple[int, int]]) -> int:
        """基于范围的分片"""
        key_hash = hash(key)
        for i, (start, end) in enumerate(ranges):
            if start <= key_hash <= end:
                return i
        return 0  # 默认分片
    
    @staticmethod
    def consistent_hashing(key: str, nodes: List[str]) -> str:
        """一致性哈希"""
        # 简化的实现
        key_hash = hash(key)
        node_index = key_hash % len(nodes)
        return nodes[node_index]

class LoadBalancer:
    """负载均衡器"""
    
    def __init__(self, strategy: str = "round_robin"):
        self.strategy = strategy
        self.current_index = 0
        self.node_weights = {}
        self.node_loads = {}
    
    def add_node(self, node_id: str, weight: int = 1):
        """添加节点"""
        self.node_weights[node_id] = weight
        self.node_loads[node_id] = 0
    
    def select_node(self, nodes: List[str]) -> str:
        """选择节点"""
        if not nodes:
            raise ValueError("没有可用节点")
        
        if self.strategy == "round_robin":
            return self._round_robin(nodes)
        elif self.strategy == "least_connections":
            return self._least_connections(nodes)
        elif self.strategy == "weighted":
            return self._weighted_selection(nodes)
        else:
            return random.choice(nodes)
    
    def _round_robin(self, nodes: List[str]) -> str:
        """轮询"""
        node = nodes[self.current_index % len(nodes)]
        self.current_index += 1
        return node
    
    def _least_connections(self, nodes: List[str]) -> str:
        """最少连接"""
        min_load = float('inf')
        selected_node = nodes[0]
        
        for node in nodes:
            load = self.node_loads.get(node, 0)
            if load < min_load:
                min_load = load
                selected_node = node
        
        return selected_node
    
    def _weighted_selection(self, nodes: List[str]) -> str:
        """加权选择"""
        total_weight = sum(self.node_weights.get(node, 1) for node in nodes)
        rand = random.uniform(0, total_weight)
        
        current_weight = 0
        for node in nodes:
            weight = self.node_weights.get(node, 1)
            current_weight += weight
            if rand <= current_weight:
                return node
        
        return nodes[0]
    
    def update_load(self, node_id: str, load: int):
        """更新节点负载"""
        self.node_loads[node_id] = load

class ShardedDatabase:
    """分片数据库"""
    
    def __init__(self, num_shards: int = 4):
        self.num_shards = num_shards
        self.shards: Dict[int, Shard] = {}
        self.load_balancer = LoadBalancer("least_connections")
        self.nodes = []
        
        # 初始化分片
        self._initialize_shards()
    
    def _initialize_shards(self):
        """初始化分片"""
        shard_size = 2**32 // self.num_shards
        
        for i in range(self.num_shards):
            start = i * shard_size
            end = (i + 1) * shard_size - 1 if i < self.num_shards - 1 else 2**32 - 1
            
            shard = Shard(
                id=f"shard_{i}",
                range_start=start,
                range_end=end,
                node_id=""
            )
            self.shards[i] = shard
    
    def add_node(self, node_id: str, weight: int = 1):
        """添加节点"""
        self.nodes.append(node_id)
        self.load_balancer.add_node(node_id, weight)
        
        # 分配分片给新节点
        self._assign_shards()
    
    def _assign_shards(self):
        """分配分片"""
        available_nodes = self.nodes.copy()
        
        for shard_id, shard in self.shards.items():
            if not available_nodes:
                available_nodes = self.nodes.copy()
            
            # 选择负载最少的节点
            selected_node = self.load_balancer.select_node(available_nodes)
            shard.node_id = selected_node
            
            # 从可用节点列表中移除已分配的节点
            if selected_node in available_nodes:
                available_nodes.remove(selected_node)
    
    def get_shard_for_key(self, key: str) -> Shard:
        """获取键对应的分片"""
        shard_id = ShardingStrategy.hash_based_sharding(key, self.num_shards)
        return self.shards[shard_id]
    
    def put(self, key: str, value: Any):
        """存储数据"""
        shard = self.get_shard_for_key(key)
        shard.data[key] = value
        
        # 更新节点负载
        self.load_balancer.update_load(shard.node_id, len(shard.data))
        print(f"存储 {key} 到分片 {shard.id} (节点: {shard.node_id})")
    
    def get(self, key: str) -> Any:
        """获取数据"""
        shard = self.get_shard_for_key(key)
        value = shard.data.get(key)
        
        if value is not None:
            print(f"从分片 {shard.id} (节点: {shard.node_id}) 获取 {key}")
        else:
            print(f"键 {key} 不存在")
        
        return value
    
    def get_shard_distribution(self) -> Dict[str, int]:
        """获取分片分布"""
        distribution = {}
        for shard in self.shards.values():
            node_id = shard.node_id
            distribution[node_id] = distribution.get(node_id, 0) + 1
        return distribution
    
    def get_load_distribution(self) -> Dict[str, int]:
        """获取负载分布"""
        return {node_id: len(shard.data) for node_id, shard in 
                ((shard.node_id, shard) for shard in self.shards.values())}

# 使用示例
def sharding_example():
    """分片数据库示例"""
    db = ShardedDatabase(num_shards=4)
    
    # 添加节点
    nodes = ["node1", "node2", "node3", "node4"]
    for i, node in enumerate(nodes):
        db.add_node(node, weight=i+1)
    
    # 存储数据
    test_data = {
        "user_001": {"name": "Alice", "age": 25},
        "user_002": {"name": "Bob", "age": 30},
        "user_003": {"name": "Charlie", "age": 35},
        "user_004": {"name": "David", "age": 40},
        "user_005": {"name": "Eve", "age": 45},
    }
    
    for key, value in test_data.items():
        db.put(key, value)
    
    # 检索数据
    for key in test_data.keys():
        value = db.get(key)
        print(f"{key}: {value}")
    
    # 查看分布
    print(f"\n分片分布: {db.get_shard_distribution()}")
    print(f"负载分布: {db.get_load_distribution()}")

if __name__ == "__main__":
    sharding_example()
```

## 3. 数据流处理模型实践案例

### 3.1 实时数据流处理

```python
# 实时数据流处理
import time
import threading
from queue import Queue
from typing import Dict, List, Callable, Any
from dataclasses import dataclass
import json

@dataclass
class DataEvent:
    """数据事件"""
    id: str
    timestamp: float
    data: Dict[str, Any]
    source: str

class StreamProcessor:
    """流处理器"""
    
    def __init__(self):
        self.input_queue = Queue()
        self.output_queue = Queue()
        self.processors: List[Callable] = []
        self.running = False
        self.thread = None
    
    def add_processor(self, processor: Callable):
        """添加处理器"""
        self.processors.append(processor)
    
    def start(self):
        """启动处理"""
        self.running = True
        self.thread = threading.Thread(target=self._process_loop)
        self.thread.start()
        print("流处理器已启动")
    
    def stop(self):
        """停止处理"""
        self.running = False
        if self.thread:
            self.thread.join()
        print("流处理器已停止")
    
    def _process_loop(self):
        """处理循环"""
        while self.running:
            try:
                # 从输入队列获取数据
                event = self.input_queue.get(timeout=1)
                
                # 应用所有处理器
                processed_event = event
                for processor in self.processors:
                    processed_event = processor(processed_event)
                
                # 发送到输出队列
                if processed_event:
                    self.output_queue.put(processed_event)
                
            except Exception as e:
                print(f"处理错误: {e}")
    
    def send_event(self, event: DataEvent):
        """发送事件"""
        self.input_queue.put(event)
    
    def get_processed_event(self) -> DataEvent:
        """获取处理后的事件"""
        return self.output_queue.get()

class DataAggregator:
    """数据聚合器"""
    
    def __init__(self, window_size: int = 100, window_time: float = 60):
        self.window_size = window_size
        self.window_time = window_time
        self.events = []
        self.last_aggregation = time.time()
    
    def __call__(self, event: DataEvent) -> DataEvent:
        """处理事件"""
        self.events.append(event)
        
        # 检查是否需要聚合
        if (len(self.events) >= self.window_size or 
            time.time() - self.last_aggregation >= self.window_time):
            return self._aggregate_events()
        
        return event
    
    def _aggregate_events(self) -> DataEvent:
        """聚合事件"""
        if not self.events:
            return None
        
        # 计算聚合统计
        values = [event.data.get('value', 0) for event in self.events]
        aggregated_data = {
            'count': len(self.events),
            'sum': sum(values),
            'avg': sum(values) / len(values) if values else 0,
            'min': min(values) if values else 0,
            'max': max(values) if values else 0,
            'timestamp': time.time()
        }
        
        # 创建聚合事件
        aggregated_event = DataEvent(
            id=f"agg_{int(time.time())}",
            timestamp=time.time(),
            data=aggregated_data,
            source="aggregator"
        )
        
        # 清空事件列表
        self.events.clear()
        self.last_aggregation = time.time()
        
        return aggregated_event

class DataFilter:
    """数据过滤器"""
    
    def __init__(self, filter_condition: Callable):
        self.filter_condition = filter_condition
    
    def __call__(self, event: DataEvent) -> DataEvent:
        """过滤事件"""
        if self.filter_condition(event):
            return event
        return None

class DataTransformer:
    """数据转换器"""
    
    def __init__(self, transform_func: Callable):
        self.transform_func = transform_func
    
    def __call__(self, event: DataEvent) -> DataEvent:
        """转换事件"""
        transformed_data = self.transform_func(event.data)
        
        return DataEvent(
            id=event.id,
            timestamp=event.timestamp,
            data=transformed_data,
            source=event.source
        )

# 使用示例
def stream_processing_example():
    """流处理示例"""
    # 创建流处理器
    processor = StreamProcessor()
    
    # 添加过滤器（只处理值大于10的事件）
    filter_processor = DataFilter(lambda event: event.data.get('value', 0) > 10)
    processor.add_processor(filter_processor)
    
    # 添加转换器（将值乘以2）
    transform_processor = DataTransformer(lambda data: {**data, 'value': data.get('value', 0) * 2})
    processor.add_processor(transform_processor)
    
    # 添加聚合器
    aggregator = DataAggregator(window_size=5, window_time=10)
    processor.add_processor(aggregator)
    
    # 启动处理器
    processor.start()
    
    # 模拟数据流
    for i in range(20):
        event = DataEvent(
            id=f"event_{i}",
            timestamp=time.time(),
            data={'value': i, 'source': 'sensor'},
            source='sensor'
        )
        processor.send_event(event)
        time.sleep(0.1)
    
    # 等待处理完成
    time.sleep(2)
    
    # 获取处理结果
    print("处理结果:")
    while not processor.output_queue.empty():
        event = processor.get_processed_event()
        print(f"  {event.id}: {event.data}")
    
    # 停止处理器
    processor.stop()

if __name__ == "__main__":
    stream_processing_example()
```

## 4. 总结

本文档提供了数据模型理论体系在实际项目中的具体应用案例，包括：

1. **数据存储模型实践**：分布式存储系统设计和数据分片负载均衡
2. **数据流处理模型实践**：实时数据流处理系统

这些案例展示了数据模型理论在分布式系统、大数据处理等领域的实际应用，为数据模型项目的开发和部署提供了具体的参考和指导。
