# 4. æ•°æ®æ¨¡å‹ç†è®ºä½“ç³»å®è·µæ¡ˆä¾‹

## ğŸ“‘ ç›®å½•

- [4. æ•°æ®æ¨¡å‹ç†è®ºä½“ç³»å®è·µæ¡ˆä¾‹](#4-æ•°æ®æ¨¡å‹ç†è®ºä½“ç³»å®è·µæ¡ˆä¾‹)
  - [ğŸ“‘ ç›®å½•](#-ç›®å½•)
- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. æ•°æ®å­˜å‚¨æ¨¡å‹å®è·µæ¡ˆä¾‹](#2-æ•°æ®å­˜å‚¨æ¨¡å‹å®è·µæ¡ˆä¾‹)
  - [2.1. åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿè®¾è®¡](#21-åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿè®¾è®¡)
- [3. æ•°æ®åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡](#3-æ•°æ®åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡)
- [4. æ•°æ®æµå¤„ç†æ¨¡å‹å®è·µæ¡ˆä¾‹](#4-æ•°æ®æµå¤„ç†æ¨¡å‹å®è·µæ¡ˆä¾‹)
  - [4.1. å®æ—¶æ•°æ®æµå¤„ç†](#41-å®æ—¶æ•°æ®æµå¤„ç†)
- [5. æ€»ç»“](#5-æ€»ç»“)
---


## 1. æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ•°æ®æ¨¡å‹ç†è®ºä½“ç³»åœ¨å®é™…é¡¹ç›®ä¸­çš„å…·ä½“åº”ç”¨æ¡ˆä¾‹ï¼Œæ¶µç›–æ•°æ®å­˜å‚¨ã€æ•°æ®æµå¤„ç†ã€æ•°æ®å¯è§†åŒ–ã€æ•°æ®å®‰å…¨ç­‰å„ä¸ªæ–¹é¢çš„å®é™…åº”ç”¨ã€‚

## 2. æ•°æ®å­˜å‚¨æ¨¡å‹å®è·µæ¡ˆä¾‹

### 2.1. åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿè®¾è®¡

```python
# åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿè®¾è®¡
import hashlib
import json
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import time

@dataclass
class StorageNode:
    """å­˜å‚¨èŠ‚ç‚¹"""
    id: str
    host: str
    port: int
    capacity: int  # å­˜å‚¨å®¹é‡(GB)
    used_space: int = 0  # å·²ä½¿ç”¨ç©ºé—´(GB)
    status: str = "active"  # active, inactive, failed

    def get_available_space(self) -> int:
        """è·å–å¯ç”¨ç©ºé—´"""
        return self.capacity - self.used_space

    def can_store(self, size: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å­˜å‚¨æŒ‡å®šå¤§å°çš„æ•°æ®"""
        return self.get_available_space() >= size and self.status == "active"

@dataclass
class DataChunk:
    """æ•°æ®å—"""
    id: str
    data: bytes
    size: int
    checksum: str
    replica_nodes: List[str]  # å‰¯æœ¬å­˜å‚¨çš„èŠ‚ç‚¹IDåˆ—è¡¨
    created_at: float

    def verify_checksum(self) -> bool:
        """éªŒè¯æ ¡éªŒå’Œ"""
        calculated_checksum = hashlib.md5(self.data).hexdigest()
        return calculated_checksum == self.checksum

class DistributedStorageSystem:
    """åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ"""

    def __init__(self):
        self.nodes: Dict[str, StorageNode] = {}
        self.chunks: Dict[str, DataChunk] = {}
        self.replication_factor = 3  # å‰¯æœ¬å› å­

    def add_node(self, node: StorageNode):
        """æ·»åŠ å­˜å‚¨èŠ‚ç‚¹"""
        self.nodes[node.id] = node
        print(f"æ·»åŠ å­˜å‚¨èŠ‚ç‚¹: {node.id} ({node.host}:{node.port})")

    def remove_node(self, node_id: str):
        """ç§»é™¤å­˜å‚¨èŠ‚ç‚¹"""
        if node_id in self.nodes:
# é‡æ–°åˆ†é…è¯¥èŠ‚ç‚¹ä¸Šçš„æ•°æ®
            self._redistribute_data(node_id)
            del self.nodes[node_id]
            print(f"ç§»é™¤å­˜å‚¨èŠ‚ç‚¹: {node_id}")

    def store_data(self, data: bytes, chunk_size: int = 1024*1024) -> str:
        """å­˜å‚¨æ•°æ®"""
# ç”Ÿæˆæ•°æ®ID
        data_id = hashlib.sha256(data).hexdigest()

# åˆ†å—å­˜å‚¨
        chunks = []
        for i in range(0, len(data), chunk_size):
            chunk_data = data[i:i+chunk_size]
            chunk_id = f"{data_id}_chunk_{i//chunk_size}"

# åˆ›å»ºæ•°æ®å—
            chunk = self._create_chunk(chunk_id, chunk_data)
            chunks.append(chunk)

# ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'data_id': data_id,
            'chunk_ids': [chunk.id for chunk in chunks],
            'total_size': len(data),
            'created_at': time.time()
        }

        metadata_chunk = self._create_chunk(f"{data_id}_metadata", json.dumps(metadata).encode())
        chunks.append(metadata_chunk)

        print(f"æ•°æ® {data_id} å·²å­˜å‚¨ï¼Œå…± {len(chunks)} ä¸ªå—")
        return data_id

    def retrieve_data(self, data_id: str) -> Optional[bytes]:
        """æ£€ç´¢æ•°æ®"""
# è·å–å…ƒæ•°æ®
        metadata_chunk = self.chunks.get(f"{data_id}_metadata")
        if not metadata_chunk:
            print(f"æ•°æ® {data_id} ä¸å­˜åœ¨")
            return None

# è§£æå…ƒæ•°æ®
        metadata = json.loads(metadata_chunk.data.decode())
        chunk_ids = metadata['chunk_ids']

# æ£€ç´¢æ‰€æœ‰æ•°æ®å—
        data_parts = []
        for chunk_id in chunk_ids:
            chunk = self.chunks.get(chunk_id)
            if not chunk:
                print(f"æ•°æ®å— {chunk_id} ä¸¢å¤±")
                return None

            if not chunk.verify_checksum():
                print(f"æ•°æ®å— {chunk_id} æ ¡éªŒå¤±è´¥")
                return None

            data_parts.append(chunk.data)

# åˆå¹¶æ•°æ®
        data = b''.join(data_parts)
        print(f"æ•°æ® {data_id} æ£€ç´¢æˆåŠŸï¼Œå¤§å°: {len(data)} å­—èŠ‚")
        return data

    def _create_chunk(self, chunk_id: str, data: bytes) -> DataChunk:
        """åˆ›å»ºæ•°æ®å—"""
# è®¡ç®—æ ¡éªŒå’Œ
        checksum = hashlib.md5(data).hexdigest()

# é€‰æ‹©å­˜å‚¨èŠ‚ç‚¹
        replica_nodes = self._select_storage_nodes(len(data))

# åˆ›å»ºæ•°æ®å—
        chunk = DataChunk(
            id=chunk_id,
            data=data,
            size=len(data),
            checksum=checksum,
            replica_nodes=replica_nodes,
            created_at=time.time()
        )

# å­˜å‚¨åˆ°é€‰å®šçš„èŠ‚ç‚¹
        for node_id in replica_nodes:
            if node_id in self.nodes:
                self.nodes[node_id].used_space += len(data)

        self.chunks[chunk_id] = chunk
        return chunk

    def _select_storage_nodes(self, data_size: int) -> List[str]:
        """é€‰æ‹©å­˜å‚¨èŠ‚ç‚¹"""
# è¿‡æ»¤å¯ç”¨èŠ‚ç‚¹
        available_nodes = [
            node_id for node_id, node in self.nodes.items()
            if node.can_store(data_size)
        ]

        if len(available_nodes) < self.replication_factor:
            raise Exception(f"å¯ç”¨èŠ‚ç‚¹ä¸è¶³ï¼Œéœ€è¦ {self.replication_factor} ä¸ªï¼Œåªæœ‰ {len(available_nodes)} ä¸ª")

# æŒ‰å¯ç”¨ç©ºé—´æ’åºï¼Œé€‰æ‹©ç©ºé—´æœ€å¤§çš„èŠ‚ç‚¹
        available_nodes.sort(
            key=lambda node_id: self.nodes[node_id].get_available_space(),
            reverse=True
        )

        return available_nodes[:self.replication_factor]

    def _redistribute_data(self, failed_node_id: str):
        """é‡æ–°åˆ†é…æ•°æ®"""
        chunks_to_redistribute = []

# æ‰¾åˆ°éœ€è¦é‡æ–°åˆ†é…çš„æ•°æ®å—
        for chunk_id, chunk in self.chunks.items():
            if failed_node_id in chunk.replica_nodes:
                chunks_to_redistribute.append(chunk)

# é‡æ–°åˆ†é…æ¯ä¸ªæ•°æ®å—
        for chunk in chunks_to_redistribute:
# ç§»é™¤å¤±è´¥çš„èŠ‚ç‚¹
            chunk.replica_nodes.remove(failed_node_id)

# é€‰æ‹©æ–°çš„å­˜å‚¨èŠ‚ç‚¹
            new_nodes = self._select_storage_nodes(chunk.size)
            chunk.replica_nodes.extend(new_nodes[:self.replication_factor - len(chunk.replica_nodes)])

# æ›´æ–°èŠ‚ç‚¹ä½¿ç”¨ç©ºé—´
            for node_id in new_nodes[:self.replication_factor - len(chunk.replica_nodes)]:
                if node_id in self.nodes:
                    self.nodes[node_id].used_space += chunk.size

        print(f"é‡æ–°åˆ†é…äº† {len(chunks_to_redistribute)} ä¸ªæ•°æ®å—")

    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        total_capacity = sum(node.capacity for node in self.nodes.values())
        total_used = sum(node.used_space for node in self.nodes.values())

        return {
            'total_nodes': len(self.nodes),
            'active_nodes': len([n for n in self.nodes.values() if n.status == 'active']),
            'total_capacity_gb': total_capacity,
            'used_capacity_gb': total_used,
            'usage_percentage': (total_used / total_capacity * 100) if total_capacity > 0 else 0,
            'total_chunks': len(self.chunks)
        }

# ä½¿ç”¨ç¤ºä¾‹
def distributed_storage_example():
    """åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿç¤ºä¾‹"""
    storage_system = DistributedStorageSystem()

# æ·»åŠ å­˜å‚¨èŠ‚ç‚¹
    nodes = [
        StorageNode("node1", "192.168.1.10", 8080, 1000),
        StorageNode("node2", "192.168.1.11", 8080, 1000),
        StorageNode("node3", "192.168.1.12", 8080, 1000),
        StorageNode("node4", "192.168.1.13", 8080, 1000),
    ]

    for node in nodes:
        storage_system.add_node(node)

# å­˜å‚¨æ•°æ®
    test_data = b"This is a test data for distributed storage system. " * 1000
    data_id = storage_system.store_data(test_data)

# æ£€ç´¢æ•°æ®
    retrieved_data = storage_system.retrieve_data(data_id)
    print(f"æ•°æ®ä¸€è‡´æ€§: {test_data == retrieved_data}")

# æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ
    print("\næ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ...")
    storage_system.nodes["node1"].status = "failed"
    storage_system._redistribute_data("node1")

# å†æ¬¡æ£€ç´¢æ•°æ®
    retrieved_data_after_failure = storage_system.retrieve_data(data_id)
    print(f"æ•…éšœåæ•°æ®ä¸€è‡´æ€§: {test_data == retrieved_data_after_failure}")

# ç³»ç»ŸçŠ¶æ€
    status = storage_system.get_system_status()
    print(f"\nç³»ç»ŸçŠ¶æ€: {status}")

if __name__ == "__main__":
    distributed_storage_example()
```

## 3. æ•°æ®åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡

```python
# æ•°æ®åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡
import random
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class Shard:
    """æ•°æ®åˆ†ç‰‡"""
    id: str
    range_start: int
    range_end: int
    node_id: str
    data: Dict[str, Any] = None

    def __post_init__(self):
        if self.data is None:
            self.data = {}

class ShardingStrategy:
    """åˆ†ç‰‡ç­–ç•¥"""

    @staticmethod
    def hash_based_sharding(key: str, num_shards: int) -> int:
        """åŸºäºå“ˆå¸Œçš„åˆ†ç‰‡"""
        return hash(key) % num_shards

    @staticmethod
    def range_based_sharding(key: str, ranges: List[Tuple[int, int]]) -> int:
        """åŸºäºèŒƒå›´çš„åˆ†ç‰‡"""
        key_hash = hash(key)
        for i, (start, end) in enumerate(ranges):
            if start <= key_hash <= end:
                return i
        return 0  # é»˜è®¤åˆ†ç‰‡

    @staticmethod
    def consistent_hashing(key: str, nodes: List[str]) -> str:
        """ä¸€è‡´æ€§å“ˆå¸Œ"""
# ç®€åŒ–çš„å®ç°
        key_hash = hash(key)
        node_index = key_hash % len(nodes)
        return nodes[node_index]

class LoadBalancer:
    """è´Ÿè½½å‡è¡¡å™¨"""

    def __init__(self, strategy: str = "round_robin"):
        self.strategy = strategy
        self.current_index = 0
        self.node_weights = {}
        self.node_loads = {}

    def add_node(self, node_id: str, weight: int = 1):
        """æ·»åŠ èŠ‚ç‚¹"""
        self.node_weights[node_id] = weight
        self.node_loads[node_id] = 0

    def select_node(self, nodes: List[str]) -> str:
        """é€‰æ‹©èŠ‚ç‚¹"""
        if not nodes:
            raise ValueError("æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹")

        if self.strategy == "round_robin":
            return self._round_robin(nodes)
        elif self.strategy == "least_connections":
            return self._least_connections(nodes)
        elif self.strategy == "weighted":
            return self._weighted_selection(nodes)
        else:
            return random.choice(nodes)

    def _round_robin(self, nodes: List[str]) -> str:
        """è½®è¯¢"""
        node = nodes[self.current_index % len(nodes)]
        self.current_index += 1
        return node

    def _least_connections(self, nodes: List[str]) -> str:
        """æœ€å°‘è¿æ¥"""
        min_load = float('inf')
        selected_node = nodes[0]

        for node in nodes:
            load = self.node_loads.get(node, 0)
            if load < min_load:
                min_load = load
                selected_node = node

        return selected_node

    def _weighted_selection(self, nodes: List[str]) -> str:
        """åŠ æƒé€‰æ‹©"""
        total_weight = sum(self.node_weights.get(node, 1) for node in nodes)
        rand = random.uniform(0, total_weight)

        current_weight = 0
        for node in nodes:
            weight = self.node_weights.get(node, 1)
            current_weight += weight
            if rand <= current_weight:
                return node

        return nodes[0]

    def update_load(self, node_id: str, load: int):
        """æ›´æ–°èŠ‚ç‚¹è´Ÿè½½"""
        self.node_loads[node_id] = load

class ShardedDatabase:
    """åˆ†ç‰‡æ•°æ®åº“"""

    def __init__(self, num_shards: int = 4):
        self.num_shards = num_shards
        self.shards: Dict[int, Shard] = {}
        self.load_balancer = LoadBalancer("least_connections")
        self.nodes = []

# åˆå§‹åŒ–åˆ†ç‰‡
        self._initialize_shards()

    def _initialize_shards(self):
        """åˆå§‹åŒ–åˆ†ç‰‡"""
        shard_size = 2**32 // self.num_shards

        for i in range(self.num_shards):
            start = i * shard_size
            end = (i + 1) * shard_size - 1 if i < self.num_shards - 1 else 2**32 - 1

            shard = Shard(
                id=f"shard_{i}",
                range_start=start,
                range_end=end,
                node_id=""
            )
            self.shards[i] = shard

    def add_node(self, node_id: str, weight: int = 1):
        """æ·»åŠ èŠ‚ç‚¹"""
        self.nodes.append(node_id)
        self.load_balancer.add_node(node_id, weight)

# åˆ†é…åˆ†ç‰‡ç»™æ–°èŠ‚ç‚¹
        self._assign_shards()

    def _assign_shards(self):
        """åˆ†é…åˆ†ç‰‡"""
        available_nodes = self.nodes.copy()

        for shard_id, shard in self.shards.items():
            if not available_nodes:
                available_nodes = self.nodes.copy()

# é€‰æ‹©è´Ÿè½½æœ€å°‘çš„èŠ‚ç‚¹
            selected_node = self.load_balancer.select_node(available_nodes)
            shard.node_id = selected_node

# ä»å¯ç”¨èŠ‚ç‚¹åˆ—è¡¨ä¸­ç§»é™¤å·²åˆ†é…çš„èŠ‚ç‚¹
            if selected_node in available_nodes:
                available_nodes.remove(selected_node)

    def get_shard_for_key(self, key: str) -> Shard:
        """è·å–é”®å¯¹åº”çš„åˆ†ç‰‡"""
        shard_id = ShardingStrategy.hash_based_sharding(key, self.num_shards)
        return self.shards[shard_id]

    def put(self, key: str, value: Any):
        """å­˜å‚¨æ•°æ®"""
        shard = self.get_shard_for_key(key)
        shard.data[key] = value

# æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
        self.load_balancer.update_load(shard.node_id, len(shard.data))
        print(f"å­˜å‚¨ {key} åˆ°åˆ†ç‰‡ {shard.id} (èŠ‚ç‚¹: {shard.node_id})")

    def get(self, key: str) -> Any:
        """è·å–æ•°æ®"""
        shard = self.get_shard_for_key(key)
        value = shard.data.get(key)

        if value is not None:
            print(f"ä»åˆ†ç‰‡ {shard.id} (èŠ‚ç‚¹: {shard.node_id}) è·å– {key}")
        else:
            print(f"é”® {key} ä¸å­˜åœ¨")

        return value

    def get_shard_distribution(self) -> Dict[str, int]:
        """è·å–åˆ†ç‰‡åˆ†å¸ƒ"""
        distribution = {}
        for shard in self.shards.values():
            node_id = shard.node_id
            distribution[node_id] = distribution.get(node_id, 0) + 1
        return distribution

    def get_load_distribution(self) -> Dict[str, int]:
        """è·å–è´Ÿè½½åˆ†å¸ƒ"""
        return {node_id: len(shard.data) for node_id, shard in
                ((shard.node_id, shard) for shard in self.shards.values())}

# ä½¿ç”¨ç¤ºä¾‹
def sharding_example():
    """åˆ†ç‰‡æ•°æ®åº“ç¤ºä¾‹"""
    db = ShardedDatabase(num_shards=4)

# æ·»åŠ èŠ‚ç‚¹
    nodes = ["node1", "node2", "node3", "node4"]
    for i, node in enumerate(nodes):
        db.add_node(node, weight=i+1)

# å­˜å‚¨æ•°æ®
    test_data = {
        "user_001": {"name": "Alice", "age": 25},
        "user_002": {"name": "Bob", "age": 30},
        "user_003": {"name": "Charlie", "age": 35},
        "user_004": {"name": "David", "age": 40},
        "user_005": {"name": "Eve", "age": 45},
    }

    for key, value in test_data.items():
        db.put(key, value)

# æ£€ç´¢æ•°æ®
    for key in test_data.keys():
        value = db.get(key)
        print(f"{key}: {value}")

# æŸ¥çœ‹åˆ†å¸ƒ
    print(f"\nåˆ†ç‰‡åˆ†å¸ƒ: {db.get_shard_distribution()}")
    print(f"è´Ÿè½½åˆ†å¸ƒ: {db.get_load_distribution()}")

if __name__ == "__main__":
    sharding_example()
```

## 4. æ•°æ®æµå¤„ç†æ¨¡å‹å®è·µæ¡ˆä¾‹

### 4.1. å®æ—¶æ•°æ®æµå¤„ç†

```python
# å®æ—¶æ•°æ®æµå¤„ç†
import time
import threading
from queue import Queue
from typing import Dict, List, Callable, Any
from dataclasses import dataclass
import json

@dataclass
class DataEvent:
    """æ•°æ®äº‹ä»¶"""
    id: str
    timestamp: float
    data: Dict[str, Any]
    source: str

class StreamProcessor:
    """æµå¤„ç†å™¨"""

    def __init__(self):
        self.input_queue = Queue()
        self.output_queue = Queue()
        self.processors: List[Callable] = []
        self.running = False
        self.thread = None

    def add_processor(self, processor: Callable):
        """æ·»åŠ å¤„ç†å™¨"""
        self.processors.append(processor)

    def start(self):
        """å¯åŠ¨å¤„ç†"""
        self.running = True
        self.thread = threading.Thread(target=self._process_loop)
        self.thread.start()
        print("æµå¤„ç†å™¨å·²å¯åŠ¨")

    def stop(self):
        """åœæ­¢å¤„ç†"""
        self.running = False
        if self.thread:
            self.thread.join()
        print("æµå¤„ç†å™¨å·²åœæ­¢")

    def _process_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.running:
            try:
# ä»è¾“å…¥é˜Ÿåˆ—è·å–æ•°æ®
                event = self.input_queue.get(timeout=1)

# åº”ç”¨æ‰€æœ‰å¤„ç†å™¨
                processed_event = event
                for processor in self.processors:
                    processed_event = processor(processed_event)

# å‘é€åˆ°è¾“å‡ºé˜Ÿåˆ—
                if processed_event:
                    self.output_queue.put(processed_event)

            except Exception as e:
                print(f"å¤„ç†é”™è¯¯: {e}")

    def send_event(self, event: DataEvent):
        """å‘é€äº‹ä»¶"""
        self.input_queue.put(event)

    def get_processed_event(self) -> DataEvent:
        """è·å–å¤„ç†åçš„äº‹ä»¶"""
        return self.output_queue.get()

class DataAggregator:
    """æ•°æ®èšåˆå™¨"""

    def __init__(self, window_size: int = 100, window_time: float = 60):
        self.window_size = window_size
        self.window_time = window_time
        self.events = []
        self.last_aggregation = time.time()

    def __call__(self, event: DataEvent) -> DataEvent:
        """å¤„ç†äº‹ä»¶"""
        self.events.append(event)

# æ£€æŸ¥æ˜¯å¦éœ€è¦èšåˆ
        if (len(self.events) >= self.window_size or
            time.time() - self.last_aggregation >= self.window_time):
            return self._aggregate_events()

        return event

    def _aggregate_events(self) -> DataEvent:
        """èšåˆäº‹ä»¶"""
        if not self.events:
            return None

# è®¡ç®—èšåˆç»Ÿè®¡
        values = [event.data.get('value', 0) for event in self.events]
        aggregated_data = {
            'count': len(self.events),
            'sum': sum(values),
            'avg': sum(values) / len(values) if values else 0,
            'min': min(values) if values else 0,
            'max': max(values) if values else 0,
            'timestamp': time.time()
        }

# åˆ›å»ºèšåˆäº‹ä»¶
        aggregated_event = DataEvent(
            id=f"agg_{int(time.time())}",
            timestamp=time.time(),
            data=aggregated_data,
            source="aggregator"
        )

# æ¸…ç©ºäº‹ä»¶åˆ—è¡¨
        self.events.clear()
        self.last_aggregation = time.time()

        return aggregated_event

class DataFilter:
    """æ•°æ®è¿‡æ»¤å™¨"""

    def __init__(self, filter_condition: Callable):
        self.filter_condition = filter_condition

    def __call__(self, event: DataEvent) -> DataEvent:
        """è¿‡æ»¤äº‹ä»¶"""
        if self.filter_condition(event):
            return event
        return None

class DataTransformer:
    """æ•°æ®è½¬æ¢å™¨"""

    def __init__(self, transform_func: Callable):
        self.transform_func = transform_func

    def __call__(self, event: DataEvent) -> DataEvent:
        """è½¬æ¢äº‹ä»¶"""
        transformed_data = self.transform_func(event.data)

        return DataEvent(
            id=event.id,
            timestamp=event.timestamp,
            data=transformed_data,
            source=event.source
        )

# ä½¿ç”¨ç¤ºä¾‹
def stream_processing_example():
    """æµå¤„ç†ç¤ºä¾‹"""
# åˆ›å»ºæµå¤„ç†å™¨
    processor = StreamProcessor()

# æ·»åŠ è¿‡æ»¤å™¨ï¼ˆåªå¤„ç†å€¼å¤§äº10çš„äº‹ä»¶ï¼‰
    filter_processor = DataFilter(lambda event: event.data.get('value', 0) > 10)
    processor.add_processor(filter_processor)

# æ·»åŠ è½¬æ¢å™¨ï¼ˆå°†å€¼ä¹˜ä»¥2ï¼‰
    transform_processor = DataTransformer(lambda data: {**data, 'value': data.get('value', 0) * 2})
    processor.add_processor(transform_processor)

# æ·»åŠ èšåˆå™¨
    aggregator = DataAggregator(window_size=5, window_time=10)
    processor.add_processor(aggregator)

# å¯åŠ¨å¤„ç†å™¨
    processor.start()

# æ¨¡æ‹Ÿæ•°æ®æµ
    for i in range(20):
        event = DataEvent(
            id=f"event_{i}",
            timestamp=time.time(),
            data={'value': i, 'source': 'sensor'},
            source='sensor'
        )
        processor.send_event(event)
        time.sleep(0.1)

# ç­‰å¾…å¤„ç†å®Œæˆ
    time.sleep(2)

# è·å–å¤„ç†ç»“æœ
    print("å¤„ç†ç»“æœ:")
    while not processor.output_queue.empty():
        event = processor.get_processed_event()
        print(f"  {event.id}: {event.data}")

# åœæ­¢å¤„ç†å™¨
    processor.stop()

if __name__ == "__main__":
    stream_processing_example()
```

## 5. æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†æ•°æ®æ¨¡å‹ç†è®ºä½“ç³»åœ¨å®é™…é¡¹ç›®ä¸­çš„å…·ä½“åº”ç”¨æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬ï¼š

1. **æ•°æ®å­˜å‚¨æ¨¡å‹å®è·µ**ï¼šåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿè®¾è®¡å’Œæ•°æ®åˆ†ç‰‡è´Ÿè½½å‡è¡¡
2. **æ•°æ®æµå¤„ç†æ¨¡å‹å®è·µ**ï¼šå®æ—¶æ•°æ®æµå¤„ç†ç³»ç»Ÿ

è¿™äº›æ¡ˆä¾‹å±•ç¤ºäº†æ•°æ®æ¨¡å‹ç†è®ºåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿã€å¤§æ•°æ®å¤„ç†ç­‰é¢†åŸŸçš„å®é™…åº”ç”¨ï¼Œä¸ºæ•°æ®æ¨¡å‹é¡¹ç›®çš„å¼€å‘å’Œéƒ¨ç½²æä¾›äº†å…·ä½“çš„å‚è€ƒå’ŒæŒ‡å¯¼ã€‚
